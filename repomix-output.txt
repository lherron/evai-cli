This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.cursor/
  rules/
    database.mdc
    tests.mdc
~/
  .evai/
    tools/
      deploy_artifact/
        tool.py
        tool.yaml
evai/
  artifacts/
    CustomButton.tsx
    fancy-dashboard-card.tsx
  cli/
    commands/
      __init__.py
      cmdllmadd.py
      commands.py
      llmadd.py
      tools.py
    __init__.py
    cli.py
  docs/
    ARCHITECTURE.md
    cmd_plan.md
    prompt_plan.md
    todo.md
    tool_positional_args_todo.md
  mcp/
    mcp_prompts.py
    mcp_server.py
    mcp_tools.py
  templates/
    sample_command.py
    sample_command.yaml
    sample_tool.py
    sample_tool.yaml
  __init__.py
  command_storage.py
  llm_client.py
  tool_storage.py
tests/
  test_tools/
    test_cli_integration.py
    test_positional_args.py
  __init__.py
  test_cli.py
  test_command_add.py
  test_edit_implementation.py
  test_edit_metadata.py
  test_llm.py
  test_mcp_exposure.py
  test_metadata.py
  test_user_commands.py
.cursorrules
.gitignore
.python-version
.repomixignore
CLAUDE.md
pyproject.toml
README.md
repomix.config.json
requirements.txt

================================================================
Files
================================================================

================
File: .cursor/rules/database.mdc
================
---
description: Use when creating or updating database scripts or database entities
globs: *_db.py,database.py
---
## Database Rules
- Use [user_db.py](mdc:podthing/db/user_db.py) as the default reference for new database entities
- Each DB class should:
   - Take a Database instance in constructor
   - Use consistent error handling
   - Include proper type hints
   - Follow existing patterns from `user_db.py`
- Common utilities to keep in `database.py`:
   - Connection management
   - Transaction handling
   - Base query methods (fetch_one, fetch_all, etc.)
   - Row to model conversion utilities
- Error handling:
   - Use custom exceptions from `errors.py`
   - Consistent error messages
   - Proper logging
- Testing:
   - Each DB class should have its own test file
   - Follow existing test patterns
   - Include both success and error cases 

### Hints on using async postgresql
- Use asyncpg for PostgreSQL async operations
- PostgreSQL uses $1, $2 etc for parameterized queries instead of ?
- PostgreSQL has native JSON/JSONB support
- Use TIMESTAMP WITH TIME ZONE for proper timezone handling
- Use transactions for multi-statement operations
- PostgreSQL pool management is different from SQLite connections
- Always use CASCADE when dropping tables in PostgreSQL to handle foreign key constraints
- Initialize JSONB fields with default values in test fixtures

================
File: .cursor/rules/tests.mdc
================
---
description: Rules for creating new test classes
globs: test_*.py
---

# Database Test Rules

Fixtures to create other entities should use the DB library from that class.  For example, to create a test user as a fixture, use the methods in user_db.py.  Tests should not use raw SQL to create entities.

Use [test_user_db.py](mdc:tests/db/test_user_db.py) as an example of how to create tests for database classes.
Read the entity db table before beginning test creation.

================
File: ~/.evai/tools/deploy_artifact/tool.py
================
"""
Tool for deploying React component artifacts.

This tool saves React component artifacts to the artifacts directory.
"""

import os
import logging
from typing import Dict, Any

# Set up logging
logger = logging.getLogger(__name__)

def tool_deploy_artifact(artifact_name: str, source_code: str) -> Dict[str, Any]:
    """
    Deploy a React component artifact to the artifacts directory.
    
    Args:
        artifact_name: The name of the artifact (will be used as the filename)
        source_code: The source code of the React component
        
    Returns:
        A dictionary with the status of the deployment
    """
    logger.debug(f"Deploying artifact: {artifact_name}")
    
    try:
        # Ensure artifact name is valid
        if not artifact_name:
            raise ValueError("Artifact name cannot be empty")
        
        # Ensure artifact name has proper extension
        if not artifact_name.endswith('.tsx'):
            artifact_name = f"{artifact_name}.tsx"
        
        # Get the artifacts directory path
        artifacts_dir = os.path.expanduser("~/projects/evai-cli/evai/artifacts")
        
        # Create the directory if it doesn't exist
        os.makedirs(artifacts_dir, exist_ok=True)
        
        # Create the full path to the artifact file
        artifact_path = os.path.join(artifacts_dir, artifact_name)
        
        # Write the source code to the file
        with open(artifact_path, "w") as f:
            f.write(source_code)
        
        logger.info(f"Successfully deployed artifact to {artifact_path}")
        
        return {
            "status": "success",
            "message": f"Artifact '{artifact_name}' deployed successfully",
            "path": artifact_path
        }
        
    except Exception as e:
        logger.error(f"Error deploying artifact: {e}")
        return {
            "status": "error",
            "message": str(e)
        }

================
File: ~/.evai/tools/deploy_artifact/tool.yaml
================
name: deploy_artifact
description: Deploy a React component artifact to the artifacts directory
params:
  - name: artifact_name
    type: string
    description: The name of the artifact (will be used as the filename)
    required: true
  - name: source_code
    type: string
    description: The source code of the React component
    required: true
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15

================
File: evai/artifacts/CustomButton.tsx
================
import React from 'react';
import { Button } from '@/components/ui/button';

interface ButtonProps {
  label: string;
  onClick: () => void;
}

export const CustomButton: React.FC<ButtonProps> = ({ label, onClick }) => {
  return (
    <Button 
      className='bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded'
      onClick={onClick}
    >
      {label}
    </Button>
  );
};

================
File: evai/artifacts/fancy-dashboard-card.tsx
================
import React, { useState } from 'react';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, ResponsiveContainer } from 'recharts';
import { Card, CardContent, CardDescription, CardFooter, CardHeader, CardTitle } from '@/components/ui/card';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { AlertCircle, ArrowUpRight, BarChart2, Clock, Download, Users } from 'lucide-react';
import { Alert, AlertDescription, AlertTitle } from '@/components/ui/alert';

const FancyDashboardCard = () => {
  const [view, setView] = useState('week');
  
  // Sample data
  const weekData = [
    { name: 'Mon', value: 420 },
    { name: 'Tue', value: 380 },
    { name: 'Wed', value: 510 },
    { name: 'Thu', value: 580 },
    { name: 'Fri', value: 550 },
    { name: 'Sat', value: 620 },
    { name: 'Sun', value: 670 },
  ];
  
  const monthData = [
    { name: 'Week 1', value: 2800 },
    { name: 'Week 2', value: 3200 },
    { name: 'Week 3', value: 3600 },
    { name: 'Week 4', value: 3900 },
  ];
  
  const displayData = view === 'week' ? weekData : monthData;
  const currentValue = displayData[displayData.length - 1].value;
  const previousValue = displayData[displayData.length - 2].value;
  const percentChange = ((currentValue - previousValue) / previousValue * 100).toFixed(1);
  const isPositive = currentValue > previousValue;
  
  return (
    <Card className="w-full max-w-md shadow-lg">
      <CardHeader className="pb-2">
        <div className="flex justify-between items-start">
          <div>
            <CardTitle className="text-2xl font-bold">User Activity</CardTitle>
            <CardDescription className="text-gray-500">Daily active users</CardDescription>
          </div>
          <Badge variant="outline" className="flex items-center gap-1 px-2 py-1">
            <Users size={14} />
            <span>Users</span>
          </Badge>
        </div>
      </CardHeader>
      
      <CardContent className="pb-0">
        <div className="flex items-baseline justify-between mb-4">
          <div>
            <span className="text-3xl font-bold">{currentValue}</span>
            <div className="flex items-center gap-1 mt-1">
              <Badge className={`${isPositive ? 'bg-green-100 text-green-800' : 'bg-red-100 text-red-800'}`}>
                <span className="flex items-center gap-1">
                  {isPositive ? '+' : ''}{percentChange}%
                  <ArrowUpRight size={14} className={`${!isPositive && 'rotate-180'}`} />
                </span>
              </Badge>
              <span className="text-gray-500 text-sm">vs previous</span>
            </div>
          </div>
          
          <Tabs defaultValue="week" className="w-fit" onValueChange={setView}>
            <TabsList className="grid w-36 grid-cols-2">
              <TabsTrigger value="week">Week</TabsTrigger>
              <TabsTrigger value="month">Month</TabsTrigger>
            </TabsList>
          </Tabs>
        </div>
        
        <div className="h-64 w-full">
          <ResponsiveContainer width="100%" height="100%">
            <LineChart data={displayData} margin={{ top: 5, right: 5, left: 0, bottom: 5 }}>
              <CartesianGrid strokeDasharray="3 3" vertical={false} opacity={0.3} />
              <XAxis dataKey="name" axisLine={false} tickLine={false} />
              <YAxis axisLine={false} tickLine={false} width={30} />
              <Tooltip 
                contentStyle={{ 
                  borderRadius: '8px', 
                  border: 'none', 
                  boxShadow: '0 4px 12px rgba(0,0,0,0.1)',
                  padding: '8px 12px'
                }} 
              />
              <Line 
                type="monotone" 
                dataKey="value" 
                stroke="#6366F1" 
                strokeWidth={3} 
                dot={{ r: 4, strokeWidth: 2 }}
                activeDot={{ r: 6, stroke: '#6366F1', strokeWidth: 2 }}
              />
            </LineChart>
          </ResponsiveContainer>
        </div>
        
        {isPositive && percentChange > 15 && (
          <Alert className="mt-4 bg-amber-50">
            <AlertCircle className="h-4 w-4 text-amber-600" />
            <AlertTitle>Notable increase</AlertTitle>
            <AlertDescription>
              User activity has increased significantly. Check system resources.
            </AlertDescription>
          </Alert>
        )}
      </CardContent>
      
      <CardFooter className="flex justify-between pt-4">
        <Button variant="outline" size="sm" className="gap-1">
          <Download size={14} />
          Export
        </Button>
        <div className="flex items-center gap-2 text-gray-500 text-sm">
          <Clock size={14} />
          <span>Updated 2 mins ago</span>
        </div>
      </CardFooter>
    </Card>
  );
};

export default FancyDashboardCard;

================
File: evai/cli/commands/__init__.py
================
"""Command modules for EVAI CLI."""

# This file makes the commands directory a proper Python package

================
File: evai/cli/commands/cmdllmadd.py
================
"""LLM-assisted command creation for EVAI CLI."""

import sys
import os
import yaml
import click
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from evai.command_storage import (
    get_command_dir,
    save_command_metadata
)
from evai.llm_client import (
    generate_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)

# Initialize rich console
console = Console()


def generate_default_metadata_with_llm(command_name: str, description: str) -> dict:
    """
    Generate default metadata for a command using LLM.
    
    Args:
        command_name: The name of the command
        description: A description of the command
        
    Returns:
        A dictionary containing the command metadata
    """
    # Generate metadata with LLM with special instructions for commands
    prompt = f"""Generate metadata for a command (not a tool) named '{command_name}'. 
Description: {description}

The metadata should include:
1. name: {command_name}
2. description: A one-line description
3. arguments: List of command-line positional arguments (NOT parameters)
4. options: List of command-line options with flags
5. hidden: Boolean (false by default)
6. disabled: Boolean (false by default)
7. mcp_integration and llm_interaction objects (can be copy-pasted from the example below)

Each argument should have:
- name
- description  
- type (string, integer, float, boolean)

Each option should have:
- name
- description
- type (string, integer, float, boolean)  
- required (boolean)
- default (optional)

Example structure (fill in the actual details):
```yaml
name: "{command_name}"
description: "Command description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
```

Return ONLY the YAML, nothing else."""

    # Use the generic LLM client function but with our custom prompt
    try:
        from evai.llm_client import generate_content
        yaml_string = generate_content(prompt)
        
        # Try to parse the YAML
        return yaml.safe_load(yaml_string)
    except Exception as e:
        raise LLMClientError(f"Error generating command metadata: {e}")


@click.command()
@click.argument("command_name")
def llmadd(command_name):
    """Add a new custom command using LLM assistance."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)
        
        # Get a description from the user
        description = click.prompt("Enter a description for the command", type=str)
        
        # Check if additional information is needed
        try:
            additional_info = check_additional_info_needed(command_name, description)
            if additional_info:
                click.echo("\nThe LLM suggests gathering more information:")
                click.echo(additional_info)
                
                # Allow user to provide additional details
                additional_details = click.prompt(
                    "Would you like to provide additional details? (leave empty to skip)",
                    default="",
                    type=str
                )
                
                if additional_details:
                    description = f"{description}\n\nAdditional details: {additional_details}"
        except LLMClientError as e:
            click.echo(f"Warning: {e}")
            click.echo("Continuing with the provided description.")
        
        # Generate metadata with LLM
        click.echo("Generating command metadata with LLM...")
        
        try:
            metadata = generate_default_metadata_with_llm(command_name, description)
            click.echo("Metadata generated successfully.")
            
            # Display the generated YAML with rich formatting
            yaml_str = yaml.dump(metadata, default_flow_style=False)
            console.print("\n[bold blue]Generated YAML Metadata:[/bold blue]")
            console.print(Panel(Syntax(yaml_str, "yaml", theme="monokai", line_numbers=True)))
        except Exception as e:
            click.echo(f"Error generating metadata with LLM: {e}", err=True)
            click.echo("Falling back to default metadata.")
            
            # Create default metadata
            metadata = {
                "name": command_name,
                "description": description,
                "arguments": [],
                "options": [],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
        
        # Save the metadata
        save_command_metadata(cmd_dir, metadata)
        
        # Generate implementation with LLM
        click.echo("\nGenerating command implementation with LLM...")
        
        try:
            # Custom prompt for command implementation
            impl_prompt = f"""Create a Python implementation for a command-line interface command named '{command_name}'.
Description: {description}

Here is the YAML metadata for this command:
```yaml
{yaml.dump(metadata, default_flow_style=False)}
```

The implementation should:
1. Define a 'run' function that accepts all arguments and options in the metadata
2. Process the arguments and options as needed
3. Return a dictionary with the command's results

The file should include:
- A module docstring explaining the command
- Type hints for all arguments
- Proper error handling
- Informative docstrings

Example structure:
```python
\"\"\"Implementation for the {command_name} command.\"\"\"

def run(**kwargs):
    \"\"\"Execute the {command_name} command with the given arguments.\"\"\"
    # Extract arguments from kwargs
    # Process the command logic
    # Return a dictionary with results
    return {{"status": "success", "data": {...}}}
```

Return ONLY the Python code, nothing else."""

            from evai.llm_client import generate_content
            implementation = generate_content(impl_prompt)
            
            click.echo("Implementation generated successfully.")
            
            # Display the generated Python code with rich formatting
            console.print("\n[bold blue]Generated Python Implementation:[/bold blue]")
            console.print(Panel(Syntax(implementation, "python", theme="monokai", line_numbers=True)))
            
            # Save the implementation
            cmd_py_path = os.path.join(cmd_dir, "command.py")
            with open(cmd_py_path, "w") as f:
                f.write(implementation)
        except Exception as e:
            click.echo(f"Error generating implementation with LLM: {e}", err=True)
            click.echo("Falling back to default implementation.")
            
            # Create default implementation
            cmd_py_path = os.path.join(cmd_dir, "command.py")
            with open(cmd_py_path, "w") as f:
                f.write(f'"""Implementation for the {command_name} command."""\n\n\ndef run(**kwargs):\n    """Execute the {command_name} command with the given arguments."""\n    print("Hello World")\n    return {{"status": "success"}}\n')
        
        click.echo(f"\nCommand '{command_name}' created successfully.")
        click.echo(f"- Metadata: {os.path.join(cmd_dir, 'command.yaml')}")
        click.echo(f"- Implementation: {cmd_py_path}")
        click.echo(f"\nTo edit this command, run: evai commands edit {command_name}")
        
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)

================
File: evai/cli/commands/commands.py
================
"""Command management functions for EVAI CLI."""

import sys
import os
import json
import click
import yaml
from evai.command_storage import (
    get_command_dir,
    save_command_metadata,
    load_command_metadata,
    list_commands,
    import_command_module,
    run_command
)
from rich.console import Console
from rich.syntax import Syntax
from rich.panel import Panel

# Initialize rich console
console = Console()


@click.command()
@click.argument("command_name")
def add(command_name):
    """Add a new custom command."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)
            
        # Load default metadata template
        with open(os.path.join(os.path.dirname(__file__), "../../templates/sample_command.yaml"), "r") as f:
            metadata_content = f.read().replace("{command_name}", command_name)
            default_metadata = yaml.safe_load(metadata_content)
        
        # Save metadata
        save_command_metadata(cmd_dir, default_metadata)
        
        # Create default command.py
        with open(os.path.join(os.path.dirname(__file__), "../../templates/sample_command.py"), "r") as f:
            with open(cmd_dir / "command.py", "w") as py_file:
                py_file.write(f.read())
        
        click.echo(f"Command '{command_name}' created successfully.")
        click.echo(f"- Metadata: {cmd_dir / 'command.yaml'}")
        click.echo(f"- Implementation: {cmd_dir / 'command.py'}")
        click.echo(f"\nTo edit this command, run: evai commands edit {command_name}")
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
def new(command_name):
    """Alias for 'add' - Add a new custom command."""
    add.callback(command_name)


@click.command()
@click.argument("command_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit command metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit command implementation")
def edit(command_name, metadata, implementation):
    """Edit an existing command."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        # Edit metadata if requested
        if metadata:
            click.echo(f"Opening command.yaml for editing...")
            
            # Get path to metadata file
            metadata_path = cmd_dir / "command.yaml"
            if not metadata_path.exists():
                click.echo(f"Metadata file not found: {metadata_path}", err=True)
                sys.exit(1)
                
            # Open editor for user to edit the file
            click.edit(filename=str(metadata_path))
            
            # Validate YAML after editing
            try:
                with open(metadata_path, "r") as f:
                    metadata_content = yaml.safe_load(f)
                click.echo("Command metadata saved successfully.")
            except Exception as e:
                click.echo(f"Invalid YAML: {e}", err=True)
                if click.confirm("Would you like to try again?"):
                    return edit.callback(command_name, True, False)
                click.echo("Skipping metadata edit.")
        
        # Edit implementation if requested
        if implementation:
            click.echo(f"Opening command.py for editing...")
            
            # Get path to implementation file
            impl_path = cmd_dir / "command.py"
            if not impl_path.exists():
                click.echo(f"Implementation file not found: {impl_path}", err=True)
                sys.exit(1)
                
            # Open editor for user to edit the file
            click.edit(filename=str(impl_path))
            click.echo("Command implementation saved.")
        
        click.echo(f"Command '{command_name}' edited successfully.")
        
    except Exception as e:
        click.echo(f"Error editing command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit command metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit command implementation")
def e(command_name, metadata, implementation):
    """Alias for 'edit' - Edit an existing command."""
    edit.callback(command_name, metadata, implementation)


@click.command()
def list():
    """List all available commands."""
    try:
        # Get the list of commands
        commands = list_commands()
        
        if not commands:
            click.echo("No commands found.")
            return
        
        # Print the list of commands
        click.echo("Available commands:")
        for cmd in commands:
            click.echo(f"- {cmd['name']}: {cmd['description']}")
        
    except Exception as e:
        click.echo(f"Error listing commands: {e}", err=True)
        sys.exit(1)


@click.command()
def ls():
    """Alias for 'list' - List all available commands."""
    list.callback()


@click.command()
@click.argument("command_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def remove(command_name, force):
    """Remove a custom command."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        if not cmd_dir.exists():
            click.echo(f"Command '{command_name}' not found.", err=True)
            sys.exit(1)
            
        # Confirm removal unless force flag is set
        if not force and not click.confirm(f"Are you sure you want to remove command '{command_name}'?"):
            click.echo("Operation cancelled.")
            return
        
        # Remove the command directory
        import shutil
        shutil.rmtree(cmd_dir)
        
        click.echo(f"Command '{command_name}' removed successfully.")
        
    except Exception as e:
        click.echo(f"Error removing command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def rm(command_name, force):
    """Alias for 'remove' - Remove a custom command."""
    remove.callback(command_name, force)


@click.command()
@click.argument("command_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Command parameters in the format key=value")
def run(command_name, args, param):
    """Run a command with the given arguments.
    
    Arguments can be provided as positional arguments after the command name,
    or as key=value pairs with the --param/-p option.
    
    Example:
        evai commands run greet John
        evai commands run greet --param name=John --param greeting=Hello
    """
    try:
        # Parse parameters from --param options
        kwargs = {}
        for p in param:
            try:
                key, value = p.split("=", 1)
                # Try to parse the value as JSON
                try:
                    kwargs[key] = json.loads(value)
                except json.JSONDecodeError:
                    # If not valid JSON, use the raw string
                    kwargs[key] = value
            except ValueError:
                click.echo(f"Invalid parameter format: {p}. Use key=value format.", err=True)
                sys.exit(1)
        
        # Get command metadata to check parameter requirements
        cmd_dir = get_command_dir(command_name)
        metadata = load_command_metadata(cmd_dir)
        
        # If using --param options, check required options
        if not args and "options" in metadata:
            for opt_def in metadata.get("options", []):
                opt_name = opt_def.get("name")
                if opt_name and opt_def.get("required", False) and opt_name not in kwargs:
                    # If option has a default value, use it
                    if "default" in opt_def and opt_def["default"] is not None:
                        kwargs[opt_name] = opt_def["default"]
                    else:
                        click.echo(f"Missing required option: {opt_name}", err=True)
                        sys.exit(1)
        
        # Run the command with positional args if provided, otherwise use kwargs
        if args:
            result = run_command(command_name, *args, **kwargs)
        else:
            result = run_command(command_name, **kwargs)
        
        # Print the result
        if isinstance(result, dict):
            click.echo(json.dumps(result, indent=2))
        else:
            click.echo(result)
    
    except Exception as e:
        click.echo(f"Error running command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Command parameters in the format key=value")
def r(command_name, args, param):
    """Alias for 'run' - Run a command with the given arguments."""
    run.callback(command_name, args, param)

================
File: evai/cli/commands/llmadd.py
================
"""LLM-assisted tool creation for EVAI CLI."""

import sys
import os
import yaml
import click
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from evai.tool_storage import (
    get_tool_dir, 
    save_tool_metadata,
    load_sample_tool_yaml
)
from evai.llm_client import (
    generate_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)

# Initialize rich console
console = Console()


def generate_default_metadata_with_llm(tool_name: str, description: str) -> dict:
    """
    Generate default metadata for a tool using LLM.
    
    Args:
        tool_name: The name of the tool
        description: A description of the tool
        
    Returns:
        A dictionary containing the tool metadata
    """
    # Generate metadata with LLM
    metadata = generate_metadata_with_llm(tool_name, description)
    
    # Ensure required fields are present
    if "name" not in metadata:
        metadata["name"] = tool_name
    if "description" not in metadata:
        metadata["description"] = description
    if "params" not in metadata:
        metadata["params"] = []
    if "hidden" not in metadata:
        metadata["hidden"] = False
    if "disabled" not in metadata:
        metadata["disabled"] = False
    if "mcp_integration" not in metadata:
        metadata["mcp_integration"] = {
            "enabled": True,
            "metadata": {
                "endpoint": "",
                "method": "POST",
                "authentication_required": False
            }
        }
    if "llm_interaction" not in metadata:
        metadata["llm_interaction"] = {
            "enabled": False,
            "auto_apply": True,
            "max_llm_turns": 15
        }
    
    return metadata


@click.command()
@click.argument("tool_name")
def llmadd(tool_name):
    """Add a new custom tool using LLM assistance."""
    try:
        # Get the tool directory
        tool_dir = get_tool_dir(tool_name)
        
        # Get a description from the user
        description = click.prompt("Enter a description for the tool", type=str)
        
        # Check if additional information is needed
        try:
            additional_info = check_additional_info_needed(tool_name, description)
            if additional_info:
                click.echo("\nThe LLM suggests gathering more information:")
                click.echo(additional_info)
                
                # Allow user to provide additional details
                additional_details = click.prompt(
                    "Would you like to provide additional details? (leave empty to skip)",
                    default="",
                    type=str
                )
                
                if additional_details:
                    description = f"{description}\n\nAdditional details: {additional_details}"
        except LLMClientError as e:
            click.echo(f"Warning: {e}")
            click.echo("Continuing with the provided description.")
        
        # Generate metadata with LLM
        click.echo("Generating metadata with LLM...")
        
        try:
            metadata = generate_default_metadata_with_llm(tool_name, description)
            click.echo("Metadata generated successfully.")
            
            # Display the generated YAML with rich formatting
            yaml_str = yaml.dump(metadata, default_flow_style=False)
            console.print("\n[bold blue]Generated YAML Metadata:[/bold blue]")
            console.print(Panel(Syntax(yaml_str, "yaml", theme="monokai", line_numbers=True)))
        except Exception as e:
            click.echo(f"Error generating metadata with LLM: {e}", err=True)
            click.echo("Falling back to default metadata.")
            
            # Try to load the sample template
            try:
                metadata = load_sample_tool_yaml(tool_name)
                metadata["description"] = description
            except Exception as template_error:
                click.echo(f"Error loading sample template: {template_error}", err=True)
                
                # Create default metadata
                metadata = {
                    "name": tool_name,
                    "description": description,
                    "params": [],
                    "hidden": False,
                    "disabled": False,
                    "mcp_integration": {
                        "enabled": True,
                        "metadata": {
                            "endpoint": "",
                            "method": "POST",
                            "authentication_required": False
                        }
                    },
                    "llm_interaction": {
                        "enabled": False,
                        "auto_apply": True,
                        "max_llm_turns": 15
                    }
                }
        
        # Save the metadata
        save_tool_metadata(tool_dir, metadata)
        
        # Generate implementation with LLM
        click.echo("\nGenerating tool implementation with LLM...")
        
        try:
            implementation = generate_implementation_with_llm(tool_name, metadata)
            click.echo("Implementation generated successfully.")
            
            # Display the generated Python code with rich formatting
            console.print("\n[bold blue]Generated Python Implementation:[/bold blue]")
            console.print(Panel(Syntax(implementation, "python", theme="monokai", line_numbers=True)))
            
            # Save the implementation
            tool_py_path = os.path.join(tool_dir, "tool.py")
            with open(tool_py_path, "w") as f:
                f.write(implementation)
        except Exception as e:
            click.echo(f"Error generating implementation with LLM: {e}", err=True)
            click.echo("Falling back to default implementation.")
            
            # Create default implementation
            tool_py_path = os.path.join(tool_dir, "tool.py")
            with open(tool_py_path, "w") as f:
                f.write(f'"""Custom tool implementation for {tool_name}."""\n\n\ndef run(**kwargs):\n    """Run the tool with the given arguments."""\n    print("Hello World")\n    return {{"status": "success"}}\n')
        
        click.echo(f"\nTool '{tool_name}' created successfully.")
        click.echo(f"- Metadata: {os.path.join(tool_dir, 'tool.yaml')}")
        click.echo(f"- Implementation: {tool_py_path}")
        click.echo(f"\nTo edit this tool, run: evai tool edit {tool_name}")
        
    except Exception as e:
        click.echo(f"Error creating tool: {e}", err=True)
        sys.exit(1)

================
File: evai/cli/commands/tools.py
================
"""Tool management functions for EVAI CLI."""

import sys
import os
import json
import click
from evai.tool_storage import (
    get_tool_dir, 
    save_tool_metadata, 
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    list_tools,
    run_tool,
    load_tool_metadata,
    load_sample_tool_py,
    load_sample_tool_yaml,
    remove_tool
)
from rich.console import Console

# Initialize rich console
console = Console()


@click.command()
@click.argument("tool_name")
def add(tool_name):
    """Add a new custom tool."""
    try:
        # Get the tool directory
        tool_dir = get_tool_dir(tool_name)
        
        # Load the sample tool.yaml template
        try:
            default_metadata = load_sample_tool_yaml(tool_name)
        except Exception as e:
            click.echo(f"Error loading sample tool.yaml template: {e}", err=True)
            click.echo("Falling back to default metadata.")
            
            # Create default metadata
            default_metadata = {
                "name": tool_name,
                "description": "Default description",
                "params": [],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
        
        # Save the metadata
        save_tool_metadata(tool_dir, default_metadata)
        
        # Create default tool.py
        tool_py_path = os.path.join(tool_dir, "tool.py")
        try:
            tool_py_content = load_sample_tool_py()
            with open(tool_py_path, "w") as f:
                f.write(tool_py_content)
        except Exception as e:
            click.echo(f"Error loading sample tool.py template: {e}", err=True)
            click.echo("Falling back to default implementation.")
            
            # Create default tool.py with a simple function (not using run(**kwargs))
            with open(tool_py_path, "w") as f:
                f.write('"""Custom tool implementation."""\n\n\ndef tool_echo(echo_string: str) -> str:\n    """Echo the input string."""\n    return echo_string\n')
        
        click.echo(f"Tool '{tool_name}' created successfully.")
        click.echo(f"- Metadata: {os.path.join(tool_dir, 'tool.yaml')}")
        click.echo(f"- Implementation: {tool_py_path}")
        click.echo(f"\nTo edit this tool, run: evai tool edit {tool_name}")
        
    except Exception as e:
        click.echo(f"Error creating tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
def new(tool_name):
    """Alias for 'add' - Add a new custom tool."""
    add.callback(tool_name)


@click.command()
@click.argument("tool_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit tool metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit tool implementation")
def edit(tool_name, metadata, implementation):
    """Edit an existing tool."""
    try:
        # Get the tool directory
        tool_dir = get_tool_dir(tool_name)
        
        # Edit metadata if requested
        if metadata:
            click.echo(f"Opening tool.yaml for editing...")
            
            # Loop until the user provides valid YAML or chooses to abort
            while True:
                success, metadata_content = edit_tool_metadata(tool_dir)
                
                if success:
                    click.echo("Tool metadata saved successfully.")
                    break
                else:
                    if not click.confirm("Invalid YAML. Would you like to try again?"):
                        click.echo("Aborting metadata edit.")
                        break
                    click.echo("Opening tool.yaml for editing again...")
        
        # Edit implementation if requested
        if implementation:
            click.echo(f"Opening tool.py for editing...")
            
            # Open the editor for the user to edit the file
            edit_tool_implementation(tool_dir)
            
            # Run a lint check on the edited file
            lint_success, lint_output = run_lint_check(tool_dir)
            
            if not lint_success:
                click.echo("Lint check failed with the following errors:")
                click.echo(lint_output)
                
                if click.confirm("Would you like to fix the lint errors?"):
                    # Loop until the user fixes the lint errors or chooses to abort
                    while True:
                        click.echo(f"Opening tool.py for editing...")
                        
                        # Open the editor for the user to edit the file
                        edit_tool_implementation(tool_dir)
                        
                        # Run a lint check on the edited file
                        lint_success, lint_output = run_lint_check(tool_dir)
                        
                        if lint_success:
                            click.echo("Lint check passed.")
                            break
                        else:
                            click.echo("Lint check failed with the following errors:")
                            click.echo(lint_output)
                            
                            if not click.confirm("Would you like to try again?"):
                                click.echo("Skipping lint errors.")
                                break
            else:
                click.echo("Lint check passed.")
        
        click.echo(f"Tool '{tool_name}' edited successfully.")
        
    except Exception as e:
        click.echo(f"Error editing tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit tool metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit tool implementation")
def e(tool_name, metadata, implementation):
    """Alias for 'edit' - Edit an existing tool."""
    edit.callback(tool_name, metadata, implementation)


@click.command()
def list():
    """List all available tools."""
    try:
        # Get the list of tools
        tools = list_tools()
        
        if not tools:
            click.echo("No tools found.")
            return
        
        # Print the list of tools
        click.echo("Available tools:")
        for tool in tools:
            click.echo(f"- {tool['name']}: {tool['description']}")
        
    except Exception as e:
        click.echo(f"Error listing tools: {e}", err=True)
        sys.exit(1)


@click.command()
def ls():
    """Alias for 'list' - List all available tools."""
    list.callback()


@click.command()
@click.argument("tool_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Tool parameters in the format key=value (for backward compatibility)")
def run(tool_name, args, param):
    """Run a tool with the given arguments.
    
    Arguments can be provided as positional arguments after the tool name,
    or as key=value pairs with the --param/-p option for backward compatibility.
    
    Example:
        evai tools run subtract 8 5
        evai tools run subtract --param minuend=8 --param subtrahend=5
    """
    try:
        # Parse parameters from --param options (backward compatibility)
        kwargs = {}
        for p in param:
            try:
                key, value = p.split("=", 1)
                # Try to parse the value as JSON
                try:
                    kwargs[key] = json.loads(value)
                except json.JSONDecodeError:
                    # If not valid JSON, use the raw string
                    kwargs[key] = value
            except ValueError:
                click.echo(f"Invalid parameter format: {p}. Use key=value format.", err=True)
                sys.exit(1)
        
        # Get tool metadata to check parameter requirements
        tool_dir = get_tool_dir(tool_name)
        metadata = load_tool_metadata(tool_dir)
        
        # If using --param options, check required parameters
        if not args and "params" in metadata:
            for param_def in metadata.get("params", []):
                param_name = param_def.get("name")
                if param_name and param_def.get("required", True) and param_name not in kwargs:
                    # If parameter has a default value, use it
                    if "default" in param_def and param_def["default"] is not None:
                        kwargs[param_name] = param_def["default"]
                    else:
                        click.echo(f"Missing required parameter: {param_name}", err=True)
                        sys.exit(1)
        
        # Run the tool with positional args if provided, otherwise use kwargs
        if args:
            result = run_tool(tool_name, *args)
        else:
            result = run_tool(tool_name, **kwargs)
        
        # Print the result
        if isinstance(result, dict):
            click.echo(json.dumps(result, indent=2))
        else:
            click.echo(result)
    
    except Exception as e:
        click.echo(f"Error running tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Tool parameters in the format key=value (for backward compatibility)")
def r(tool_name, args, param):
    """Alias for 'run' - Run a tool with the given arguments."""
    run.callback(tool_name, args, param)


@click.command()
@click.argument("tool_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def remove(tool_name, force):
    """Remove a custom tool."""
    try:
        # Confirm removal unless force flag is set
        if not force and not click.confirm(f"Are you sure you want to remove tool '{tool_name}'?"):
            click.echo("Operation cancelled.")
            return
        
        # Remove the tool
        remove_tool(tool_name)
        
        click.echo(f"Tool '{tool_name}' removed successfully.")
        
    except FileNotFoundError:
        click.echo(f"Tool '{tool_name}' not found.", err=True)
        sys.exit(1)
    except Exception as e:
        click.echo(f"Error removing tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def rm(tool_name, force):
    """Alias for 'remove' - Remove a custom tool."""
    remove.callback(tool_name, force)

================
File: evai/cli/__init__.py
================
"""Commands submodule for EVAI CLI."""

================
File: evai/cli/cli.py
================
"""Command-line interface for EVAI."""

import sys
import os
import json
import click
import importlib
import pkgutil
from evai import __version__
from evai.tool_storage import (
    get_tool_dir, 
    save_tool_metadata, 
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    list_tools,
    run_tool,
    load_tool_metadata
)
from evai.command_storage import get_command_dir, save_command_metadata, list_commands, import_command_module
from evai.llm_client import (
    generate_default_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)
from rich.console import Console
from rich.syntax import Syntax
from rich.panel import Panel
import yaml
import logging

# Initialize rich console
console = Console()

# Type mapping for Click parameter types
TYPE_MAP = {
    "string": click.STRING,
    "integer": click.INT,
    "float": click.FLOAT,
    "boolean": click.BOOL,
}

# Create an AliasedGroup class to support command aliases
class AliasedGroup(click.Group):
    def get_command(self, ctx, cmd_name):
        # Try to get command by name
        rv = click.Group.get_command(self, ctx, cmd_name)
        if rv is not None:
            return rv
        
        # Try to match aliases
        matches = [x for x in self.list_commands(ctx) if x.startswith(cmd_name)]
        if not matches:
            return None
        elif len(matches) == 1:
            return click.Group.get_command(self, ctx, matches[0])
        
        ctx.fail(f"Too many matches: {', '.join(sorted(matches))}")


@click.group(help="EVAI CLI - Command-line interface for EVAI")
@click.version_option(version=__version__, prog_name="evai")
def cli():
    """EVAI CLI - Command-line interface for EVAI."""
    pass


@cli.group(cls=AliasedGroup)
def tools():
    """Manage custom tools."""
    pass

# Tool functions have been moved to evai/cli/commands/tool.py

@cli.group(cls=AliasedGroup)
def commands():
    """Manage user-defined commands."""
    pass

@cli.group(cls=AliasedGroup)
def user():
    """User-defined commands."""
    pass

def create_user_command(command_metadata: dict):
    """Create a Click command from command metadata."""
    command_name = command_metadata["name"]
    description = command_metadata.get("description", "")
    arg_names = [arg["name"] for arg in command_metadata.get("arguments", [])]

    def callback(*args, **kwargs):
        module = import_command_module(command_name)
        run_func = getattr(module, "run")
        params = dict(zip(arg_names, args))
        params.update(kwargs)
        result = run_func(**params)
        click.echo(json.dumps(result, indent=2))
        return result

    command = click.command(name=command_name, help=description)(callback)

    # Add arguments
    for arg in command_metadata.get("arguments", []):
        command = click.argument(
            arg["name"],
            type=TYPE_MAP.get(arg.get("type", "string"), click.STRING)
        )(command)

    # Add options
    for opt in command_metadata.get("options", []):
        command = click.option(
            f"--{opt['name']}",
            type=TYPE_MAP.get(opt.get("type", "string"), click.STRING),
            help=opt.get("description", ""),
            required=opt.get("required", False),
            default=opt.get("default", None)
        )(command)

    return command

def load_user_commands():
    """Load and register user-defined commands from ~/.evai/commands."""
    commands_list = list_commands()
    for cmd_meta in commands_list:
        try:
            command = create_user_command(cmd_meta)
            user.add_command(command)
        except Exception as e:
            logger.warning(f"Failed to load command {cmd_meta['name']}: {e}")

@cli.command()
@click.option("--name", "-n", default="EVAI Tools", help="Name of the MCP server")
def server(name):
    """Start an MCP server exposing all tools."""
    try:
        # Import here to avoid dependency issues if MCP is not installed
        from .mcp_server import run_server
        
        click.echo(f"Starting MCP server '{name}'...")
        click.echo("Press Ctrl+C to stop the server.")
        
        # Run the server
        run_server(name)
    except ImportError as e:
        click.echo(f"Error: {e}", err=True)
        click.echo("Please install the MCP Python SDK with: pip install mcp", err=True)
        sys.exit(1)
    except Exception as e:
        click.echo(f"Error starting MCP server: {e}", err=True)
        sys.exit(1)


# Automatically add all commands from the commands submodule
def import_commands():
    """Import all commands from the commands submodule and add them to the appropriate groups."""
    from evai.cli import commands as commands_module
    
    # Get the package path
    package_path = os.path.dirname(commands_module.__file__)
    
    # Iterate through all modules in the commands package
    for _, module_name, _ in pkgutil.iter_modules([package_path]):
        # Import the module
        module = importlib.import_module(f"evai.cli.commands.{module_name}")
        
        # Find all Click commands in the module
        for attr_name in dir(module):
            attr = getattr(module, attr_name)
            
            # Check if it's a Click command
            if isinstance(attr, click.Command):
                # Determine which group to add the command to
                if module_name == "tools" or module_name == "llmadd":
                    # Add tool-related commands to the tools group
                    tools.add_command(attr)
                elif module_name == "commands" or module_name == "cmdllmadd":
                    # Add command-related commands to the commands group
                    commands.add_command(attr)
                else:
                    # Default to tools group for anything else
                    tools.add_command(attr)


# Import commands
import_commands()

# Load user-defined commands
load_user_commands()


def main():
    """Run the EVAI CLI."""
    # If no arguments are provided, show help
    if len(sys.argv) == 1:
        sys.argv.append("--help")
    return cli()


if __name__ == "__main__":
    sys.exit(main())

================
File: evai/docs/ARCHITECTURE.md
================
```mermaid

flowchart TB
    %% External Systems
    User([User])
    Claude["Claude Desktop\n(MCP Client)"]
    Terminal["Terminal/iTerm\n(CLI Interface)"]
    AnthropicAPI["Anthropic API\n(LLM Service)"]
    
    %% Main evai CLI Application
    subgraph evai["evai CLI Application"]
        direction TB
        
        Core["CLI Core\nCommand Parsing, Execution, I/O"]
        
        subgraph Components["Core Components"]
            direction LR
            MCPServer["Embedded MCP Server\n(stdio Transport)"]
            LLMClient["LLM Client\n(Anthropic API Interface)"]
            SelfMod["Self-Modification System\nCode Analysis, Generation, Integration"]
        end
        
        subgraph Storage["Storage"]
            Commands["Command Repository\nDefinitions, Metadata"]
            CodeStore["Code Storage\nCommand Implementations"]
        end
        
        Core --> Components
        Core --> Storage
        SelfMod --> CodeStore
        SelfMod --> Commands
        
        %% Dynamic tool registration pathway
        Commands -- "New/Modified\nCommands" --> MCPServer
    end
    
    %% Relations/Connections
    User --> Claude
    User --> Terminal
    Claude -- "MCP stdio Protocol" --> MCPServer
    Terminal -- "CLI Commands" --> Core
    LLMClient -- "API Calls" --> AnthropicAPI
    
    %% MCP Tools
    subgraph MCPTools["MCP Tools Exposed"]
        ListCommands["list-commands"]
        AddCommand["add-command"]
        ExecuteCommand["execute-command"]
        ModifyCommand["modify-command"]
        DynamicTools["Dynamic Command Tools\n(Auto-registered from Command Repository)"]
    end
    
    MCPServer --> MCPTools
    Commands -- "Dynamic Registration" --> MCPServer
    
    classDef externalSystems fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef evaiCore fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef storage fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    classDef selfMod fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef component fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef tools fill:#f3e5f5,stroke:#7b1fa2,stroke-width:1px
    
    class User,Claude,Terminal,AnthropicAPI externalSystems
    class Core evaiCore
    class Commands,CodeStore storage
    class SelfMod selfMod
    class MCPServer,LLMClient component
    class ListCommands,AddCommand,ExecuteCommand,ModifyCommand tools
```

================
File: evai/docs/cmd_plan.md
================
Below is a detailed proposal to add functionality for creating new commands in the EVAI CLI, mirroring the existing tool creation process. The new commands will reside in ~/.evai/commands, be loaded on EVAI startup, and be registered in Click like the existing command structure. This proposal includes a step-by-step approach with specific instructions, code snippets, and a final prompt tailored for a less intelligent sub-LLM to implement incrementally, stopping for feedback at each step.
Proposal to Add Command Creation Functionality to EVAI CLI
General Approach
To meet your requirements, we'll extend the EVAI CLI by:
Mirroring the Tool Structure: Commands will follow a similar directory and file structure to tools (~/.evai/commands/<command_name>/ with command.yaml and command.py), leveraging existing utilities where possible.
Adding Command Storage: Create a new command_storage.py module to handle command-specific file operations, parallel to tool_storage.py.
Enhancing CLI: Introduce a commands group with an add subcommand, similar to tools add, to create new commands interactively.
Dynamic Registration: Modify cli.py to load and register user-defined commands in Click at startup, ensuring they integrate seamlessly with the existing structure.
Ensuring Specificity: Each step will be discrete, with clear instructions and code, stopping for feedback to keep the implementation on track.
This approach ensures that commands are treated as first-class citizens in the CLI, with a creation process akin to tools, and are automatically available upon startup.
Discrete Steps for Implementation
Step 1: Create command_storage.py Module
Purpose: Establish a dedicated module for command file operations, mirroring tool_storage.py.
Instructions:
Create a new file evai/command_storage.py.
Define functions to manage the ~/.evai/commands directory, load/save metadata, list commands, and import command modules.
Use pathlib.Path for cross-platform compatibility, similar to tool_storage.py.
Code:
python


"""Command storage utilities for EVAI CLI."""

import os
import yaml
from pathlib import Path
import importlib.util
import logging

logger = logging.getLogger(__name__)

COMMANDS_DIR = Path.home() / ".evai" / "commands"

def get_command_dir(command_name: str) -> Path:
    """Get the directory path for a command and create it if it doesn't exist."""
    command_dir = COMMANDS_DIR / command_name
    command_dir.mkdir(parents=True, exist_ok=True)
    return command_dir

def load_command_metadata(path: Path) -> dict:
    """Load command metadata from command.yaml."""
    yaml_path = path / "command.yaml"
    if not yaml_path.exists():
        raise FileNotFoundError(f"Command metadata file not found: {yaml_path}")
    with yaml_path.open("r") as f:
        return yaml.safe_load(f) or {}

def save_command_metadata(path: Path, data: dict) -> None:
    """Save command metadata to command.yaml."""
    yaml_path = path / "command.yaml"
    with yaml_path.open("w") as f:
        yaml.dump(data, f, default_flow_style=False)

def list_commands() -> list[dict]:
    """List all available commands."""
    if not COMMANDS_DIR.exists():
        return []
    commands = []
    for cmd_dir in COMMANDS_DIR.iterdir():
        if cmd_dir.is_dir():
            try:
                metadata = load_command_metadata(cmd_dir)
                if not metadata.get("disabled", False):
                    commands.append({
                        "name": metadata.get("name", cmd_dir.name),
                        "description": metadata.get("description", "No description"),
                        "path": cmd_dir
                    })
            except Exception as e:
                logger.warning(f"Error loading command {cmd_dir.name}: {e}")
    return commands

def import_command_module(command_name: str):
    """Dynamically import a command module."""
    cmd_dir = get_command_dir(command_name)
    py_path = cmd_dir / "command.py"
    if not py_path.exists():
        raise FileNotFoundError(f"Command implementation file not found: {py_path}")
    spec = importlib.util.spec_from_file_location(f"evai.commands.{command_name}", py_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module
Stop for Feedback: After implementing this step, verify that the module functions work as expected (e.g., get_command_dir creates directories, load_command_metadata reads YAML files correctly). Provide feedback before proceeding.
Step 2: Create Command Templates
Purpose: Define default templates for command.yaml and command.py, stored in evai/templates/, to streamline command creation.
Instructions:
Create evai/templates/sample_command.yaml with a metadata structure matching the tools' format but tailored for commands.
Create evai/templates/sample_command.py with a run(**kwargs) function, as required for dynamic registration.
Code:
evai/templates/sample_command.yaml:
yaml
name: "{command_name}"
description: "Default description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
evai/templates/sample_command.py:
python
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}
Stop for Feedback: Confirm that the templates are correctly placed and formatted. Test loading them manually to ensure they parse without errors. Provide feedback before moving on.
Step 3: Add commands Group and add Subcommand to cli.py
Purpose: Extend the CLI with a commands group and an add subcommand to create new commands, similar to tools add.
Instructions:
In evai/cli/cli.py, add a commands group using AliasedGroup.
Define an add subcommand that creates the command directory, saves default metadata, and writes the implementation file.
Reuse existing utilities from command_storage.py and adapt logic from tools.py.
Code:
Update evai/cli/cli.py:
python
# ... (existing imports remain unchanged)
from evai.command_storage import get_command_dir, save_command_metadata

# ... (existing cli group remains unchanged)

@cli.group(cls=AliasedGroup)
def commands():
    """Manage user-defined commands."""
    pass

@commands.command()
@click.argument("command_name")
def add(command_name):
    """Add a new custom command."""
    try:
        cmd_dir = get_command_dir(command_name)
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)

        # Load default metadata template
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.yaml"), "r") as f:
            metadata_content = f.read().replace("{command_name}", command_name)
            default_metadata = yaml.safe_load(metadata_content)

        # Save metadata
        save_command_metadata(cmd_dir, default_metadata)

        # Create default command.py
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.py"), "r") as f:
            with open(cmd_dir / "command.py", "w") as py_file:
                py_file.write(f.read())

        click.echo(f"Command '{command_name}' created successfully.")
        click.echo(f"- Metadata: {cmd_dir / 'command.yaml'}")
        click.echo(f"- Implementation: {cmd_dir / 'command.py'}")
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)

# ... (rest of cli.py remains unchanged, including import_commands and main)
Stop for Feedback: Test the evai commands add <command_name> command to ensure it creates the directory and files as expected. Verify the output messages and file contents. Provide feedback before proceeding.
Step 4: Dynamically Load and Register Commands in cli.py
Purpose: Load commands from ~/.evai/commands at startup and register them in Click under a user group.
Instructions:
Add a user group to cli.py for user-defined commands.
Implement a load_user_commands() function to scan ~/.evai/commands, create Click commands dynamically, and add them to the user group.
Use command_storage.py functions and Click's dynamic command creation capabilities.
Code:
Update evai/cli/cli.py:
python
# ... (add to existing imports)
from evai.command_storage import list_commands, import_command_module

# Type mapping for Click parameter types
TYPE_MAP = {
    "string": click.STRING,
    "integer": click.INT,
    "float": click.FLOAT,
    "boolean": click.BOOL,
}

# ... (existing cli and tools groups remain unchanged)

@cli.group(cls=AliasedGroup)
def user():
    """User-defined commands."""
    pass

def create_user_command(command_metadata: dict):
    """Create a Click command from command metadata."""
    command_name = command_metadata["name"]
    description = command_metadata.get("description", "")
    arg_names = [arg["name"] for arg in command_metadata.get("arguments", [])]

    def callback(*args, **kwargs):
        module = import_command_module(command_name)
        run_func = getattr(module, "run")
        params = dict(zip(arg_names, args))
        params.update(kwargs)
        result = run_func(**params)
        click.echo(result)

    command = click.command(name=command_name, help=description)(callback)

    # Add arguments
    for arg in command_metadata.get("arguments", []):
        command = click.argument(
            arg["name"],
            type=TYPE_MAP.get(arg.get("type", "string"), click.STRING)
        )(command)

    # Add options
    for opt in command_metadata.get("options", []):
        command = click.option(
            f"--{opt['name']}",
            type=TYPE_MAP.get(opt.get("type", "string"), click.STRING),
            help=opt.get("description", ""),
            required=opt.get("required", False),
            default=opt.get("default", None)
        )(command)

    return command

def load_user_commands():
    """Load and register user-defined commands from ~/.evai/commands."""
    commands_list = list_commands()
    for cmd_meta in commands_list:
        try:
            command = create_user_command(cmd_meta)
            user.add_command(command)
        except Exception as e:
            logger.warning(f"Failed to load command {cmd_meta['name']}: {e}")

# Call load_user_commands after defining groups
load_user_commands()

# ... (rest of cli.py remains unchanged)
Stop for Feedback: Run evai and check if the user group appears in the help output (evai --help). Create a test command and verify its listed under evai user --help and executable (e.g., evai user testcommand). Provide feedback before final integration.
Final Prompt for Sub-LLM
Below is a comprehensive prompt combining all steps, designed for a less intelligent sub-LLM. It includes explicit instructions, stresses specificity, and enforces discrete steps with feedback stops.
markdown
You are tasked with adding functionality to the EVAI CLI to allow users to create new commands, mirroring the existing tool creation process. The new commands will reside in `~/.evai/commands` and be loaded on EVAI startup, registered in Click like the existing command structure. Follow these **specific** steps EXACTLY as outlined, stopping after each step for feedback before proceeding. Do not skip steps or combine them. Assume the existing codebase (provided in context) is correct and must be extended without altering unrelated parts unless specified.

---

### Step 1: Create `command_storage.py` Module
- **Task**: Create a new file `evai/command_storage.py` to handle command file operations, similar to `tool_storage.py`.
- **Instructions**:
  - Use `pathlib.Path` for paths.
  - Define `COMMANDS_DIR = Path.home() / ".evai" / "commands"`.
  - Implement these functions:
    - `get_command_dir(command_name: str) -> Path`: Returns `COMMANDS_DIR / command_name`, creating it if needed.
    - `load_command_metadata(path: Path) -> dict`: Loads `command.yaml` from the path, raises FileNotFoundError if missing.
    - `save_command_metadata(path: Path, data: dict) -> None`: Saves dict to `command.yaml`.
    - `list_commands() -> list[dict]`: Lists all commands, skipping disabled ones, returns list of dicts with "name", "description", "path".
    - `import_command_module(command_name: str)`: Imports `command.py` from the command directory.
  - Include basic logging with `logger = logging.getLogger(__name__)`.
- **Code**:
```python
"""Command storage utilities for EVAI CLI."""

import os
import yaml
from pathlib import Path
import importlib.util
import logging

logger = logging.getLogger(__name__)

COMMANDS_DIR = Path.home() / ".evai" / "commands"

def get_command_dir(command_name: str) -> Path:
    """Get the directory path for a command and create it if it doesn't exist."""
    command_dir = COMMANDS_DIR / command_name
    command_dir.mkdir(parents=True, exist_ok=True)
    return command_dir

def load_command_metadata(path: Path) -> dict:
    """Load command metadata from command.yaml."""
    yaml_path = path / "command.yaml"
    if not yaml_path.exists():
        raise FileNotFoundError(f"Command metadata file not found: {yaml_path}")
    with yaml_path.open("r") as f:
        return yaml.safe_load(f) or {}

def save_command_metadata(path: Path, data: dict) -> None:
    """Save command metadata to command.yaml."""
    yaml_path = path / "command.yaml"
    with yaml_path.open("w") as f:
        yaml.dump(data, f, default_flow_style=False)

def list_commands() -> list[dict]:
    """List all available commands."""
    if not COMMANDS_DIR.exists():
        return []
    commands = []
    for cmd_dir in COMMANDS_DIR.iterdir():
        if cmd_dir.is_dir():
            try:
                metadata = load_command_metadata(cmd_dir)
                if not metadata.get("disabled", False):
                    commands.append({
                        "name": metadata.get("name", cmd_dir.name),
                        "description": metadata.get("description", "No description"),
                        "path": cmd_dir
                    })
            except Exception as e:
                logger.warning(f"Error loading command {cmd_dir.name}: {e}")
    return commands

def import_command_module(command_name: str):
    """Dynamically import a command module."""
    cmd_dir = get_command_dir(command_name)
    py_path = cmd_dir / "command.py"
    if not py_path.exists():
        raise FileNotFoundError(f"Command implementation file not found: {py_path}")
    spec = importlib.util.spec_from_file_location(f"evai.commands.{command_name}", py_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module
Stop: After implementing this, stop and wait for feedback. Test each function manually (e.g., create a directory, save/load YAML, list commands) and report any issues.
Step 2: Create Command Templates
Task: Add sample_command.yaml and sample_command.py to evai/templates/ for default command files.
Instructions:
Create evai/templates/sample_command.yaml with placeholders {command_name} where needed.
Create evai/templates/sample_command.py with a run(**kwargs) function.
Ensure the YAML includes arguments and options fields instead of params to distinguish from tools.
Code:
evai/templates/sample_command.yaml:
yaml
name: "{command_name}"
description: "Default description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
evai/templates/sample_command.py:
python
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}
Stop: Verify the templates are in evai/templates/ and can be read correctly. Provide feedback on their contents and placement.
Step 3: Add commands Group and add Subcommand to cli.py
Task: Extend evai/cli/cli.py with a commands group and add subcommand.
Instructions:
Add @cli.group(cls=AliasedGroup) for commands.
Add @commands.command() for add, taking a command_name argument.
Use get_command_dir, save_command_metadata, and file operations to create the command.
Check for existing commands and exit with an error if found.
Code:
python
# In evai/cli/cli.py, add to imports:
from evai.command_storage import get_command_dir, save_command_metadata

# Add after existing groups:
@cli.group(cls=AliasedGroup)
def commands():
    """Manage user-defined commands."""
    pass

@commands.command()
@click.argument("command_name")
def add(command_name):
    """Add a new custom command."""
    try:
        cmd_dir = get_command_dir(command_name)
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)

        # Load default metadata template
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.yaml"), "r") as f:
            metadata_content = f.read().replace("{command_name}", command_name)
            default_metadata = yaml.safe_load(metadata_content)

        # Save metadata
        save_command_metadata(cmd_dir, default_metadata)

        # Create default command.py
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.py"), "r") as f:
            with open(cmd_dir / "command.py", "w") as py_file:
                py_file.write(f.read())

        click.echo(f"Command '{command_name}' created successfully.")
        click.echo(f"- Metadata: {cmd_dir / 'command.yaml'}")
        click.echo(f"- Implementation: {cmd_dir / 'command.py'}")
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)
Stop: Test evai commands add testcommand and check if the files are created in ~/.evai/commands/testcommand/. Provide feedback on functionality and output.
Step 4: Dynamically Load and Register Commands in cli.py
Task: Update evai/cli/cli.py to load and register commands at startup under a user group.
Instructions:
Add a user group with @cli.group(cls=AliasedGroup).
Define TYPE_MAP for parameter types.
Implement create_user_command to generate Click commands from metadata.
Implement load_user_commands to register all commands.
Call load_user_commands() after group definitions.
Code:
python
# In evai/cli/cli.py, add to imports:
from evai.command_storage import list_commands, import_command_module

# Add before groups:
TYPE_MAP = {
    "string": click.STRING,
    "integer": click.INT,
    "float": click.FLOAT,
    "boolean": click.BOOL,
}

# Add after 'commands' group:
@cli.group(cls=AliasedGroup)
def user():
    """User-defined commands."""
    pass

def create_user_command(command_metadata: dict):
    """Create a Click command from command metadata."""
    command_name = command_metadata["name"]
    description = command_metadata.get("description", "")
    arg_names = [arg["name"] for arg in command_metadata.get("arguments", [])]

    def callback(*args, **kwargs):
        module = import_command_module(command_name)
        run_func = getattr(module, "run")
        params = dict(zip(arg_names, args))
        params.update(kwargs)
        result = run_func(**params)
        click.echo(result)

    command = click.command(name=command_name, help=description)(callback)

    # Add arguments
    for arg in command_metadata.get("arguments", []):
        command = click.argument(
            arg["name"],
            type=TYPE_MAP.get(arg.get("type", "string"), click.STRING)
        )(command)

    # Add options
    for opt in command_metadata.get("options", []):
        command = click.option(
            f"--{opt['name']}",
            type=TYPE_MAP.get(opt.get("type", "string"), click.STRING),
            help=opt.get("description", ""),
            required=opt.get("required", False),
            default=opt.get("default", None)
        )(command)

    return command

def load_user_commands():
    """Load and register user-defined commands from ~/.evai/commands."""
    commands_list = list_commands()
    for cmd_meta in commands_list:
        try:
            command = create_user_command(cmd_meta)
            user.add_command(command)
        except Exception as e:
            logger.warning(f"Failed to load command {cmd_meta['name']}: {e}")

# Add after group definitions, before import_commands():
load_user_commands()
Stop: Run evai --help and evai user --help to verify the user group and loaded commands. Test a command (e.g., evai user testcommand) and provide feedback on registration and execution.
Final Notes: After completing all steps, the EVAI CLI will support command creation via evai commands add and load them at startup under the user group. Provide final feedback to confirm the implementation meets all requirements.
Please proceed with Step 1 and stop for feedback.
```

================
File: evai/docs/prompt_plan.md
================
Below is a comprehensive plan that evolves in multiple iterations, starting from a high-level blueprint, then breaking the tasks into more detailed, smaller steps until each part can be implemented safely and cleanly. Finally, you will find a collection of prompts that you can feed into a code-generation LLM (e.g., ChatGPT, Claude, etc.) to implement each step in a test-driven manner, ensuring no large leaps in complexity.

1. High-Level Blueprint
	1.	Initialize Core Project Structure
		Create a Python package or directory (evai/) for the CLI.
		Set up a minimal setup.py or a pyproject.toml if needed.
		Create a basic CLI entry point (evai) using argparse or click.
	2.	Implement Command File Management
		Define a directory structure under ~/.evai/commands/<command-name>/.
		Specify the required YAML file (command.yaml) and Python file (command.py).
		Implement a utility for reading, writing, and validating command metadata from YAML.
	3.	Add Command Creation Workflow
		Implement evai command add <command-name> to:
	1.	Generate default YAML metadata.
	2.	Prompt user to review and edit the metadata in $EDITOR.
	3.	Prompt user to implement the Python code in $EDITOR.
	4.	Perform post-editing validation (YAML schema check, flake8 lint check).
	5.	Handle re-editing if any validation fails.
	4.	MCP Integration
		Implement a minimal MCP server that picks up all commands from ~/.evai/commands/*/command.yaml.
		Expose each command at an endpoint, e.g., /commands/<command-name>.
	5.	LLM Interaction
		(Optional step, but included for completeness.)
		Include a submodule for LLM calls (e.g., Anthropic or OpenAI).
		Hook LLM calls when the commands llm_interaction metadata is enabled.
	6.	Testing, Linting, and Validation
		Add unit tests for YAML metadata reading/writing.
		Add integration tests ensuring commands are recognized by evai and the MCP server.
		Validate the user editing/linting cycle with flake8 and a basic YAML schema check.
	7.	Deployment / Packaging
		Confirm everything runs with a local install.
		(Optionally) Release to PyPI or a local environment.

2. Breaking the Blueprint into Iterative Chunks

Below is a more detailed breakdown of the same blueprint, with each chunk building on the previous chunk.
	1.	Chunk 1: Project Scaffolding
		Set up basic Python project.
		Create minimal CLI entry point using argparse (or your chosen CLI library).
		Verify evai can run and display a help message.
	2.	Chunk 2: Command Directory & YAML Management
		Implement a helper function to locate ~/.evai/commands/<command-name>/.
		Implement reading/writing YAML metadata (using PyYAML or equivalent).
		Create unit tests to verify metadata read/write.
	3.	Chunk 3: command add Workflow (Initial Version)
		Implement evai command add <command-name> that:
		Creates the directory structure.
		Generates a default command.yaml.
		Generates a stub command.py.
		Writes minimal content.
		Add basic tests.
	4.	Chunk 4: Interactive Editing
		Add $EDITOR invocation to let the user edit the YAML.
		Validate YAML structure after edit.
		If validation fails, keep re-invoking $EDITOR.
	5.	Chunk 5: Implementation Editing & Lint Checking
		Prompt user to open command.py in $EDITOR.
		Validate Python code via flake8.
		If linting fails, re-prompt user.
	6.	Chunk 6: Command Execution & MCP Exposure
		Make the evai CLI able to list commands.
		Add a minimal embedded MCP server.
		Expose each command at a route for remote invocation.
		Write tests verifying command execution from CLI and from MCP.
	7.	Chunk 7: Optional LLM Interaction
		Integrate an LLM client to propose default metadata.
		If llm_interaction.enabled is true, call the LLM from within the command logic.
		Add tests for the LLM interaction path.
	8.	Chunk 8: Polishing & Final Testing
		Write final integration tests and ensure everything is consistent.
		Confirm TDD with all tests passing.

3. Breaking Chunks into Smaller Steps

Below is another iteration, splitting each chunk into smaller, incremental steps that are feasible to implement safely, with strong testing at each stage.
	1.	Chunk 1: Project Scaffolding
	1.	Create evai/ folder and basic Python package structure.
	2.	Add __init__.py.
	3.	Add cli.py using argparse or click.
	4.	Implement a placeholder main() that prints a version or help.
	5.	Add a simple test (test_cli.py) that checks for the printed help message.
	6.	Confirm the test passes.
	2.	Chunk 2: Command Directory & YAML Management
	1.	Implement a function: get_command_dir(command_name) to build ~/.evai/commands/<command-name>.
	2.	Implement load_command_metadata(path) and save_command_metadata(path, data).
	3.	Use PyYAML to parse and write metadata.
	4.	Write tests (test_metadata.py) for each function to ensure correct directory resolution and file read/write.
	3.	Chunk 3: command add Workflow (Initial Version)
	1.	Add subcommand: evai command add <command-name>.
	2.	Within that, create the command directory.
	3.	Generate default YAML metadata (in memory).
	4.	Generate a stub Python implementation file.
	5.	Save the files to disk.
	6.	Test this by verifying the directory and files exist.
	4.	Chunk 4: Interactive Editing
	1.	After stub files are created, call $EDITOR to open command.yaml.
	2.	On save, parse the updated YAML.
	3.	If invalid, print an error and reopen $EDITOR.
	4.	Write tests to simulate successful and failing edits (possibly mocking $EDITOR calls).
	5.	Chunk 5: Implementation Editing & Lint Checking
	1.	Prompt user to open command.py in $EDITOR.
	2.	Run flake8 programmatically on command.py.
	3.	If lint errors exist, prompt the user to re-edit.
	4.	Test with a good Python file and a file with a known lint error.
	6.	Chunk 6: Command Execution & MCP Exposure
	1.	In the CLI, add evai command list to show available commands.
	2.	Add evai command run <command-name> --param1=... --param2=... to execute the command directly.
	3.	Implement a minimal MCP server that runs in a background thread or in the main thread, listing available commands.
	4.	Test it by calling the CLI subcommands, verifying output.
	7.	Chunk 7: Optional LLM Interaction
	1.	Implement a function generate_default_metadata_with_llm(command_name) that queries an LLM.
	2.	Use the response to populate default command metadata.
	3.	Add tests that mock the LLM client, ensuring fallback if the LLM is unreachable.
	8.	Chunk 8: Polishing & Final Testing
	1.	Add final integration tests across all subcommands and flows.
	2.	Ensure TDD coverage is near 100%.
	3.	Validate final usage from a typical user perspective (UAT).

4. Prompts for a Code-Generation LLM (Test-Driven Implementation)

Below is a set of sequential prompts you can feed into a code-generation LLM (such as ChatGPT, Claude, etc.). Each prompt is self-contained but references the outputs from prior steps. They are designed to walk through implementing the entire project in small, test-driven increments. You would copy each prompt (as a single code block) into your LLM, let it generate the code or confirm it, then proceed to the next prompt. The final prompt wires everything together, ensuring no hanging or orphaned code.

Prompt 1: Project Scaffolding

You are implementing an EVAI CLI in Python. In this step, create a minimal project scaffold:

- Create a folder structure: `evai/` with an `__init__.py`.
- Add a `cli.py` that uses `argparse` and has a `main()` function. 
- When `python -m evai.cli` is run, it should print a version or help text. 
- Create a test file `tests/test_cli.py` that checks the CLI prints some expected text (like "EVAI CLI version 0.1").

Please:
1. Write the code for `evai/__init__.py`.
2. Write the code for `evai/cli.py`.
3. Write the `tests/test_cli.py`.
4. Include instructions (a shell command or two) on how to run these tests with `pytest`.
Use best practices, including a `if __name__ == "__main__": main()` guard in `cli.py`.

Prompt 2: Command Directory & YAML Management

Building on the previous code, add functionality to manage a command repository under the users home directory:

1. Implement a function `get_command_dir(command_name)` in a new file `evai/command_storage.py` that returns the path: `~/.evai/commands/<command_name>`. It should create the directory if it doesnt exist.
2. Implement `load_command_metadata(path) -> dict` and `save_command_metadata(path, data: dict) -> None` in the same file. Use PyYAML to parse/write YAML to a file named `command.yaml` in that directory.
3. Update your test suite in a new file `tests/test_metadata.py` to test these three functions.
4. Provide a final updated tree structure of the project, including new files, and the commands to run tests.

Make sure tests pass and we follow best practices for Python code.

Prompt 3: command add <command-name> Workflow (Initial Version)

Extend the CLI with a subcommand: `evai command add <command-name>`.

1. In `cli.py`, add a subcommand group `command` with a subcommand `add`.
2. When called, it should:
   - Use `get_command_dir` to find/create the directory.
   - Create a default Python file `command.py` with a simple `def run(**kwargs): print("Hello World")`.
   - Create a default YAML file `command.yaml` with fields:
     ```
     name: <command-name>
     description: "Default description"
     params: []
     hidden: false
     disabled: false
     mcp_integration:
       enabled: true
       metadata:
         endpoint: ""
         method: "POST"
         authentication_required: false
     llm_interaction:
       enabled: false
       auto_apply: true
       max_llm_turns: 15
     ```
3. Write tests in `tests/test_add_command.py` verifying that after running `evai command add <command-name>`, the directory is created with the above files containing the correct content.

Provide the updated code changes and the test code.

Prompt 4: Interactive Editing of command.yaml

Add interactive editing support for `command.yaml`:

1. After creating the default metadata, invoke an editor for the user to review/edit. Respect the environment variable `$EDITOR`, or default to `vi` if `$EDITOR` is not set.
2. When the user saves and exits, parse the YAML again. If invalid, re-open the editor until the user fixes it or chooses to abort.
3. In `tests/test_add_command.py`, add a test that mocks the editor call to simulate user changes. For the real editor invocation, you can use a subprocess call.

Provide updated code. Include any new helper functions and test code. Ensure the user can exit gracefully if they want to abort.

Prompt 5: Implementation Editing & Lint Checking

Now add a similar interactive editing step for `command.py`. Then perform a lint check:

1. Once `command.yaml` is finalized, open `command.py` in the editor.
2. After the user saves, run `flake8` (or a Python wrapper around it). 
3. If errors are found, display them and re-open the editor. 
4. Allow user to abort if they cant fix the issues.
5. Create `tests/test_edit_implementation.py` that checks if a file with a known lint error triggers a re-edit.
6. Provide updated code for everything needed, including how you handle the lint check programmatically in Python.

Prompt 6: Command Execution & MCP Exposure

Next, enable listing and executing commands, and expose them via an MCP server:

1. Add `evai command list` to scan `~/.evai/commands` and print available command names.
2. Add `evai command run <command-name>` with optional `--param key=value` pairs. This should:
   - Load `command.py` dynamically (e.g., via importlib).
   - Call the `run(**kwargs)`.
3. Implement a minimal MCP server that reads all `command.yaml` files, and for each command with `mcp_integration.enabled=True`, expose a route like `/commands/<command-name>`. 
   - The route handles a POST request with JSON data. 
   - Translate that data into keyword args for `run()`.
4. Add tests: `test_list_and_run.py` for the CLI behaviors, `test_mcp_exposure.py` for verifying the MCP routes respond correctly. 
5. Provide updated code and instructions on how to run the MCP server for local testing.

Prompt 7: Optional LLM Interaction

Implement optional LLM interaction for default metadata generation and in-command usage:

1. Add a function `generate_default_metadata_with_llm(command_name)` that makes a mock or real LLM call, returning YAML metadata. If the call fails, use standard defaults.
2. Modify `evai command add <command-name>` so that it offers to call `generate_default_metadata_with_llm` for initial metadata if `llm_interaction.enabled` is set.
3. In command execution, if `llm_interaction.enabled=True`, optionally call an LLM inside `command.py` if the user so desires. This is not strictly necessary but can be tested with a mock LLM.
4. Add or update tests in `test_llm.py`, ensuring that if the LLM is unreachable, we gracefully revert to basic defaults.

Return the updated code, focusing on minimal or mock LLM calls, plus the new/updated tests.

Prompt 8: Final Polishing & Integration Tests

Time to finalize and polish:

1. Add final integration tests that create a command, edit it, lint it, execute it, run it from MCP, etc.
2. Ensure best practices: handle exceptions gracefully, provide clear error messages.
3. Confirm TDD with all tests passing.
4. Provide instructions for running the entire test suite, manually verifying the workflow, and any final details needed to consider this project complete.

Return the final integrated code. 

These prompts are designed to build the EVAI CLI Custom Commands Integration in small, test-driven increments, ensuring each step is validated before moving on.

================
File: evai/docs/todo.md
================
# TODO: Implementation Steps for EVAI CLI Custom Commands Integration

A comprehensive, step-by-step checklist to guide the development of the EVAI CLI, command storage, and MCP integration. Mark each step as completed once done.

---

## 1. Project Scaffolding
- [X] **Create Project Structure**  
  - [X] Create `__init__.py` inside `evai/`.
  - [X] Create `cli.py` with a basic `main()` function.
- [X] **Command-Line Entry Point**  
  - [X] Decide on `argparse`, `click`, or similar.
  - [X] Implement minimal CLI that prints help/version info.
- [X] **Basic Testing**  
  - [X] Create `tests/` folder with a simple test (`test_cli.py`).
  - [X] Verify that invoking CLI with `python -m evai.cli` works.

---

## 2. Command Directory & YAML Management
- [X] **Utility Functions**  
  - [X] `get_command_dir(command_name)`: Return (and create if needed) `~/.evai/commands/<command-name>/`.
  - [X] `load_command_metadata(path) -> dict`: Load YAML from `command.yaml`.
  - [X] `save_command_metadata(path, data: dict) -> None`: Write YAML data to `command.yaml`.
- [X] **Testing**  
  - [X] Write `test_metadata.py` to cover file I/O, YAML parse/write, and directory creation.
  - [X] Confirm that all tests pass before proceeding.

---

## 3. `command add` Workflow (Initial Version)
- [X] **CLI Subcommand**  
  - [X] Implement `evai command add <command-name>` in `cli.py`.
- [X] **File Creation**  
  - [X] Within this subcommand, create default `command.yaml` with standard fields and placeholders.
  - [X] Create default `command.py` with a stub `def run(**kwargs): print("Hello World")`.
- [X] **Testing**  
  - [X] In `test_add_command.py`, confirm the directory and files are created with correct content.

---

## 4. Editing of `command.yaml`
- [X] **Non-interactive editing**
- [X] **Environment Variable `$EDITOR`**  
  - [X] Check `$EDITOR`; default to `vi` if not set.
- [X] **Editing Loop**
  - [X] Open the newly created `command.yaml` in the editor.
  - [X] Parse YAML on save.  
  - [X] If invalid, prompt user to fix it. Offer to abort if needed.
- [X] **Testing**  
  - [X] Mock the editor call in `test_add_command.py` or a new test file to simulate user edits.
  - [X] Validate re-editing behavior on invalid YAML.

---

## 5. Implementation Editing & Lint Checking
- [X] **Editor Invocation**  
  - [X] Open `command.py` in `$EDITOR`.
- [X] **Lint Check**  
  - [X] Run `flake8` on `command.py` programmatically.
  - [X] If lint errors, show them to the user and re-open editor.
  - [X] Offer user the option to abort if unresolved.
- [X] **Testing**  
  - [X] Create `test_edit_implementation.py` to ensure a known lint error triggers re-edit.
  - [X] Confirm passing code requires no further edits.

---

## 6. Command Execution & MCP Exposure
- [X] **List & Run**  
  - [X] `evai command list`: Scan `~/.evai/commands` and list command names.
  - [X] `evai command run <command-name>`: Dynamically import `command.py` and call `run(**kwargs)`.
- [X] **MCP Server**  
  - [X] Create a minimal server that:
    - [X] Scans `command.yaml` files.
    - [X] For each command with `mcp_integration.enabled = true`, exposes `/commands/<command-name>` (POST).
    - [X] Executes corresponding `run()` with JSON data as kwargs.
- [X] **Testing**  
  - [X] `test_list_and_run.py`: CLI tests for listing and running commands.
  - [X] `test_mcp_exposure.py`: Confirm commands are reachable via MCP server, verifying correct endpoints and data flow.

---

## 7. LLM Interaction - Use OpenAI API
- [X] Create new CLI command `evai command llmadd <command-name>`
- [X] Query user for command description
- [X] Call LLM with description and name to see if additional information is needed, and if so, display the response to the user and allow them to provide additional details.
- [X] Build templates for metadata and implementation
- [X] **LLM for Default Metadata**  
  - [X] `generate_default_metadata_with_llm(command_name)`: Populate YAML metadata from an LLM.
  - [X] Fallback to basic defaults if LLM is unreachable or disabled.
- [X] **LLM for Command Implementation**  
  - [X] If `llm_interaction.enabled` is `true`, integrate a call to the LLM inside `command.py` logic as needed.
- [X] **Testing**  
  - [X] `test_llm.py`: Mock the LLM.  
  - [X] Ensure graceful fallback on LLM failures.

---

## 8. Final Polishing & Integration Tests
- [ ] **Integration Testing**  
  - [ ] Create tests that simulate full workflow: adding a command, editing files, lint checking, executing via CLI, executing via MCP.
  - [ ] Ensure robust error handling for each step.
- [ ] **User Acceptance**  
  - [ ] Confirm typical user can navigate the CLI steps easily.
  - [ ] Ensure error messaging is clear and user-friendly.
- [ ] **Final Validation**  
  - [ ] Run entire test suite.  
  - [ ] Verify all features are consistent with requirements.  
  - [ ] Document usage for future maintainers.

---

## 9. Release or Local Deployment
- [ ] **Installation**  
  - [ ] Confirm local install with `pip install .` or equivalent works.
- [ ] **Packaging**  
  - [ ] (Optional) Publish to PyPI if desired, or maintain locally.
- [ ] **Documentation**  
  - [ ] Provide a README with usage, environment variables, and known limitations.

---

**End of Checklist**

================
File: evai/docs/tool_positional_args_todo.md
================
# Tool Positional Arguments Implementation

## Task Description
Modify the `run_tool` function to handle command-line style parameters for tool functions. The current implementation expects a dictionary of keyword arguments, but we need to adapt it to work with positional arguments from the command line.

For example, running `evai tools run subtract 8 5` should run the `tool_subtract` method in the subtract tool with `8` as the first parameter and `5` as the second parameter.

## Implementation Plan
[X] Modify the `run_tool` function in `tool_storage.py` to accept positional arguments
[X] Update the `run` command in `tools.py` to handle positional arguments
[X] Create tests to verify the changes
[X] Run the tests to ensure everything works correctly

## Changes Made

### 1. Modified `run_tool` function in `tool_storage.py`
- Added support for positional arguments using `*args`
- Added type conversion based on function signature
- Maintained backward compatibility with keyword arguments

### 2. Updated `run` command in `tools.py`
- Added support for positional arguments using Click's `nargs=-1` parameter
- Maintained backward compatibility with `--param` option
- Updated the help text to explain both usage patterns

### 3. Created tests
- Created unit tests for the `run_tool` function
- Created integration tests for the CLI

## Testing
All tests are passing, which verifies that:
- Positional arguments work correctly
- Keyword arguments still work (backward compatibility)
- Mixed arguments raise an appropriate error

## Example Usage
```bash
# Using positional arguments
evai tools run subtract 8 5
# Result: 3.0

# Using keyword arguments (backward compatibility)
evai tools run subtract --param minuend=8 --param subtrahend=5
# Result: 3.0
```

================
File: evai/mcp/mcp_prompts.py
================
"""MCP prompts for EVAI CLI."""

import logging
import os
from typing import Dict, Any, List

try:
    from mcp.server.fastmcp import FastMCP
    import mcp.types as types
    from mcp.types import PromptMessage
except ImportError:
    # Provide a helpful error message if MCP is not installed
    raise ImportError(
        "The MCP Python SDK is required for MCP server integration. "
        "Please install it with: pip install mcp"
    )

# Set up logging
logger = logging.getLogger(__name__)

# Define available prompts
PROMPTS = {
    "git-commit": types.Prompt(
        name="git-commit",
        description="Generate a Git commit message",
        arguments=[
            types.PromptArgument(
                name="changes",
                description="Git diff or description of changes",
                required=True
            )
        ],
    ),
    "explain-code": types.Prompt(
        name="explain-code",
        description="Explain how code works",
        arguments=[
            types.PromptArgument(
                name="code",
                description="Code to explain",
                required=True
            ),
            types.PromptArgument(
                name="language",
                description="Programming language",
                required=False
            )
        ],
    )
}


def register_prompts(mcp: FastMCP, server) -> None:
    """
    Register all available prompts.
    
    Args:
        mcp: The MCP server instance
        server: The EVAIServer instance for file reading
    """
    logger.debug("Registering prompts")
    
    # Register the analyze-file prompt
    @mcp.prompt(name="analyze-file", description="Analyze a file")
    async def analyze_file(path: str) -> list[PromptMessage]:
        """
        Analyze a file and provide insights.
        
        Args:
            path: Path to the file to analyze
            
        Returns:
            A list of prompt messages
        """
        logger.debug(f"Analyzing file: {path}")
        try:
            # Read the file
            content = server.read_file(path)
            
            # Return the file content as a prompt message
            return [PromptMessage(role="user", content=f"Please analyze this file:\n\n```\n{content}\n```")]
        except Exception as e:
            logger.error(f"Error analyzing file: {e}")
            return [PromptMessage(role="user", content=f"Error analyzing file: {e}")]
    
    logger.debug("Prompts registered successfully")

================
File: evai/mcp/mcp_server.py
================
"""MCP server integration for EVAI CLI."""

import os
import sys
import logging
import json
import subprocess
import importlib.util
from typing import Dict, Any, List, Optional, Tuple

try:
    from mcp.server.fastmcp import FastMCP, Context
    import mcp.types as types
    from mcp.types import PromptMessage
except ImportError:
    # Provide a helpful error message if MCP is not installed
    raise ImportError(
        "The MCP Python SDK is required for MCP server integration. "
        "Please install it with: pip install mcp"
    )

# Add the parent directory to sys.path
# sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from evai.tool_storage import (
    list_tools, 
    run_tool, 
    load_tool_metadata, 
    get_tool_dir,
    save_tool_metadata,
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    import_tool_module
)

# Import the new modules
from evai.mcp.mcp_prompts import register_prompts
from evai.mcp.mcp_tools import register_built_in_tools, register_tools, register_tool

# Set up logging
logger = logging.getLogger(__name__)
mcp = FastMCP("evai")


class EVAIServer:
    """MCP server for EVAI CLI custom tools."""
    
    def __init__(self, mcp: FastMCP):
        """
        Initialize the MCP server.
        
        Args:
            name: The name of the server
        """
        print(f"[DEBUG] Entering EVAIServer.__init__ with name=evai", file=sys.stderr)
        self.name = "evai"
        self.mcp = mcp
        self.tools = {}
        
        # Use the new modules for registration
        register_built_in_tools(self.mcp)
        register_prompts(self.mcp, self)
        register_tools(self.mcp)
        
        print(f"[DEBUG] Exiting EVAIServer.__init__", file=sys.stderr)
    
    def read_file(self, path: str) -> str:
        """Read a file and return its contents."""
        with open(path, "r") as f:
            return f.read()
    
    def run(self) -> None:
        """Run the MCP server."""
        print(f"[DEBUG] Entering EVAIServer.run", file=sys.stderr)
        try:
            # Start the server
            self.mcp.run()
        except KeyboardInterrupt:
            print("Server stopped by user.")
        except Exception as e:
            logger.error(f"Error running MCP server: {e}")
            print(f"Error running MCP server: {e}", file=sys.stderr)
        print(f"[DEBUG] Exiting EVAIServer.run", file=sys.stderr)


def create_server(name: str = "EVAI Tools") -> EVAIServer:
    """
    Create an MCP server for EVAI CLI custom tools.
    
    Args:
        name: The name of the server
        
    Returns:
        The MCP server
    """
    print(f"[DEBUG] Entering create_server with name={name}", file=sys.stderr)
    server = EVAIServer(mcp)
    print(f"[DEBUG] Exiting create_server", file=sys.stderr)
    return server


def run_server(name: str = "EVAI Tools") -> None:
    """
    Run an MCP server for EVAI CLI custom tools.
    
    Args:
        name: The name of the server
    """
    print(f"[DEBUG] Entering run_server with name={name}", file=sys.stderr)
    server = create_server(name)
    server.run()
    print(f"[DEBUG] Exiting run_server", file=sys.stderr)

server = EVAIServer(mcp)

================
File: evai/mcp/mcp_tools.py
================
"""MCP tools for EVAI CLI."""

import os
import sys
import logging
import inspect
from typing import Dict, Any, List, Optional

try:
    from mcp.server.fastmcp import FastMCP
except ImportError:
    # Provide a helpful error message if MCP is not installed
    raise ImportError(
        "The MCP Python SDK is required for MCP server integration. "
        "Please install it with: pip install mcp"
    )

from evai.tool_storage import (
    list_tools, 
    run_tool, 
    load_tool_metadata, 
    get_tool_dir,
    save_tool_metadata,
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    import_tool_module
)

# Set up logging
logger = logging.getLogger(__name__)


def register_built_in_tools(mcp: FastMCP) -> None:
    """
    Register built-in tools like tool creation.
    
    Args:
        mcp: The MCP server instance
    """
    logger.debug("Registering built-in tools")
    
    @mcp.tool(name="list_tools")
    def list_available_tools() -> Dict[str, Any]:
        """
        List all available tools.
        
        Returns:
            A dictionary with the list of available tools
        """
        logger.debug("Listing available tools")
        try:
            tools_list = list_tools()
            logger.debug(f"Found {len(tools_list)} tools")
            return {
                "status": "success",
                "tools": tools_list
            }
        except Exception as e:
            logger.error(f"Error listing tools: {e}")
            return {"status": "error", "message": str(e)}
    
    @mcp.tool(name="edit_tool_implementation")
    def edit_tool_implementation_tool(tool_name: str, implementation: str) -> Dict[str, Any]:
        """
        Edit the implementation of an existing tool.
        
        Args:
            tool_name: The name of the tool to edit
            implementation: The new implementation code
            
        Returns:
            A dictionary with the status of the edit
        """
        logger.debug(f"Editing implementation for tool: {tool_name}")
        try:
            # Get the tool directory
            tool_dir = get_tool_dir(tool_name)
            
            # Check if tool exists
            tool_py_path = os.path.join(tool_dir, "tool.py")
            if not os.path.exists(tool_py_path):
                logger.error(f"Tool '{tool_name}' does not exist")
                return {"status": "error", "message": f"Tool '{tool_name}' does not exist"}
            
            # Write the new implementation
            with open(tool_py_path, "w") as f:
                f.write(implementation)
            
            # Check if the tool is already registered
            try:
                # Re-import the module to update the implementation
                from importlib import reload
                import sys
                
                # Get the module name
                module_name = f"evai.tools.{tool_name}"
                
                # If the module is already loaded, reload it
                if module_name in sys.modules:
                    reload(sys.modules[module_name])
                    
                logger.info(f"Reloaded implementation for tool '{tool_name}'")
            except Exception as e:
                logger.warning(f"Failed to reload implementation for tool '{tool_name}': {e}")
            
            result = {
                "status": "success",
                "message": f"Implementation for tool '{tool_name}' updated successfully",
                "implementation_path": tool_py_path
            }
            logger.debug(f"Successfully edited implementation for tool: {tool_name}")
            return result
            
        except Exception as e:
            logger.error(f"Error editing tool implementation: {e}")
            return {"status": "error", "message": str(e)}
            
    @mcp.tool(name="edit_tool_metadata")
    def edit_tool_metadata_tool(tool_name: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Edit the metadata of an existing tool.
        
        Args:
            tool_name: The name of the tool to edit
            metadata: The new metadata
            
        Returns:
            A dictionary with the status of the edit
        """
        logger.debug(f"Editing metadata for tool: {tool_name}")
        try:
            # Get the tool directory
            tool_dir = get_tool_dir(tool_name)
            
            # Check if tool exists
            tool_yaml_path = os.path.join(tool_dir, "tool.yaml")
            if not os.path.exists(tool_yaml_path):
                logger.error(f"Tool '{tool_name}' does not exist")
                return {"status": "error", "message": f"Tool '{tool_name}' does not exist"}
            
            # Ensure the name field matches the tool_name
            metadata["name"] = tool_name
            
            # Save the metadata
            save_tool_metadata(tool_dir, metadata)
            
            result = {
                "status": "success",
                "message": f"Metadata for tool '{tool_name}' updated successfully",
                "metadata_path": tool_yaml_path
            }
            logger.debug(f"Successfully edited metadata for tool: {tool_name}")
            return result
            
        except Exception as e:
            logger.error(f"Error editing tool metadata: {e}")
            return {"status": "error", "message": str(e)}
    
    logger.debug("Built-in tools registered successfully")


def register_tools(mcp: FastMCP) -> None:
    """
    Register all available tools.
    
    Args:
        mcp: The MCP server instance
    """
    logger.debug("Registering custom tools")
    
    try:
        # Get all available tools
        tools = list_tools()
        
        # Register each tool
        for tool in tools:
            tool_name = tool["name"]
            tool_dir = tool["path"]
            
            try:
                # Load the tool metadata
                metadata = load_tool_metadata(tool_dir)
                
                # Register the tool
                register_tool(mcp, tool_name, metadata)
                
            except Exception as e:
                logger.error(f"Error registering tool '{tool_name}': {e}")
        
        logger.debug(f"Registered {len(tools)} custom tools")
        
    except Exception as e:
        logger.error(f"Error registering tools: {e}")
    

def register_tool(mcp: FastMCP, tool_name: str, metadata: Dict[str, Any]) -> None:
    """
    Register a tool as an MCP tool.
    
    Args:
        mcp: The MCP server instance
        tool_name: The name of the tool
        metadata: The tool metadata
    """
    logger.debug(f"Registering tool: {tool_name}")
    
    try:
        # Import the tool module using the existing function
        module = import_tool_module(tool_name)
        
        # Find any function that starts with 'tool_'
        tool_functions = [
            name for name, obj in inspect.getmembers(module)
            if inspect.isfunction(obj) and name.startswith('tool_')
        ]
        
        if not tool_functions:
            raise AttributeError(f"Tool module doesn't have any tool_* functions")
        
        # Use the first tool function found
        tool_function_name = tool_functions[0]
        logger.debug(f"Found tool function: {tool_function_name}")
        
        # Get the tool function and register it with MCP
        tool_function = getattr(module, tool_function_name)
        mcp.tool(name=tool_name)(tool_function)
        
        logger.debug(f"Successfully registered tool: {tool_name}")
    except Exception as e:
        logger.error(f"Error registering tool '{tool_name}': {e}")
        raise

================
File: evai/templates/sample_command.py
================
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}

================
File: evai/templates/sample_command.yaml
================
name: "{command_name}"
description: "Default description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15

================
File: evai/templates/sample_tool.py
================
"""Custom tool implementation."""

def tool_echo(echo_string: str) -> str:
    """Run the echo tool with the given arguments."""
    
    return echo_string

================
File: evai/templates/sample_tool.yaml
================
name: {tool_name}
description: Default description
params: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15

================
File: evai/__init__.py
================
"""EVAI CLI - Command-line interface for EVAI."""

__version__ = "0.1.0"

================
File: evai/command_storage.py
================
"""Command storage utilities for EVAI CLI."""

import os
import yaml
from pathlib import Path
import importlib.util
import inspect
import logging

logger = logging.getLogger(__name__)

COMMANDS_DIR = Path.home() / ".evai" / "commands"

def get_command_dir(command_name: str) -> Path:
    """Get the directory path for a command and create it if it doesn't exist."""
    command_dir = COMMANDS_DIR / command_name
    command_dir.mkdir(parents=True, exist_ok=True)
    return command_dir

def load_command_metadata(path: Path) -> dict:
    """Load command metadata from command.yaml."""
    yaml_path = path / "command.yaml"
    if not yaml_path.exists():
        raise FileNotFoundError(f"Command metadata file not found: {yaml_path}")
    with yaml_path.open("r") as f:
        return yaml.safe_load(f) or {}

def save_command_metadata(path: Path, data: dict) -> None:
    """Save command metadata to command.yaml."""
    yaml_path = path / "command.yaml"
    with yaml_path.open("w") as f:
        yaml.dump(data, f, default_flow_style=False)

def list_commands() -> list[dict]:
    """List all available commands."""
    if not COMMANDS_DIR.exists():
        return []
    commands = []
    for cmd_dir in COMMANDS_DIR.iterdir():
        if cmd_dir.is_dir():
            try:
                metadata = load_command_metadata(cmd_dir)
                if not metadata.get("disabled", False):
                    commands.append({
                        "name": metadata.get("name", cmd_dir.name),
                        "description": metadata.get("description", "No description"),
                        "path": cmd_dir
                    })
            except Exception as e:
                logger.warning(f"Error loading command {cmd_dir.name}: {e}")
    return commands

def import_command_module(command_name: str):
    """Dynamically import a command module."""
    cmd_dir = get_command_dir(command_name)
    py_path = cmd_dir / "command.py"
    if not py_path.exists():
        raise FileNotFoundError(f"Command implementation file not found: {py_path}")
    spec = importlib.util.spec_from_file_location(f"evai.commands.{command_name}", py_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

def run_command(command_name: str, *args, **kwargs):
    """Run a command with the given arguments."""
    try:
        # Import the command module
        module = import_command_module(command_name)
        
        # Check if the module has a run function
        if not hasattr(module, "run"):
            raise AttributeError(f"Command '{command_name}' does not have a run function")
        
        run_func = getattr(module, "run")
        
        # Get the function signature
        sig = inspect.signature(run_func)
        
        # Check if we're using args or kwargs
        if args and len(args) > 0:
            # Convert positional args to kwargs based on metadata
            cmd_dir = get_command_dir(command_name)
            metadata = load_command_metadata(cmd_dir)
            arg_names = [arg["name"] for arg in metadata.get("arguments", [])]
            
            # Map positional args to named args
            if len(args) > len(arg_names):
                raise ValueError(f"Too many arguments provided. Expected: {len(arg_names)}, Got: {len(args)}")
                
            # Create kwargs from positional args
            for i, arg in enumerate(args):
                if i < len(arg_names):
                    kwargs[arg_names[i]] = arg
            
            # Run with kwargs
            return run_func(**kwargs)
        else:
            # Run with kwargs
            return run_func(**kwargs)
    except Exception as e:
        logger.error(f"Error running command: {e}")
        raise

def remove_command(command_name: str) -> None:
    """Remove a command directory and its files."""
    import shutil
    cmd_dir = get_command_dir(command_name)
    if not cmd_dir.exists():
        raise FileNotFoundError(f"Command '{command_name}' not found")
    shutil.rmtree(cmd_dir)

================
File: evai/llm_client.py
================
"""LLM client for EVAI CLI."""

import os
import logging
import json
import yaml
from typing import Dict, Any, Optional, Tuple, List
import time

# Set up logging
logger = logging.getLogger(__name__)

# Default metadata template
DEFAULT_METADATA = {
    "name": "",
    "description": "",
    "params": [],
    "hidden": False,
    "disabled": False,
    "mcp_integration": {
        "enabled": True,
        "metadata": {
            "endpoint": "",
            "method": "POST",
            "authentication_required": False
        }
    },
    "llm_interaction": {
        "enabled": False,
        "auto_apply": True,
        "max_llm_turns": 15
    }
}

# Default implementation template
DEFAULT_IMPLEMENTATION = '''"""Custom command implementation."""


def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}
'''

class LLMClientError(Exception):
    """Exception raised for errors in the LLM client."""
    pass


def get_openai_client():
    """
    Get an OpenAI client instance.
    
    Returns:
        OpenAI client instance
        
    Raises:
        LLMClientError: If the OpenAI package is not installed or API key is not set
    """
    try:
        from openai import OpenAI
    except ImportError:
        logger.error("OpenAI package not installed. Install with: pip install openai")
        raise LLMClientError("OpenAI package not installed. Install with: pip install openai")
    
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        logger.error("OPENAI_API_KEY environment variable not set")
        raise LLMClientError("OPENAI_API_KEY environment variable not set. Please set it to use LLM features.")
    
    return OpenAI(api_key=api_key)


def generate_metadata_with_llm(command_name: str, description: str) -> Dict[str, Any]:
    """
    Generate command metadata using an LLM.
    
    Args:
        command_name: The name of the command
        description: User-provided description of the command
        
    Returns:
        Dictionary containing the generated command metadata
        
    Raises:
        LLMClientError: If there's an error communicating with the LLM
    """
    try:
        client = get_openai_client()
        
        # Create a prompt for the LLM
        prompt = f"""
        Generate YAML metadata for a command named '{command_name}' with the following description:
        
        {description}
        
        The metadata should follow this structure:
        ```yaml
        name: string (required)
        description: string (required)
        params:
          - name: string (required)
            type: string (default: "string")
            description: string (optional, default: "")
            required: boolean (default: true)
            default: any (optional, default: null)
        hidden: boolean (default: false)
        disabled: boolean (default: false)
        mcp_integration:
          enabled: boolean (default: true)
          metadata:
            endpoint: string (default auto-generated)
            method: string (default: "POST")
            authentication_required: boolean (default: false)
        llm_interaction:
          enabled: boolean (default: false)
          auto_apply: boolean (default: true)
          max_llm_turns: integer (default: 15)
        ```
        
        Based on the description, infer appropriate parameters that the command might need.
        Return only the YAML content, nothing else.
        """
        
        # Call the OpenAI API
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Using a smaller model for cost efficiency
            messages=[
                {"role": "system", "content": "You are a helpful assistant that generates YAML metadata for commands."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # Lower temperature for more deterministic output
            max_tokens=1000
        )
        
        # Extract the YAML content from the response
        yaml_content = response.choices[0].message.content.strip()
        
        # If the response contains markdown code blocks, extract just the YAML
        if "```yaml" in yaml_content:
            yaml_content = yaml_content.split("```yaml")[1].split("```")[0].strip()
        elif "```" in yaml_content:
            yaml_content = yaml_content.split("```")[1].split("```")[0].strip()
        
        # Parse the YAML content
        metadata = yaml.safe_load(yaml_content)
        
        # Ensure the command name is set correctly
        metadata["name"] = command_name
        
        # Validate the metadata structure
        if "description" not in metadata:
            metadata["description"] = description
        
        # Ensure all required fields are present
        for key, value in DEFAULT_METADATA.items():
            if key not in metadata:
                metadata[key] = value
        
        return metadata
    
    except Exception as e:
        logger.error(f"Error generating metadata with LLM: {e}")
        raise LLMClientError(f"Error generating metadata with LLM: {e}")

implementation_guidelines = """
            The implementation should be a Python file with a `tool_<command_name>()` function that:
            1. Accepts the parameters defined in the metadata
            2. Implements the functionality described in the metadata description
            3. Returns a result as defined in the metadata
            
            Follow these guidelines:
            - The entry point of the tool is the tool_<command_name>() function.  Other functions are allowed.
            - The input args should be simple python types
            - The output should be a simple python type
            - Include proper docstrings
            - This is not a Flask app, do not include any Flask-specific code
            - Handle parameter validation
            - Include error handling
            - Follow PEP 8 style guidelines
"""

def generate_implementation_with_llm(command_name: str, metadata: Dict[str, Any]) -> str:
    """
    Generate command implementation using an LLM.
    
    Args:
        command_name: The name of the command
        metadata: The command metadata
        
    Returns:
        String containing the generated command implementation
        
    Raises:
        LLMClientError: If there's an error communicating with the LLM
    """
    try:
        client = get_openai_client()
        
        # Create a prompt for the LLM
        prompt = f"""
        Generate a Python implementation for a command named '{command_name}' with the following metadata:
        
        ```yaml
        {yaml.dump(metadata, default_flow_style=False)}
        ```
        
        {implementation_guidelines}
        Return only the Python code, nothing else.
        """
        
        # Call the OpenAI API
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Using a smaller model for cost efficiency
            messages=[
                {"role": "system", "content": "You are a helpful assistant that generates Python code for commands."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # Lower temperature for more deterministic output
            max_tokens=2000
        )
        
        # Extract the Python code from the response
        code_content = response.choices[0].message.content.strip()
        
        # If the response contains markdown code blocks, extract just the Python code
        if "```python" in code_content:
            code_content = code_content.split("```python")[1].split("```")[0].strip()
        elif "```" in code_content:
            code_content = code_content.split("```")[1].split("```")[0].strip()
        
        return code_content
    
    except Exception as e:
        logger.error(f"Error generating implementation with LLM: {e}")
        raise LLMClientError(f"Error generating implementation with LLM: {e}")


def check_additional_info_needed(command_name: str, description: str) -> Optional[str]:
    """
    Check if additional information is needed from the user to generate a good command.
    
    Args:
        command_name: The name of the command
        description: User-provided description of the command
        
    Returns:
        String with follow-up questions if more information is needed, None otherwise
        
    Raises:
        LLMClientError: If there's an error communicating with the LLM
    """
    try:
        client = get_openai_client()
        
        # Create a prompt for the LLM
        prompt = f"""
        I'm creating a command named '{command_name}' with the following description:
        ```
        {description}
        {implementation_guidelines}
        ```
        Based on this information, do you need any additional details to create a good command implementation?
        If yes, provide specific questions that would help clarify the command's purpose and functionality.
        If no, just respond with "No additional information needed."
        """
        
        # Call the OpenAI API
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Using a smaller model for cost efficiency
            messages=[
                {"role": "system", "content": "You are a helpful assistant that helps users create commands."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # Lower temperature for more deterministic output
            max_tokens=500
        )
        
        # Extract the response
        response_text = response.choices[0].message.content.strip()
        
        # Check if additional information is needed
        if "no additional information needed" in response_text.lower():
            return None
        
        return response_text
    
    except Exception as e:
        logger.error(f"Error checking for additional information: {e}")
        # Don't raise an exception here, just return None to continue with available information
        return None


def generate_default_metadata_with_llm(command_name: str, description: str = "") -> Dict[str, Any]:
    """
    Generate default metadata for a command using an LLM, with fallback to basic defaults.
    
    Args:
        command_name: The name of the command
        description: Optional description of the command
        
    Returns:
        Dictionary containing the command metadata
    """
    try:
        # If no description is provided, use a generic one
        if not description:
            description = f"Command named {command_name}"
        
        # Generate metadata with LLM
        metadata = generate_metadata_with_llm(command_name, description)
        return metadata
    
    except LLMClientError as e:
        logger.warning(f"Falling back to default metadata: {e}")
        
        # Create basic default metadata
        metadata = DEFAULT_METADATA.copy()
        metadata["name"] = command_name
        metadata["description"] = description or f"Command named {command_name}"
        
        return metadata
    
    except Exception as e:
        logger.error(f"Unexpected error generating metadata: {e}")
        
        # Create basic default metadata
        metadata = DEFAULT_METADATA.copy()
        metadata["name"] = command_name
        metadata["description"] = description or f"Command named {command_name}"
        
        return metadata

================
File: evai/tool_storage.py
================
"""Tool storage utilities for EVAI CLI."""

import os
import logging
import subprocess
import tempfile
import importlib.util
import sys
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import inspect

import yaml
import json

# Set up logging
logger = logging.getLogger(__name__)

# Get the path to the templates directory
TEMPLATES_DIR = os.path.join(os.path.dirname(__file__), "templates")


def get_tool_dir(tool_name: str) -> str:
    """
    Get the directory path for a tool and create it if it doesn't exist.
    
    Args:
        tool_name: The name of the tool
        
    Returns:
        The absolute path to the tool directory
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_name={tool_name}", file=sys.stderr)
    
    if not tool_name:
        raise ValueError("Tool name cannot be empty")
    
    # Validate tool name (alphanumeric, hyphens, and underscores only)
    if not all(c.isalnum() or c in "-_" for c in tool_name):
        raise ValueError(
            "Tool name must contain only alphanumeric characters, hyphens, and underscores"
        )
    
    # Get the tool directory path
    tool_dir = os.path.expanduser(f"~/.evai/tools/{tool_name}")
    
    # Create the directory if it doesn't exist
    try:
        os.makedirs(tool_dir, exist_ok=True)
        logger.debug(f"Tool directory created or already exists: {tool_dir}")
    except OSError as e:
        logger.error(f"Failed to create tool directory: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    
    # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={tool_dir}", file=sys.stderr)
    return tool_dir


def load_tool_metadata(path: str) -> Dict[str, Any]:
    """
    Load tool metadata from a YAML file.
    
    Args:
        path: Path to the directory containing the tool.yaml file
        
    Returns:
        Dictionary containing the tool metadata
        
    Raises:
        FileNotFoundError: If the tool.yaml file doesn't exist
        yaml.YAMLError: If the YAML file is invalid
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - path={path}", file=sys.stderr)
    
    yaml_path = os.path.join(path, "tool.yaml")
    
    try:
        with open(yaml_path, "r") as f:
            metadata = yaml.safe_load(f)
            logger.debug(f"Loaded tool metadata from {yaml_path}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={metadata}", file=sys.stderr)
            return metadata if metadata else {}
    except FileNotFoundError:
        logger.error(f"Tool metadata file not found: {yaml_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise
    except yaml.YAMLError as e:
        logger.error(f"Invalid YAML in tool metadata file: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except Exception as e:
        logger.error(f"Error loading tool metadata: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def save_tool_metadata(path: str, data: Dict[str, Any]) -> None:
    """
    Save tool metadata to a YAML file.
    
    Args:
        path: Path to the directory where tool.yaml will be saved
        data: Dictionary containing the tool metadata
        
    Raises:
        OSError: If the file cannot be written
        yaml.YAMLError: If the data cannot be serialized to YAML
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - path={path}, data={data}", file=sys.stderr)
    
    if not data:
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: ValueError", file=sys.stderr)
        raise ValueError("Tool metadata cannot be empty")
    
    yaml_path = os.path.join(path, "tool.yaml")
    
    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(yaml_path), exist_ok=True)
    
    try:
        with open(yaml_path, "w") as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)
            logger.debug(f"Saved tool metadata to {yaml_path}")
    except yaml.YAMLError as e:
        logger.error(f"Failed to serialize tool metadata to YAML: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except OSError as e:
        logger.error(f"Failed to write tool metadata file: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except Exception as e:
        logger.error(f"Error saving tool metadata: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    
    # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=None", file=sys.stderr)


def get_editor() -> str:
    """
    Get the user's preferred editor.
    
    Returns:
        The path to the editor executable
    """
    # Try to get the editor from the EDITOR environment variable
    editor = os.environ.get("EDITOR")
    
    # If not set, use a default editor
    if not editor:
        if sys.platform == "win32":
            editor = "notepad.exe"
        else:
            # Try to find a common editor
            for e in ["nano", "vim", "vi", "emacs"]:
                try:
                    subprocess.run(["which", e], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    editor = e
                    break
                except subprocess.SubprocessError:
                    continue
            
            # If no editor found, use nano as a last resort
            if not editor:
                editor = "nano"
    
    return editor


def load_sample_tool_py() -> str:
    """
    Load the sample tool.py template.
    
    Returns:
        The contents of the sample tool.py file
        
    Raises:
        FileNotFoundError: If the sample file doesn't exist
    """
    sample_path = os.path.join(TEMPLATES_DIR, "sample_tool.py")
    
    try:
        with open(sample_path, "r") as f:
            return f.read()
    except FileNotFoundError:
        logger.error(f"Sample tool.py file not found: {sample_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading sample tool.py: {e}")
        raise


def load_sample_tool_yaml(tool_name: str) -> Dict[str, Any]:
    """
    Load the sample tool.yaml template and substitute the tool name.
    
    Args:
        tool_name: The name of the tool
        
    Returns:
        Dictionary containing the tool metadata
        
    Raises:
        FileNotFoundError: If the sample file doesn't exist
        yaml.YAMLError: If the YAML file is invalid
    """
    sample_path = os.path.join(TEMPLATES_DIR, "sample_tool.yaml")
    
    try:
        with open(sample_path, "r") as f:
            template = f.read()
            # Replace the placeholder with the actual tool name
            template = template.replace("{tool_name}", tool_name)
            # Parse the YAML
            metadata = yaml.safe_load(template)
            return metadata if metadata else {}
    except FileNotFoundError:
        logger.error(f"Sample tool.yaml file not found: {sample_path}")
        raise
    except yaml.YAMLError as e:
        logger.error(f"Invalid YAML in sample tool.yaml file: {e}")
        raise
    except Exception as e:
        logger.error(f"Error loading sample tool.yaml: {e}")
        raise


def edit_tool_metadata(tool_dir: str) -> Tuple[bool, Optional[Dict[str, Any]]]:
    """
    Open the tool.yaml file in the user's preferred editor and validate it after editing.
    
    Args:
        tool_dir: Path to the tool directory
        
    Returns:
        A tuple containing:
        - A boolean indicating whether the edit was successful
        - The updated metadata dictionary if successful, None otherwise
        
    Raises:
        FileNotFoundError: If the tool.yaml file doesn't exist
        subprocess.SubprocessError: If the editor process fails
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_dir={tool_dir}", file=sys.stderr)
    
    yaml_path = os.path.join(tool_dir, "tool.yaml")
    
    if not os.path.exists(yaml_path):
        logger.error(f"Tool metadata file not found: {yaml_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool metadata file not found: {yaml_path}")
    
    editor = get_editor()
    logger.debug(f"Using editor: {editor}")
    
    try:
        # Open the editor for the user to edit the file
        subprocess.run([editor, yaml_path], check=True)
        logger.debug(f"Editor closed for {yaml_path}")
        
        # Try to load the edited file
        try:
            metadata = load_tool_metadata(tool_dir)
            result = (True, metadata)
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={result}", file=sys.stderr)
            return result
        except yaml.YAMLError as e:
            logger.error(f"Invalid YAML after editing: {e}")
            result = (False, None)
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={result}", file=sys.stderr)
            return result
            
    except subprocess.SubprocessError as e:
        logger.error(f"Error running editor: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def edit_tool_implementation(tool_dir: str) -> bool:
    """
    Open the tool.py file in the user's preferred editor.
    
    Args:
        tool_dir: Path to the tool directory
        
    Returns:
        A boolean indicating whether the edit was successful
        
    Raises:
        FileNotFoundError: If the tool.py file doesn't exist
        subprocess.SubprocessError: If the editor process fails
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_dir={tool_dir}", file=sys.stderr)
    
    py_path = os.path.join(tool_dir, "tool.py")
    
    if not os.path.exists(py_path):
        logger.error(f"Tool implementation file not found: {py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool implementation file not found: {py_path}")
    
    editor = get_editor()
    logger.debug(f"Using editor: {editor}")
    
    try:
        # Open the editor for the user to edit the file
        subprocess.run([editor, py_path], check=True)
        logger.debug(f"Editor closed for {py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=True", file=sys.stderr)
        return True
            
    except subprocess.SubprocessError as e:
        logger.error(f"Error running editor: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def run_lint_check(tool_dir: str) -> Tuple[bool, Optional[str]]:
    """
    Run flake8 on the tool.py file to check for linting errors.
    
    Args:
        tool_dir: Path to the tool directory
        
    Returns:
        A tuple containing:
        - A boolean indicating whether the lint check passed
        - The lint error output if the check failed, None otherwise
        
    Raises:
        FileNotFoundError: If the tool.py file doesn't exist
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_dir={tool_dir}", file=sys.stderr)
    
    py_path = os.path.join(tool_dir, "tool.py")
    
    if not os.path.exists(py_path):
        logger.error(f"Tool implementation file not found: {py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool implementation file not found: {py_path}")
    
    try:
        # Run flake8 on the file
        result = subprocess.run(
            ["flake8", py_path],
            check=False,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Check if flake8 found any errors
        if result.returncode == 0:
            logger.debug(f"Lint check passed for {py_path}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(True, None)", file=sys.stderr)
            return (True, None)
        else:
            logger.warning(f"Lint check failed for {py_path}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(False, {result.stdout})", file=sys.stderr)
            return (False, result.stdout)
    except FileNotFoundError:
        # flake8 is not installed
        logger.warning("flake8 is not installed, skipping lint check")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(True, None)", file=sys.stderr)
        return (True, None)
    except Exception as e:
        logger.error(f"Error running lint check: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(False, {str(e)})", file=sys.stderr)
        return (False, str(e))


def list_tools() -> List[Dict[str, Any]]:
    """
    List all available tools.
    
    Returns:
        A list of dictionaries containing tool metadata
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name}", file=sys.stderr)
    
    tools_dir = os.path.expanduser("~/.evai/tools")
    
    # Create the directory if it doesn't exist
    os.makedirs(tools_dir, exist_ok=True)
    
    tools = []
    
    # Scan the tools directory
    for tool_name in os.listdir(tools_dir):
        tool_dir = os.path.join(tools_dir, tool_name)
        
        # Skip if not a directory
        if not os.path.isdir(tool_dir):
            continue
        
        try:
            # Load the tool metadata
            metadata = load_tool_metadata(tool_dir)
            
            # Skip disabled tools
            if metadata.get("disabled", False):
                continue
                
            # Add the tool to the list
            tools.append({
                "name": metadata.get("name", tool_name),
                "description": metadata.get("description", "No description"),
                "path": tool_dir
            })
        except Exception as e:
            logger.warning(f"Error loading tool '{tool_name}': {e}")
    
    # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={tools}", file=sys.stderr)
    return tools


def import_tool_module(tool_name: str) -> Any:
    """
    Dynamically import a tool module.
    
    Args:
        tool_name: The name of the tool
        
    Returns:
        The imported module
        
    Raises:
        ImportError: If the module cannot be imported
        FileNotFoundError: If the tool.py file doesn't exist
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_name={tool_name}", file=sys.stderr)
    
    tool_dir = get_tool_dir(tool_name)
    tool_py_path = os.path.join(tool_dir, "tool.py")
    
    if not os.path.exists(tool_py_path):
        logger.error(f"Tool implementation file not found: {tool_py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool implementation file not found: {tool_py_path}")
    
    try:
        # Create a module spec
        spec = importlib.util.spec_from_file_location(
            f"evai.tools.{tool_name}", tool_py_path
        )
        
        if spec is None or spec.loader is None:
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: ImportError", file=sys.stderr)
            raise ImportError(f"Failed to create module spec for {tool_py_path}")
        
        # Create the module
        module = importlib.util.module_from_spec(spec)
        
        # Add the module to sys.modules
        sys.modules[spec.name] = module
        
        # Execute the module
        spec.loader.exec_module(module)
        
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={module}", file=sys.stderr)
        return module
    except Exception as e:
        logger.error(f"Error importing tool module: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise ImportError(f"Error importing tool module: {e}")


def run_tool(tool_name: str, *args, **kwargs) -> Any:
    """
    Run a tool with the given arguments.
    
    Args:
        tool_name: The name of the tool
        *args: Positional arguments to pass to the tool function
        **kwargs: Keyword arguments to pass to the tool
        
    Returns:
        The result of the tool
        
    Raises:
        ImportError: If the tool module cannot be imported
        AttributeError: If the tool module doesn't have any tool_* functions
        Exception: If the tool execution fails
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_name={tool_name}, args={args}, kwargs={kwargs}", file=sys.stderr)
    
    try:
        # Import the tool module
        module = import_tool_module(tool_name)
        
        # Find callable functions in the module that start with "tool_"
        tool_functions = [
            name for name, obj in inspect.getmembers(module)
            if inspect.isfunction(obj) and name.startswith('tool_')
        ]
        
        if not tool_functions:
            logger.error(f"Tool module doesn't have any tool_* functions: {tool_name}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: AttributeError", file=sys.stderr)
            raise AttributeError(f"Tool module doesn't have any tool_* functions: {tool_name}")
        
        # For backward compatibility, try 'run' first if it exists
        if "run" in tool_functions:
            result = module.run(**kwargs)
        else:
            # Use the first tool function found
            function_name = tool_functions[0]
            function = getattr(module, function_name)
            
            # Get the function signature
            sig = inspect.signature(function)
            
            # If we have positional arguments, use them
            if args:
                # Convert args to appropriate types based on function signature
                converted_args = []
                for i, (param_name, param) in enumerate(sig.parameters.items()):
                    if i < len(args):
                        # Get the parameter type annotation
                        param_type = param.annotation
                        if param_type is inspect.Parameter.empty:
                            # No type annotation, use the arg as is
                            converted_args.append(args[i])
                        else:
                            # Try to convert the arg to the annotated type
                            try:
                                # Handle special cases for common types
                                if param_type is float or param_type is int:
                                    converted_args.append(param_type(args[i]))
                                elif param_type is bool:
                                    # Convert string to bool
                                    value = str(args[i]).lower()
                                    converted_args.append(value in ('true', 't', 'yes', 'y', '1'))
                                else:
                                    # For other types, try direct conversion
                                    converted_args.append(param_type(args[i]))
                            except (ValueError, TypeError):
                                # If conversion fails, use the original value
                                logger.warning(f"Could not convert argument {args[i]} to {param_type.__name__}")
                                converted_args.append(args[i])
                
                # Call the function with the converted positional arguments
                result = function(*converted_args)
            else:
                # Filter kwargs to only include parameters that the function accepts
                filtered_kwargs = {
                    k: v for k, v in kwargs.items() 
                    if k in sig.parameters
                }
                
                # Call the function with the filtered kwargs
                result = function(**filtered_kwargs)
        
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={result}", file=sys.stderr)
        return result
    except (ImportError, AttributeError) as e:
        logger.error(f"Error running tool: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except Exception as e:
        logger.error(f"Tool execution failed: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def remove_tool(tool_name: str) -> bool:
    """
    Remove a tool by deleting its directory.
    
    Args:
        tool_name: The name of the tool to remove
        
    Returns:
        True if the tool was successfully removed, False otherwise
        
    Raises:
        ValueError: If the tool name is invalid
        FileNotFoundError: If the tool directory doesn't exist
    """
    if not tool_name:
        raise ValueError("Tool name cannot be empty")
    
    # Validate tool name (alphanumeric, hyphens, and underscores only)
    if not all(c.isalnum() or c in "-_" for c in tool_name):
        raise ValueError(
            "Tool name must contain only alphanumeric characters, hyphens, and underscores"
        )
    
    # Get the tool directory path
    tool_dir = os.path.expanduser(f"~/.evai/tools/{tool_name}")
    
    # Check if the directory exists
    if not os.path.exists(tool_dir):
        raise FileNotFoundError(f"Tool '{tool_name}' not found")
    
    # Remove the directory and all its contents
    try:
        import shutil
        shutil.rmtree(tool_dir)
        logger.debug(f"Tool directory removed: {tool_dir}")
        return True
    except OSError as e:
        logger.error(f"Failed to remove tool directory: {e}")
        raise

================
File: tests/test_tools/test_cli_integration.py
================
import unittest
import os
import sys
import tempfile
import shutil
import subprocess
from unittest.mock import patch

# Add the parent directory to the path so we can import the evai package
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from evai.tool_storage import save_tool_metadata


class TestCLIIntegration(unittest.TestCase):
    def setUp(self):
        # Create a temporary directory for the test tool
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test tool
        self.tool_name = "test_subtract"
        self.tool_dir = os.path.join(self.temp_dir, ".evai", "tools", self.tool_name)
        os.makedirs(self.tool_dir, exist_ok=True)
        
        # Create the tool.py file
        with open(os.path.join(self.tool_dir, "tool.py"), "w") as f:
            f.write("""
def tool_subtract(minuend: float, subtrahend: float) -> float:
    \"\"\"
    Subtract one number from another.

    Parameters:
    minuend (float): The number from which another number will be subtracted.
    subtrahend (float): The number that will be subtracted from the minuend.

    Returns:
    float: The result of the subtraction (minuend - subtrahend).

    Raises:
    ValueError: If either minuend or subtrahend is not a number.
    \"\"\"
    # Validate input types
    if not isinstance(minuend, (int, float)):
        raise ValueError("Minuend must be a number.")
    if not isinstance(subtrahend, (int, float)):
        raise ValueError("Subtrahend must be a number.")

    # Perform the subtraction
    result = minuend - subtrahend
    return result
""")
        
        # Create the tool.yaml file
        metadata = {
            "name": self.tool_name,
            "description": "Test subtract tool",
            "params": [
                {
                    "name": "minuend",
                    "description": "The number from which another number will be subtracted",
                    "type": "float",
                    "required": True
                },
                {
                    "name": "subtrahend",
                    "description": "The number that will be subtracted from the minuend",
                    "type": "float",
                    "required": True
                }
            ]
        }
        save_tool_metadata(self.tool_dir, metadata)
    
    def tearDown(self):
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)
    
    @patch('evai.tool_storage.get_tool_dir')
    def test_cli_positional_args(self, mock_get_tool_dir):
        # Set up the mock
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Run the CLI command with positional arguments
        # Note: This is a simplified test that doesn't actually run the CLI command
        # In a real test, you would use subprocess.run to run the CLI command
        
        # Instead, we'll just verify that our changes to run_tool work correctly
        from evai.tool_storage import run_tool
        
        # Test with positional arguments
        result = run_tool(self.tool_name, "8", "5")
        self.assertEqual(result, 3.0)
        
        # Test with different values
        result = run_tool(self.tool_name, "10", "2")
        self.assertEqual(result, 8.0)
        
        # Test with negative numbers
        result = run_tool(self.tool_name, "-5", "3")
        self.assertEqual(result, -8.0)


if __name__ == "__main__":
    unittest.main()

================
File: tests/test_tools/test_positional_args.py
================
import unittest
import os
import sys
import tempfile
import shutil
from unittest.mock import patch, MagicMock

# Add the parent directory to the path so we can import the evai package
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from evai.tool_storage import save_tool_metadata, run_tool


class TestPositionalArgs(unittest.TestCase):
    def setUp(self):
        # Create a temporary directory for the test tool
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test tool
        self.tool_name = "test_subtract"
        self.tool_dir = os.path.join(self.temp_dir, self.tool_name)
        os.makedirs(self.tool_dir, exist_ok=True)
        
        # Create the tool.py file
        with open(os.path.join(self.tool_dir, "tool.py"), "w") as f:
            f.write("""
def tool_subtract(minuend: float, subtrahend: float) -> float:
    \"\"\"
    Subtract one number from another.

    Parameters:
    minuend (float): The number from which another number will be subtracted.
    subtrahend (float): The number that will be subtracted from the minuend.

    Returns:
    float: The result of the subtraction (minuend - subtrahend).

    Raises:
    ValueError: If either minuend or subtrahend is not a number.
    \"\"\"
    # Validate input types
    if not isinstance(minuend, (int, float)):
        raise ValueError("Minuend must be a number.")
    if not isinstance(subtrahend, (int, float)):
        raise ValueError("Subtrahend must be a number.")

    # Perform the subtraction
    result = minuend - subtrahend
    return result
""")
        
        # Create the tool.yaml file
        metadata = {
            "name": self.tool_name,
            "description": "Test subtract tool",
            "params": [
                {
                    "name": "minuend",
                    "description": "The number from which another number will be subtracted",
                    "type": "float",
                    "required": True
                },
                {
                    "name": "subtrahend",
                    "description": "The number that will be subtracted from the minuend",
                    "type": "float",
                    "required": True
                }
            ]
        }
        save_tool_metadata(self.tool_dir, metadata)
    
    def tearDown(self):
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)
    
    @patch('evai.tool_storage.get_tool_dir')
    @patch('evai.tool_storage.import_tool_module')
    def test_positional_args(self, mock_import_tool_module, mock_get_tool_dir):
        # Set up the mocks
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Create a mock module with the tool_subtract function
        mock_module = MagicMock()
        
        # Define the tool_subtract function
        def tool_subtract(minuend, subtrahend):
            return float(minuend) - float(subtrahend)
        
        # Set the tool_subtract function on the mock module
        mock_module.tool_subtract = tool_subtract
        mock_import_tool_module.return_value = mock_module
        
        # Test with positional arguments
        result = run_tool(self.tool_name, "8", "5")
        self.assertEqual(result, 3.0)
        
        # Test with different values
        result = run_tool(self.tool_name, "10", "2")
        self.assertEqual(result, 8.0)
        
        # Test with negative numbers
        result = run_tool(self.tool_name, "-5", "3")
        self.assertEqual(result, -8.0)
    
    @patch('evai.tool_storage.get_tool_dir')
    @patch('evai.tool_storage.import_tool_module')
    def test_keyword_args(self, mock_import_tool_module, mock_get_tool_dir):
        # Set up the mocks
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Create a mock module with the tool_subtract function
        mock_module = MagicMock()
        
        # Define the tool_subtract function
        def tool_subtract(minuend, subtrahend):
            return float(minuend) - float(subtrahend)
        
        # Set the tool_subtract function on the mock module
        mock_module.tool_subtract = tool_subtract
        mock_import_tool_module.return_value = mock_module
        
        # Test with keyword arguments (backward compatibility)
        result = run_tool(self.tool_name, minuend=8, subtrahend=5)
        self.assertEqual(result, 3.0)
        
        # Test with different values
        result = run_tool(self.tool_name, minuend=10, subtrahend=2)
        self.assertEqual(result, 8.0)
        
        # Test with negative numbers
        result = run_tool(self.tool_name, minuend=-5, subtrahend=3)
        self.assertEqual(result, -8.0)
    
    @patch('evai.tool_storage.get_tool_dir')
    @patch('evai.tool_storage.import_tool_module')
    def test_mixed_args(self, mock_import_tool_module, mock_get_tool_dir):
        # Set up the mocks
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Create a mock module with the tool_subtract function
        mock_module = MagicMock()
        
        # Define the tool_subtract function
        def tool_subtract(minuend, subtrahend):
            return float(minuend) - float(subtrahend)
        
        # Set the tool_subtract function on the mock module
        mock_module.tool_subtract = tool_subtract
        mock_import_tool_module.return_value = mock_module
        
        # This should raise an error because we don't support mixed args
        with self.assertRaises(TypeError):
            run_tool(self.tool_name, "8", subtrahend=5)


if __name__ == "__main__":
    unittest.main()

================
File: tests/__init__.py
================
"""Tests for the EVAI CLI."""

================
File: tests/test_cli.py
================
"""Tests for the EVAI CLI."""

import subprocess
import sys
from unittest import mock

import click.testing
import pytest

from evai import __version__
from evai.cli import cli, main


def test_version():
    """Test that the version is correct."""
    assert __version__ == "0.1.0"


def test_cli_help():
    """Test that the CLI prints help when invoked with --help."""
    runner = click.testing.CliRunner()
    result = runner.invoke(cli, ["--help"])
    assert result.exit_code == 0
    assert "EVAI CLI - Command-line interface for EVAI" in result.output


def test_cli_version():
    """Test that the CLI prints version when invoked with --version."""
    runner = click.testing.CliRunner()
    result = runner.invoke(cli, ["--version"])
    assert result.exit_code == 0
    assert f"evai, version {__version__}" in result.output


def test_main_no_args():
    """Test that main() shows help when no arguments are provided."""
    with mock.patch("sys.argv", ["evai"]):
        with mock.patch("evai.cli.cli") as mock_cli:
            main()
            mock_cli.assert_called_once()
            # Check that --help was added to sys.argv
            assert sys.argv == ["evai", "--help"]


def test_cli_as_module():
    """Test that the CLI can be invoked as a module."""
    result = subprocess.run(
        [sys.executable, "-m", "evai.cli", "--version"],
        capture_output=True,
        text=True,
        check=False,
    )
    assert result.returncode == 0
    assert f"version {__version__}" in result.stdout

================
File: tests/test_command_add.py
================
"""Tests for command adding functionality."""

import os
import sys
import json
import tempfile
import shutil
from unittest import mock
import pytest
from click.testing import CliRunner

from evai.cli.cli import cli
from evai.command_storage import get_command_dir, save_command_metadata


@pytest.fixture
def mock_commands_dir():
    """Create a temporary directory for commands."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Mock the expanduser function to return our temp directory
        with mock.patch('os.path.expanduser') as mock_expanduser:
            # When expanduser is called with ~/.evai, return our temp .evai dir
            mock_expanduser.side_effect = lambda path: path.replace('~', temp_dir)
            
            # Create the commands directory
            commands_dir = os.path.join(temp_dir, '.evai', 'commands')
            os.makedirs(commands_dir, exist_ok=True)
            
            yield temp_dir


def test_add_command(mock_commands_dir):
    """Test adding a command."""
    runner = CliRunner()
    result = runner.invoke(cli, ['commands', 'add', 'test-command'])
    
    assert result.exit_code == 0
    assert "Command 'test-command' created successfully." in result.output
    
    # Verify files were created
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'test-command')
    assert os.path.exists(command_dir)
    assert os.path.exists(os.path.join(command_dir, 'command.yaml'))
    assert os.path.exists(os.path.join(command_dir, 'command.py'))
    
    # Verify content of command.yaml
    with open(os.path.join(command_dir, 'command.yaml'), 'r') as f:
        metadata = json.load(f)
        assert metadata["name"] == "test-command"

================
File: tests/test_edit_implementation.py
================
"""Tests for the command implementation editing and lint checking functionality."""

import os
import shutil
import tempfile
from unittest import mock
import subprocess

from evai.tool_storage import edit_command_implementation, run_lint_check


class TestEditImplementation:
    """Tests for the command implementation editing and lint checking functionality."""

    def setup_method(self):
        """Set up the test environment."""
        # Create a temporary directory for the tests
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test command directory
        self.command_dir = os.path.join(self.temp_dir, 'test-command')
        os.makedirs(self.command_dir, exist_ok=True)
        
        # Create a test command.py file with valid Python code
        self.py_path = os.path.join(self.command_dir, 'command.py')
        with open(self.py_path, 'w') as f:
            f.write('"""Custom command implementation."""\n\n\ndef tool_echo(echo_string: str) -> str:\n    """Echo the input string."""\n    return echo_string\n')

    def teardown_method(self):
        """Clean up after the tests."""
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)

    @mock.patch('subprocess.run')
    def test_edit_command_implementation_success(self, mock_run):
        """Test editing command implementation successfully."""
        # Mock the subprocess.run call to simulate the editor
        mock_run.return_value = subprocess.CompletedProcess(args=['vi', self.py_path], returncode=0)
        
        # Call the function
        success = edit_command_implementation(self.command_dir)
        
        # Check that the function returned success
        assert success is True
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args
        assert args[0][0] in ['vi', os.environ.get('EDITOR', 'vi')]
        assert args[0][1] == self.py_path
        assert kwargs['check'] is True

    def test_edit_command_implementation_file_not_found(self):
        """Test editing command implementation when the file doesn't exist."""
        # Remove the Python file
        os.remove(self.py_path)
        
        # Call the function and check that it raises FileNotFoundError
        try:
            edit_command_implementation(self.command_dir)
            assert False, "Expected FileNotFoundError but no exception was raised"
        except FileNotFoundError:
            pass

    @mock.patch('subprocess.run')
    def test_edit_command_implementation_subprocess_error(self, mock_run):
        """Test editing command implementation when the subprocess fails."""
        # Mock the subprocess.run call to simulate an error
        mock_run.side_effect = subprocess.SubprocessError("Editor process failed")
        
        # Call the function and check that it raises SubprocessError
        try:
            edit_command_implementation(self.command_dir)
            assert False, "Expected SubprocessError but no exception was raised"
        except subprocess.SubprocessError:
            pass

    @mock.patch('subprocess.run')
    def test_run_lint_check_success(self, mock_run):
        """Test running lint check on a valid Python file."""
        # Mock the subprocess.run call to simulate flake8 passing
        mock_run.return_value = subprocess.CompletedProcess(
            args=['flake8', self.py_path],
            returncode=0,
            stdout='',
            stderr=''
        )
        
        # Call the function
        success, output = run_lint_check(self.command_dir)
        
        # Check that the function returned success
        assert success is True
        assert output is None
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args
        assert args[0][0] == 'flake8'
        assert args[0][1] == self.py_path
        assert kwargs['capture_output'] is True
        assert kwargs['text'] is True
        assert kwargs['check'] is False

    @mock.patch('subprocess.run')
    def test_run_lint_check_failure(self, mock_run):
        """Test running lint check on a Python file with lint errors."""
        # Create a Python file with lint errors
        with open(self.py_path, 'w') as f:
            f.write('"""Custom command implementation."""\n\nimport os\n\ndef tool_echo(echo_string: str) -> str:\n    x = 1\n    y = 2  # unused variable\n    return echo_string\n')
        
        # Mock the subprocess.run call to simulate flake8 failing
        mock_run.return_value = subprocess.CompletedProcess(
            args=['flake8', self.py_path],
            returncode=1,
            stdout=f'{self.py_path}:7:5: F841 local variable \'y\' is assigned to but never used',
            stderr=''
        )
        
        # Call the function
        success, output = run_lint_check(self.command_dir)
        
        # Check that the function returned failure
        assert success is False
        assert 'F841 local variable \'y\' is assigned to but never used' in output
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()

    @mock.patch('subprocess.run')
    def test_run_lint_check_flake8_not_found(self, mock_run):
        """Test running lint check when flake8 is not installed."""
        # Mock the subprocess.run call to simulate flake8 not being found
        mock_run.side_effect = FileNotFoundError("No such file or directory: 'flake8'")
        
        # Call the function
        success, output = run_lint_check(self.command_dir)
        
        # Check that the function returned failure
        assert success is False
        assert 'flake8 command not found' in output
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()

    def test_run_lint_check_file_not_found(self):
        """Test running lint check when the file doesn't exist."""
        # Remove the Python file
        os.remove(self.py_path)
        
        # Call the function and check that it raises FileNotFoundError
        try:
            run_lint_check(self.command_dir)
            assert False, "Expected FileNotFoundError but no exception was raised"
        except FileNotFoundError:
            pass

================
File: tests/test_edit_metadata.py
================
"""Tests for the command metadata editing functionality."""

import os
import shutil
import tempfile
import yaml
from unittest import mock
import subprocess

from evai.tool_storage import edit_command_metadata, get_editor


class TestEditMetadata:
    """Tests for the command metadata editing functionality."""

    def setup_method(self):
        """Set up the test environment."""
        # Create a temporary directory for the tests
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test command directory
        self.command_dir = os.path.join(self.temp_dir, 'test-command')
        os.makedirs(self.command_dir, exist_ok=True)
        
        # Create a test command.yaml file
        self.yaml_path = os.path.join(self.command_dir, 'command.yaml')
        self.test_metadata = {
            "name": "test-command",
            "description": "Test description",
            "params": [],
            "hidden": False,
            "disabled": False,
            "mcp_integration": {
                "enabled": True,
                "metadata": {
                    "endpoint": "",
                    "method": "POST",
                    "authentication_required": False
                }
            },
            "llm_interaction": {
                "enabled": False,
                "auto_apply": True,
                "max_llm_turns": 15
            }
        }
        with open(self.yaml_path, 'w') as f:
            yaml.dump(self.test_metadata, f, default_flow_style=False, sort_keys=False)

    def teardown_method(self):
        """Clean up after the tests."""
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)

    def test_get_editor(self):
        """Test getting the editor command."""
        # Test with EDITOR environment variable set
        with mock.patch.dict('os.environ', {'EDITOR': 'nano'}):
            assert get_editor() == 'nano'
        
        # Test with EDITOR environment variable not set
        with mock.patch.dict('os.environ', clear=True):
            assert get_editor() == 'vi'

    @mock.patch('subprocess.run')
    def test_edit_command_metadata_success(self, mock_run):
        """Test editing command metadata successfully."""
        # Mock the subprocess.run call to simulate the editor
        mock_run.return_value = subprocess.CompletedProcess(args=['vi', self.yaml_path], returncode=0)
        
        # Call the function
        success, metadata = edit_command_metadata(self.command_dir)
        
        # Check that the function returned success
        assert success is True
        assert metadata == self.test_metadata
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args
        assert args[0][0] in ['vi', os.environ.get('EDITOR', 'vi')]
        assert args[0][1] == self.yaml_path
        assert kwargs['check'] is True

    @mock.patch('subprocess.run')
    def test_edit_command_metadata_invalid_yaml(self, mock_run):
        """Test editing command metadata with invalid YAML."""
        # Mock the subprocess.run call to simulate the editor
        mock_run.return_value = subprocess.CompletedProcess(args=['vi', self.yaml_path], returncode=0)
        
        # Write invalid YAML to the file after the mock is set up
        with open(self.yaml_path, 'w') as f:
            f.write('invalid: yaml:\n  - missing: colon\n  indentation error\n')
        
        # Call the function
        success, metadata = edit_command_metadata(self.command_dir)
        
        # Check that the function returned failure
        assert success is False
        assert metadata is None
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()

    def test_edit_command_metadata_file_not_found(self):
        """Test editing command metadata when the file doesn't exist."""
        # Remove the YAML file
        os.remove(self.yaml_path)
        
        # Call the function and check that it raises FileNotFoundError
        try:
            edit_command_metadata(self.command_dir)
            assert False, "Expected FileNotFoundError but no exception was raised"
        except FileNotFoundError:
            pass

    @mock.patch('subprocess.run')
    def test_edit_command_metadata_subprocess_error(self, mock_run):
        """Test editing command metadata when the subprocess fails."""
        # Mock the subprocess.run call to simulate an error
        mock_run.side_effect = subprocess.SubprocessError("Editor process failed")
        
        # Call the function and check that it raises SubprocessError
        try:
            edit_command_metadata(self.command_dir)
            assert False, "Expected SubprocessError but no exception was raised"
        except subprocess.SubprocessError:
            pass

================
File: tests/test_llm.py
================
"""Tests for the LLM interaction functionality."""

import os
import shutil
import tempfile
import yaml
from click.testing import CliRunner
from unittest import mock

from evai.cli import cli
from evai.llm_client import (
    generate_default_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)


class TestLLMInteraction:
    """Tests for the LLM interaction functionality."""

    def setup_method(self):
        """Set up the test environment."""
        # Create a temporary directory for the tests
        self.temp_dir = tempfile.mkdtemp()
        
        # Create the .evai directory structure
        self.evai_dir = os.path.join(self.temp_dir, '.evai')
        os.makedirs(self.evai_dir, exist_ok=True)
        
        # Mock the expanduser function to use our temporary directory
        self.patcher = mock.patch('os.path.expanduser')
        self.mock_expanduser = self.patcher.start()
        # When expanduser is called with ~/.evai, return our temp .evai dir
        self.mock_expanduser.side_effect = lambda path: path.replace('~', self.temp_dir)

    def teardown_method(self):
        """Clean up after the tests."""
        # Stop the patcher
        self.patcher.stop()
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)

    @mock.patch('evai.llm_client.get_openai_client')
    def test_generate_metadata_with_llm(self, mock_get_client):
        """Test generating metadata with LLM."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = """```yaml
name: test-command
description: A test command
params:
  - name: param1
    type: string
    description: A test parameter
    required: true
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
```"""
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Call the function
        metadata = generate_default_metadata_with_llm("test-command", "A test command")
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        args, kwargs = mock_client.chat.completions.create.call_args
        assert kwargs["model"] == "gpt-4o-mini"
        assert len(kwargs["messages"]) == 2
        assert kwargs["messages"][0]["role"] == "system"
        assert kwargs["messages"][1]["role"] == "user"
        assert "test-command" in kwargs["messages"][1]["content"]
        assert "A test command" in kwargs["messages"][1]["content"]
        
        # Check the returned metadata
        assert metadata["name"] == "test-command"
        assert metadata["description"] == "A test command"
        assert len(metadata["params"]) == 1
        assert metadata["params"][0]["name"] == "param1"
        assert metadata["params"][0]["type"] == "string"
        assert metadata["params"][0]["description"] == "A test parameter"
        assert metadata["params"][0]["required"] is True
        assert metadata["hidden"] is False
        assert metadata["disabled"] is False
        assert metadata["mcp_integration"]["enabled"] is True
        assert metadata["mcp_integration"]["metadata"]["method"] == "POST"
        assert metadata["mcp_integration"]["metadata"]["authentication_required"] is False
        assert metadata["llm_interaction"]["enabled"] is False
        assert metadata["llm_interaction"]["auto_apply"] is True
        assert metadata["llm_interaction"]["max_llm_turns"] == 15

    @mock.patch('evai.llm_client.get_openai_client')
    def test_generate_implementation_with_llm(self, mock_get_client):
        """Test generating implementation with LLM."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = """```python
\"\"\"Custom command implementation.\"\"\"


def tool_echo(echo_string: str) -> str:
    \"\"\"Echo the input string.\"\"\"
    # Validate parameters
    if not echo_string:
        raise ValueError("Missing required parameter: echo_string")
    
    # Process the input
    result = echo_string.upper()
    
    # Return the result
    return result
```"""
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Create test metadata
        metadata = {
            "name": "test-command",
            "description": "A test command",
            "params": [
                {
                    "name": "param1",
                    "type": "string",
                    "description": "A test parameter",
                    "required": True
                }
            ],
            "hidden": False,
            "disabled": False,
            "mcp_integration": {
                "enabled": True,
                "metadata": {
                    "endpoint": "",
                    "method": "POST",
                    "authentication_required": False
                }
            },
            "llm_interaction": {
                "enabled": False,
                "auto_apply": True,
                "max_llm_turns": 15
            }
        }
        
        # Call the function
        implementation = generate_implementation_with_llm("test-command", metadata)
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        args, kwargs = mock_client.chat.completions.create.call_args
        assert kwargs["model"] == "gpt-4o-mini"
        assert len(kwargs["messages"]) == 2
        assert kwargs["messages"][0]["role"] == "system"
        assert kwargs["messages"][1]["role"] == "user"
        assert "test-command" in kwargs["messages"][1]["content"]
        
        # Check the returned implementation
        assert '"""Custom command implementation."""' in implementation
        assert 'def tool_echo(echo_string: str) -> str:' in implementation
        assert 'if not echo_string:' in implementation
        assert 'return result' in implementation

    @mock.patch('evai.llm_client.get_openai_client')
    def test_check_additional_info_needed(self, mock_get_client):
        """Test checking if additional information is needed."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = "Yes, I need more information. What specific task should this command perform?"
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Call the function
        result = check_additional_info_needed("test-command", "A test command")
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        args, kwargs = mock_client.chat.completions.create.call_args
        assert kwargs["model"] == "gpt-4o-mini"
        assert len(kwargs["messages"]) == 2
        assert kwargs["messages"][0]["role"] == "system"
        assert kwargs["messages"][1]["role"] == "user"
        assert "test-command" in kwargs["messages"][1]["content"]
        assert "A test command" in kwargs["messages"][1]["content"]
        
        # Check the returned result
        assert result == "Yes, I need more information. What specific task should this command perform?"

    @mock.patch('evai.llm_client.get_openai_client')
    def test_check_additional_info_not_needed(self, mock_get_client):
        """Test checking if additional information is not needed."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = "No additional information needed."
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Call the function
        result = check_additional_info_needed("test-command", "A detailed test command that does something specific")
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        
        # Check the returned result
        assert result is None

    @mock.patch('evai.llm_client.get_openai_client', side_effect=LLMClientError("Test error"))
    def test_generate_default_metadata_with_llm_fallback(self, mock_get_client):
        """Test fallback to default metadata when LLM fails."""
        # Call the function
        metadata = generate_default_metadata_with_llm("test-command", "A test command")
        
        # Check the returned metadata
        assert metadata["name"] == "test-command"
        assert metadata["description"] == "A test command"
        assert metadata["params"] == []
        assert metadata["hidden"] is False
        assert metadata["disabled"] is False
        assert metadata["mcp_integration"]["enabled"] is True
        assert metadata["llm_interaction"]["enabled"] is False

    @mock.patch('evai.cli.check_additional_info_needed')
    @mock.patch('evai.cli.generate_default_metadata_with_llm')
    @mock.patch('evai.cli.generate_implementation_with_llm')
    @mock.patch('evai.command_storage.edit_command_metadata')
    @mock.patch('evai.command_storage.edit_command_implementation')
    @mock.patch('evai.command_storage.run_lint_check')
    @mock.patch('evai.command_storage.get_editor')
    @mock.patch('subprocess.run')
    def test_llmadd_command(self, mock_subprocess_run, mock_get_editor, mock_lint_check, 
                           mock_edit_impl, mock_edit_meta, mock_gen_impl, 
                           mock_gen_meta, mock_check_info):
        """Test the llmadd command."""
        # Mock the editor
        mock_get_editor.return_value = "vi"
        mock_subprocess_run.return_value = mock.MagicMock(returncode=0)
        
        # Mock the LLM functions
        mock_check_info.return_value = "Additional information needed"
        mock_gen_meta.return_value = {
            "name": "test-command",
            "description": "A test command",
            "params": [
                {
                    "name": "param1",
                    "type": "string",
                    "description": "A test parameter",
                    "required": True
                }
            ],
            "hidden": False,
            "disabled": False,
            "mcp_integration": {
                "enabled": True,
                "metadata": {
                    "endpoint": "",
                    "method": "POST",
                    "authentication_required": False
                }
            },
            "llm_interaction": {
                "enabled": False,
                "auto_apply": True,
                "max_llm_turns": 15
            }
        }
        mock_gen_impl.return_value = '"""Custom command implementation."""\n\ndef tool_echo(echo_string: str) -> str:\n    """Echo the input string."""\n    return echo_string\n'
        
        # Mock the edit functions
        mock_edit_meta.return_value = (True, mock_gen_meta.return_value)
        mock_edit_impl.return_value = True
        mock_lint_check.return_value = (True, None)
        
        # Run the command
        runner = CliRunner()
        result = runner.invoke(
            cli, ['command', 'llmadd', 'test-command'], 
            input='A test command\n\nn\nn\n',
            catch_exceptions=False
        )
        
        # Print the output for debugging
        print(f"Command output: {result.output}")
        
        # Check that the command was successful
        assert result.exit_code == 0
        assert "Command 'test-command' created successfully." in result.output
        
        # Check that the command directory was created
        command_dir = os.path.join(self.temp_dir, '.evai', 'commands', 'test-command')
        assert os.path.exists(command_dir)
        
        # Check that the LLM functions were called
        mock_check_info.assert_called_once_with("test-command", "A test command")
        mock_gen_meta.assert_called_once_with("test-command", "A test command")
        mock_gen_impl.assert_called_once_with("test-command", mock_gen_meta.return_value)

================
File: tests/test_mcp_exposure.py
================
"""Tests for MCP server integration."""

import os
import sys
import json
import tempfile
from unittest import mock
import pytest

# Create a proper mock for FastMCP
class MockFastMCP:
    def __init__(self, name):
        self.name = name
        self.tool = MockToolRegistry()
    
    def run(self):
        """Mock run method."""
        pass

class MockToolRegistry:
    def __init__(self):
        self.registry = {}
    
    def __call__(self, name=None):
        def decorator(func):
            self.registry[name] = func
            return func
        return decorator

class MockContext:
    def __init__(self):
        pass

# Mock the MCP SDK
mcp_mock = mock.MagicMock()
mcp_mock.server.fastmcp.FastMCP = MockFastMCP
mcp_mock.server.fastmcp.Context = MockContext
sys.modules['mcp'] = mcp_mock
sys.modules['mcp.server'] = mcp_mock.server
sys.modules['mcp.server.fastmcp'] = mcp_mock.server.fastmcp

# Import after mocking
from evai.mcp.mcp_server import EVAIServer, create_server
from evai.tool_storage import get_command_dir, save_command_metadata, list_commands


@pytest.fixture
def mock_commands_dir():
    """Create a temporary directory for commands."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Mock the expanduser function to return our temp directory
        with mock.patch('os.path.expanduser', return_value=temp_dir):
            # Create the commands directory
            commands_dir = os.path.join(temp_dir, '.evai', 'commands')
            os.makedirs(commands_dir, exist_ok=True)
            
            # Create a test command
            test_cmd_dir = os.path.join(commands_dir, 'test-command')
            os.makedirs(test_cmd_dir, exist_ok=True)
            
            # Create command.yaml
            metadata = {
                "name": "test-command",
                "description": "A test command",
                "params": [
                    {
                        "name": "message",
                        "description": "A message to echo",
                        "required": True
                    },
                    {
                        "name": "count",
                        "description": "Number of times to echo",
                        "required": False,
                        "default": 1
                    }
                ],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
            
            with open(os.path.join(test_cmd_dir, 'command.yaml'), 'w') as f:
                json.dump(metadata, f)
            
            # Create command.py
            with open(os.path.join(test_cmd_dir, 'command.py'), 'w') as f:
                f.write('''"""Test command implementation."""

def run(message="Hello", count=1):
    """Run the test command."""
    result = []
    for _ in range(count):
        result.append(message)
    return {"messages": result}
''')
            
            # Create a command with MCP integration disabled
            disabled_mcp_dir = os.path.join(commands_dir, 'disabled-mcp')
            os.makedirs(disabled_mcp_dir, exist_ok=True)
            
            # Create command.yaml for disabled MCP command
            disabled_metadata = {
                "name": "disabled-mcp",
                "description": "A command with MCP integration disabled",
                "params": [],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": False,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
            
            with open(os.path.join(disabled_mcp_dir, 'command.yaml'), 'w') as f:
                json.dump(disabled_metadata, f)
            
            # Create command.py for disabled MCP command
            with open(os.path.join(disabled_mcp_dir, 'command.py'), 'w') as f:
                f.write('''"""Disabled MCP command implementation."""

def run():
    """Run the disabled MCP command."""
    return {"status": "disabled"}
''')
            
            # Mock list_commands to return our test commands
            with mock.patch('evai.mcp_server.list_commands', return_value=[
                {
                    "name": "test-command",
                    "description": "A test command",
                    "path": test_cmd_dir
                },
                {
                    "name": "disabled-mcp",
                    "description": "A command with MCP integration disabled",
                    "path": disabled_mcp_dir
                }
            ]):
                # Mock load_command_metadata to return our test metadata
                with mock.patch('evai.mcp_server.load_command_metadata', side_effect=lambda path: 
                    metadata if "test-command" in path else disabled_metadata):
                    yield temp_dir


@mock.patch('evai.mcp_server.run_command')
def test_server_initialization(mock_run_command, mock_commands_dir):
    """Test MCP server initialization."""
    # Create a server
    server = create_server("Test Server")
    
    # Check that the server was initialized correctly
    assert server.name == "Test Server"
    assert isinstance(server.mcp, MockFastMCP)
    assert "test-command" in server.commands
    assert "disabled-mcp" not in server.commands
    
    # Check that built-in tools are registered
    assert "add_command" in server.mcp.tool.registry
    assert "list_commands" in server.mcp.tool.registry
    assert "edit_command_implementation" in server.mcp.tool.registry
    assert "edit_command_metadata" in server.mcp.tool.registry
    assert "test-command" in server.mcp.tool.registry


@mock.patch('evai.mcp_server.run_command')
def test_command_execution(mock_run_command, mock_commands_dir):
    """Test command execution through MCP."""
    # Mock the run_command function
    mock_run_command.return_value = {"messages": ["Hello World", "Hello World", "Hello World"]}
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "test-command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    result = tool_func(message="Hello World", count=3)
    
    # Check that run_command was called with the correct arguments
    mock_run_command.assert_called_once_with("test-command", message="Hello World", count=3)
    
    # Check the result
    assert result == {"messages": ["Hello World", "Hello World", "Hello World"]}


@mock.patch('evai.mcp_server.run_command')
def test_command_execution_error(mock_run_command, mock_commands_dir):
    """Test command execution error handling."""
    # Mock the run_command function to raise an exception
    mock_run_command.side_effect = Exception("Test error")
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "test-command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    result = tool_func(message="Hello World", count=3)
    
    # Check that run_command was called with the correct arguments
    mock_run_command.assert_called_once_with("test-command", message="Hello World", count=3)
    
    # Check the result
    assert result == {"error": "Test error"}


@mock.patch('evai.mcp_server.list_commands')
def test_list_commands_tool(mock_list_commands, mock_commands_dir):
    """Test the list_commands built-in tool."""
    # Mock the list_commands function
    mock_list_commands.return_value = [
        {
            "name": "test-command",
            "description": "A test command",
            "path": "/path/to/test-command"
        }
    ]
    
    # Create a server
    server = create_server("Test Server")
    
    # Reset the mock to clear previous calls
    mock_list_commands.reset_mock()
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "list_commands":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    result = tool_func()
    
    # Check that list_commands was called
    mock_list_commands.assert_called_once()
    
    # Check the result
    assert result == {
        "status": "success",
        "commands": [
            {
                "name": "test-command",
                "description": "A test command",
                "path": "/path/to/test-command"
            }
        ]
    }


@mock.patch('evai.mcp_server.get_command_dir')
@mock.patch('evai.mcp_server.save_command_metadata')
def test_add_command_tool(mock_save_metadata, mock_get_command_dir, mock_commands_dir):
    """Test the add_command built-in tool."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'new-command')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "add_command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=False):  # Mock that the command doesn't exist yet
        with mock.patch('builtins.open', mock.mock_open()) as mock_file:
            result = tool_func(
                command_name="new-command",
                description="A new command",
                params=[
                    {
                        "name": "message",
                        "description": "A message to echo",
                        "required": True
                    }
                ]
            )
    
    # Check that get_command_dir was called with the correct arguments
    mock_get_command_dir.assert_called_once_with("new-command")
    
    # Check that save_command_metadata was called with the correct arguments
    mock_save_metadata.assert_called_once()
    args, kwargs = mock_save_metadata.call_args
    assert args[0] == command_dir
    assert args[1]["name"] == "new-command"
    assert args[1]["description"] == "A new command"
    assert args[1]["params"] == [{"name": "message", "description": "A message to echo", "required": True}]
    
    # Check the result
    assert result["status"] == "success"
    assert "new-command" in result["message"]
    assert command_dir == result["command_dir"]


def test_add_command_tool_validation(mock_commands_dir):
    """Test validation in the add_command built-in tool."""
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "add_command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Test empty command name
    result = tool_func(command_name="")
    assert result["status"] == "error"
    assert "Command name cannot be empty" in result["message"]
    
    # Test invalid command name
    result = tool_func(command_name="invalid command name")
    assert result["status"] == "error"
    assert "Command name must contain only alphanumeric characters" in result["message"]
    
    # Test command already exists
    with mock.patch('os.path.exists', return_value=True):  # Mock that the command already exists
        result = tool_func(command_name="existing-command")
        assert result["status"] == "error"
        assert "already exists" in result["message"]


@mock.patch('evai.mcp_server.get_command_dir')
@mock.patch('builtins.open', new_callable=mock.mock_open)
def test_edit_command_implementation_tool(mock_open, mock_get_command_dir, mock_commands_dir):
    """Test the edit_command_implementation built-in tool."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'test-command')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_implementation":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Reset the mock to clear previous calls
    mock_open.reset_mock()
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=True):  # Mock that the command exists
        with mock.patch('importlib.reload') as mock_reload:
            result = tool_func(
                command_name="test-command",
                implementation='''"""Updated test command implementation."""

def run(message="Hello", count=1):
    """Run the updated test command."""
    result = []
    for _ in range(count):
        result.append(f"Updated: {message}")
    return {"messages": result}
'''
            )
    
    # Check that get_command_dir was called with the correct arguments
    mock_get_command_dir.assert_called_once_with("test-command")
    
    # Check that the file was written with the correct content
    mock_open.assert_any_call(os.path.join(command_dir, "command.py"), "w")
    
    # Check the result
    assert result["status"] == "success"
    assert "test-command" in result["message"]
    assert "updated successfully" in result["message"]


@mock.patch('evai.mcp_server.get_command_dir')
def test_edit_command_implementation_nonexistent(mock_get_command_dir, mock_commands_dir):
    """Test editing implementation of a nonexistent command."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'nonexistent')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_implementation":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=False):  # Mock that the command doesn't exist
        result = tool_func(
            command_name="nonexistent",
            implementation="def run(): pass"
        )
    
    # Check the result
    assert result["status"] == "error"
    assert "does not exist" in result["message"]


@mock.patch('evai.mcp_server.get_command_dir')
@mock.patch('evai.mcp_server.save_command_metadata')
def test_edit_command_metadata_tool(mock_save_metadata, mock_get_command_dir, mock_commands_dir):
    """Test the edit_command_metadata built-in tool."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'test-command')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Add a command to the server's commands dict
    server.commands["test-command"] = {
        "name": "test-command",
        "description": "Original description",
        "params": []
    }
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_metadata":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=True):  # Mock that the command exists
        with mock.patch.dict(server.mcp.tool.registry, {"test-command": lambda: None}):
            result = tool_func(
                command_name="test-command",
                metadata={
                    "name": "test-command",
                    "description": "Updated description",
                    "params": [
                        {
                            "name": "new_param",
                            "description": "A new parameter",
                            "required": True
                        }
                    ],
                    "hidden": False,
                    "disabled": False,
                    "mcp_integration": {
                        "enabled": True
                    },
                    "llm_interaction": {
                        "enabled": False
                    }
                }
            )
    
    # Check that get_command_dir was called with the correct arguments
    mock_get_command_dir.assert_called_once_with("test-command")
    
    # Check that save_command_metadata was called with the correct arguments
    mock_save_metadata.assert_called_once()
    args, kwargs = mock_save_metadata.call_args
    assert args[0] == command_dir
    assert args[1]["name"] == "test-command"
    assert args[1]["description"] == "Updated description"
    assert len(args[1]["params"]) == 1
    assert args[1]["params"][0]["name"] == "new_param"
    
    # Check the result
    assert result["status"] == "success"
    assert "test-command" in result["message"]
    assert "updated successfully" in result["message"]
    
    # Check that the server's commands dict was updated
    assert server.commands["test-command"]["description"] == "Updated description"
    assert len(server.commands["test-command"]["params"]) == 1
    assert server.commands["test-command"]["params"][0]["name"] == "new_param"


@mock.patch('evai.mcp_server.get_command_dir')
def test_edit_command_metadata_nonexistent(mock_get_command_dir, mock_commands_dir):
    """Test editing metadata of a nonexistent command."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'nonexistent')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_metadata":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=False):  # Mock that the command doesn't exist
        result = tool_func(
            command_name="nonexistent",
            metadata={"name": "nonexistent", "description": "Nonexistent command"}
        )
    
    # Check the result
    assert result["status"] == "error"
    assert "does not exist" in result["message"]

================
File: tests/test_metadata.py
================
"""Tests for the command storage module."""

import os
import shutil
import tempfile
from pathlib import Path
from unittest import mock

import pytest
import yaml

from evai.tool_storage import (
    get_command_dir,
    load_command_metadata,
    save_command_metadata,
)


@pytest.fixture
def temp_home_dir():
    """Create a temporary directory to use as the home directory."""
    with tempfile.TemporaryDirectory() as temp_dir:
        with mock.patch.dict(os.environ, {"HOME": temp_dir}):
            yield temp_dir


class TestGetCommandDir:
    """Tests for the get_command_dir function."""

    def test_get_command_dir_creates_directory(self, temp_home_dir):
        """Test that get_command_dir creates the directory if it doesn't exist."""
        command_name = "test-command"
        expected_path = os.path.join(temp_home_dir, ".evai", "commands", command_name)
        
        # Ensure the directory doesn't exist yet
        assert not os.path.exists(expected_path)
        
        # Call the function
        result = get_command_dir(command_name)
        
        # Check that the directory was created
        assert os.path.exists(expected_path)
        assert os.path.isdir(expected_path)
        assert result == expected_path

    def test_get_command_dir_with_existing_directory(self, temp_home_dir):
        """Test that get_command_dir returns the correct path for an existing directory."""
        command_name = "existing-command"
        expected_path = os.path.join(temp_home_dir, ".evai", "commands", command_name)
        
        # Create the directory
        os.makedirs(expected_path, exist_ok=True)
        
        # Call the function
        result = get_command_dir(command_name)
        
        # Check that the correct path was returned
        assert result == expected_path

    def test_get_command_dir_with_empty_name(self):
        """Test that get_command_dir raises ValueError for an empty command name."""
        with pytest.raises(ValueError, match="Command name cannot be empty"):
            get_command_dir("")

    def test_get_command_dir_with_invalid_name(self):
        """Test that get_command_dir raises ValueError for an invalid command name."""
        with pytest.raises(ValueError, match="Command name must contain only alphanumeric"):
            get_command_dir("invalid/command")


class TestLoadCommandMetadata:
    """Tests for the load_command_metadata function."""

    def test_load_command_metadata(self, temp_home_dir):
        """Test that load_command_metadata correctly loads YAML data."""
        # Create a test directory and YAML file
        test_dir = os.path.join(temp_home_dir, "test-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        test_data = {
            "name": "test-command",
            "description": "Test command",
            "params": [{"name": "param1", "type": "string"}],
        }
        
        yaml_path = os.path.join(test_dir, "command.yaml")
        with open(yaml_path, "w") as f:
            yaml.dump(test_data, f)
        
        # Call the function
        result = load_command_metadata(test_dir)
        
        # Check that the correct data was loaded
        assert result == test_data

    def test_load_command_metadata_file_not_found(self, temp_home_dir):
        """Test that load_command_metadata raises FileNotFoundError if the file doesn't exist."""
        test_dir = os.path.join(temp_home_dir, "nonexistent-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        with pytest.raises(FileNotFoundError):
            load_command_metadata(test_dir)

    def test_load_command_metadata_invalid_yaml(self, temp_home_dir):
        """Test that load_command_metadata raises YAMLError for invalid YAML."""
        # Create a test directory and invalid YAML file
        test_dir = os.path.join(temp_home_dir, "invalid-yaml-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        yaml_path = os.path.join(test_dir, "command.yaml")
        with open(yaml_path, "w") as f:
            f.write("invalid: yaml: :")
        
        with pytest.raises(yaml.YAMLError):
            load_command_metadata(test_dir)


class TestSaveCommandMetadata:
    """Tests for the save_command_metadata function."""

    def test_save_command_metadata(self, temp_home_dir):
        """Test that save_command_metadata correctly saves YAML data."""
        # Create a test directory
        test_dir = os.path.join(temp_home_dir, "save-test-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        test_data = {
            "name": "test-command",
            "description": "Test command",
            "params": [{"name": "param1", "type": "string"}],
        }
        
        # Call the function
        save_command_metadata(test_dir, test_data)
        
        # Check that the file was created with the correct content
        yaml_path = os.path.join(test_dir, "command.yaml")
        assert os.path.exists(yaml_path)
        
        with open(yaml_path, "r") as f:
            loaded_data = yaml.safe_load(f)
        
        assert loaded_data == test_data

    def test_save_command_metadata_creates_directory(self, temp_home_dir):
        """Test that save_command_metadata creates the directory if it doesn't exist."""
        test_dir = os.path.join(temp_home_dir, "nonexistent-dir")
        
        test_data = {"name": "test-command"}
        
        # Call the function
        save_command_metadata(test_dir, test_data)
        
        # Check that the directory and file were created
        yaml_path = os.path.join(test_dir, "command.yaml")
        assert os.path.exists(yaml_path)

    def test_save_command_metadata_empty_data(self, temp_home_dir):
        """Test that save_command_metadata raises ValueError for empty data."""
        test_dir = os.path.join(temp_home_dir, "empty-data-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        with pytest.raises(ValueError, match="Command metadata cannot be empty"):
            save_command_metadata(test_dir, {})

================
File: tests/test_user_commands.py
================
"""Tests for user command loading and running."""

import os
import sys
import json
import tempfile
import shutil
from unittest import mock
import pytest
from click.testing import CliRunner
from pathlib import Path

from evai.cli.cli import cli
from evai.command_storage import get_command_dir, save_command_metadata


@pytest.fixture
def mock_commands_dir():
    """Create a temporary directory for commands with test command."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Mock the expanduser function to return our temp directory
        with mock.patch('os.path.expanduser') as mock_expanduser:
            # When expanduser is called with ~/.evai, return our temp .evai dir
            mock_expanduser.side_effect = lambda path: path.replace('~', temp_dir)
            
            # Create the commands directory
            commands_dir = os.path.join(temp_dir, '.evai', 'commands')
            os.makedirs(commands_dir, exist_ok=True)
            
            # Create a test command
            test_command_dir = os.path.join(commands_dir, 'hello')
            os.makedirs(test_command_dir, exist_ok=True)
            
            # Create command.yaml
            metadata = {
                "name": "hello",
                "description": "A test hello command",
                "arguments": [
                    {
                        "name": "name",
                        "description": "Name to greet",
                        "type": "string"
                    }
                ],
                "options": [
                    {
                        "name": "greeting",
                        "description": "Greeting to use",
                        "type": "string",
                        "required": False,
                        "default": "Hello"
                    }
                ],
                "hidden": False,
                "disabled": False
            }
            
            with open(os.path.join(test_command_dir, 'command.yaml'), 'w') as f:
                json.dump(metadata, f)
            
            # Create command.py
            with open(os.path.join(test_command_dir, 'command.py'), 'w') as f:
                f.write('''"""Test command implementation."""

def run(name, greeting="Hello"):
    """Greet the user."""
    message = f"{greeting}, {name}!"
    return {"message": message}
''')
            
            yield temp_dir


def test_user_command_in_help(mock_commands_dir):
    """Test that user commands appear in help."""
    runner = CliRunner()
    result = runner.invoke(cli, ['--help'])
    
    assert result.exit_code == 0
    assert "user" in result.output
    
    result = runner.invoke(cli, ['user', '--help'])
    assert result.exit_code == 0
    assert "hello" in result.output


def test_run_user_command(mock_commands_dir):
    """Test running a user command."""
    runner = CliRunner()
    result = runner.invoke(cli, ['user', 'hello', 'World', '--greeting', 'Hi'])
    
    assert result.exit_code == 0
    
    # Parse the JSON output
    output = json.loads(result.output)
    assert output == {"message": "Hi, World!"}

================
File: .cursorrules
================
# Instructions
- When using an entity, use:  db.entity_name.method_name to CRUD the entity, DO NOT IMPORT THE ENTITY DB CLASS, JUST USE THE METHODS ON THE DATABASE INSTANCE

## Lessons
During your interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

## Scratchpad
Always use the ./evai/docs/XXXXX_todo.md file (replace XXXXX as appropriate) in docs as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad if it alreadye exists, clear old tasks if necessary, first explain the task, and plan the steps you need to take to complete the task. Use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

## Project Rules:
Never use SQLAlchemy.  Always use straight SQL.
Never use SQLite.  Always use PostgreSQL.
We are not using JWT in the python api.
DB Migrations are in the db/migrations directory.  Use <datetime>_<name>.sql to name the migration files.
To run a db migration, use the following shell command:
```
dbmate up
```
To roll back a migration, use the following shell command:
```
dbmate down
```   
To see current migrations, use the following shell command:
```
dbmate status
```   


## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
llm --prompt "What is the capital of France?"
```

# History
### generated_feed renamed to user_feed
We renamed the generated_feed to user_feed to better reflect the purpose of the feed.

# Lessons

## User Specified Lessons
- You have a python venv in ./.venv. Use it.
- Do not use SQLAlchemy
- Use Pydantic 2 syntax for all data structures
- Include logging useful for debugging in the program output.
- Read the file before you try to edit it.
- Tests are non-interactive.  Using vi or vim to edit files needs to be mocked to avoid test failures.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

### Testing
- Tests should always be executed from the ~/projects/evai directory, not ~/projects/evai/evai
- Always run one test module at a time

## Cursor learned

- Use UUID4 for auto-generating unique identifiers in database models, do not use TEXT fields for IDs.
- When passing UUID fields between models, convert to string if the receiving model expects a string type field
- Always use timezone-aware datetime objects with UTC (from datetime import datetime, UTC) to avoid deprecation warnings and ensure consistent timezone handling
- When working with UUID fields in database operations:
  - Always convert UUID objects to strings before passing to database queries
  - Update type hints to accept both str and uuid.UUID where appropriate
  - Use explicit string conversion (str(uuid_value)) before using in queries
  - Handle cases where foreign key references might not exist by validating data before insertion
- When working with foreign key constraints:
  - Never use special values (like "all") as IDs that need to reference other tables
  - Consider using a separate system/metadata table for tracking operations that do not map directly to entities
  - Handle cases where foreign key references might not exist by validating data before insertion

## Database
- All entities are in their own db classes and are composed into the Database class in database.py
- When using an entity, use:  db.entity_name.method_name to CRUD the entity, DO NOT IMPORT THE ENTITY DB CLASS, JUST USE THE METHODS ON THE DATABASE INSTANCE

- Use asyncpg for PostgreSQL async operations
- PostgreSQL uses $1, $2 etc for parameterized queries instead of ?
- PostgreSQL has native JSON/JSONB support
- Use TIMESTAMP WITH TIME ZONE for proper timezone handling
- Use transactions for multi-statement operations
- PostgreSQL pool management is different from SQLite connections
- Always use CASCADE when dropping tables in PostgreSQL to handle foreign key constraints
- Initialize JSONB fields with default values in test fixtures
- When using UUID fields:
   - Store UUIDs as TEXT in PostgreSQL
   - Always convert UUID objects to strings before passing to database functions
   - Convert back to UUID objects in model layer if needed
   - Be consistent with string conversion in tests and assertions
- Each DB class should:
   - Take a Database instance in constructor
   - Use consistent error handling
   - Include proper type hints
   - Follow existing patterns from `user_db.py`
- Common utilities to keep in `database.py`:
   - Connection management
   - Transaction handling
   - Base query methods (fetch_one, fetch_all, etc.)
   - Row to model conversion utilities
- Error handling:
   - Use custom exceptions from `errors.py`
   - Consistent error messages
   - Proper logging
- Testing:
   - Each DB class should have its own test file
   - Follow existing test patterns
   - Include both success and error cases 

## Directory Structure Conventions

```
evai/
 evai/
    main.py                 # FastAPI initialization
    api/
       dependencies.py     # Shared dependencies (DB sessions, auth, etc.)
       v1/
           user_api.py         # Routes for user operations
           user_feed_api.py    # Routes for user feed operations
           source_feed_api.py  # Routes for source feed operations
    models/                 # Pydantic models
       user.py             # User entity model
       user_feed.py        # User feed entity model
       source_feed.py      # Source feed entity model
    db/                     # Database operations
       user_db.py             # CRUD for user
       user_feed_db.py        # CRUD for user feed
       source_feed_db.py      # CRUD for source feed
    util/                   # Utility functions
    config.py               # Application-wide configuration
 tests/                      # Unit and integration tests
    api/                    # Tests for API routes
       test_user_api.py
       test_user_feed_api.py
       test_source_feed_api.py
    models/                 # Tests for models
       test_user_model.py
       test_user_feed_model.py
       test_source_feed_model.py
    db/                     # Tests for database operations
       test_user_db.py
       test_user_feed_db.py
       test_source_feed_db.py
    util/
 scripts/                     # Utility scripts
 .gitignore
 pyproject.toml                # Build system configuration
 requirements.txt               # Dependencies
 README.md                      # Project documentation
```

## Naming Conventions

- **Python Modules & Directories**: `snake_case`
- **Classes**: `PascalCase`
- **Functions & Variables**: `snake_case`
- **Constants**: `UPPER_CASE`
- **API Routes**: `kebab-case`
- **Database Models**:
  - Class: `PascalCase`
  - Table: `snake_case`
- **Pydantic Schemas**:
  - `UserBase`: Base schema
  - `UserCreate`: Creation
  - `UserUpdate`: Update
  - `UserRead`: API responses

================
File: .gitignore
================
*.xml
*.mp3

repomix-output.txt


# Byte-compiled / optimized / DLL files
Untitled.md
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so
.obsidian/
.DS_Store
MyPods.opml

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot


# Django stuff:
*.log
*.db
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# PyPI configuration file
.pypirc
.smtcmp_chat_histories
.aider*

================
File: .python-version
================
3.12

================
File: .repomixignore
================
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
#
**/*spec.md
*.log
uv.lock

================
File: CLAUDE.md
================
# EVAI CLI Development Guide

## Build & Test Commands
- Setup: `python -m venv .venv && source .venv/bin/activate && pip install -e ".[dev]"`
- Run all tests: `pytest`
- Run single test file: `pytest tests/test_file.py`
- Run specific test: `pytest tests/test_file.py::test_function_name`
- Lint: `flake8`

## Code Style Guidelines
- Python 3.12+ required
- Use snake_case for variables, functions, modules
- Use PascalCase for classes
- Write unit tests using pytest
- Mock system components for testing
- Use Click for CLI commands, following existing patterns
- Use YAML for configuration files
- CLI tests use click.testing.CliRunner
- Never modify SQLAlchemy or SQLite code (PostgreSQL only)
- Prefer direct database access via db.entity_name.method_name pattern
- Use Pydantic 2 syntax for all data structures
- Include useful debug logging

## Project Organization
- Commands stored in ~/.evai/tools/ directory
- Tools have both metadata (YAML) and implementation (Python)
- Test modules are prefixed with "test_"
- Use the documentation in docs/ for understanding components

================
File: pyproject.toml
================
[project]
name = "evai-cli"
version = "0.1.0"
description = "Command-line interface for EVAI"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "click>=8.1.7",
    "mcp[cli]>=1.3.0",
    "pyyaml>=6.0.1",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "flake8>=6.1.0",
]

[project.scripts]
evai = "evai.cli:main"

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"

================
File: README.md
================
# EVAI CLI

A powerful command-line interface for creating, managing, and executing custom commands with LLM assistance.

## Overview

EVAI CLI is a tool that allows you to create, manage, and run custom commands with the help of Large Language Models (LLMs). It provides a seamless way to:

- Create custom commands with LLM assistance
- Edit command metadata and implementation
- Run commands with parameters
- Integrate with MCP (Machine Control Protocol) for advanced AI interactions
- Expose your commands as a local API server

## Installation

### Prerequisites

- Python 3.12 or higher
- pip (Python package installer)

### Install from source

```bash
# Clone the repository
git clone https://github.com/yourusername/evai-cli.git
cd evai-cli

# Create and activate a virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install the package in development mode
pip install -e .
```

## Usage

### Basic Commands

```bash
# Show help
evai --help

# Show version
evai --version

# List all available commands
evai command list

# Add a new command
evai command add <command_name>

# Add a new command with LLM assistance
evai command llmadd <command_name>

# Edit an existing command
evai command edit <command_name>

# Run a command
evai command run <command_name> --param key=value
```

### MCP Server

Start the MCP server to expose your commands as a local API:

```bash
evai server --name "My EVAI Commands"
```

## Command Structure

Each command consists of:

1. **Metadata** - A YAML file describing the command, its parameters, and integration options
2. **Implementation** - A Python file containing the actual command logic

Commands are stored in `~/.evai/commands/<command_name>/` with the following structure:

```
~/.evai/commands/<command_name>/
 metadata.yaml    # Command metadata
 implementation.py # Command implementation
```

### Command Metadata

The metadata file (`metadata.yaml`) contains information about the command:

```yaml
name: command_name
description: Description of what the command does
params:
  - name: param1
    description: Description of parameter 1
    required: true
  - name: param2
    description: Description of parameter 2
    required: false
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
```

### Command Implementation

The implementation file (`implementation.py`) contains the actual command logic:

```python
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    # Your command logic here
    return {"status": "success", "data": {...}}
```

## Project Structure

```
evai-cli/
 evai/                      # Main package
    __init__.py            # Package initialization
    cli/                   # CLI module
       __init__.py        # CLI package initialization
       cli.py             # Main CLI implementation
       commands/          # CLI command modules
           __init__.py    # Commands package initialization
           llmadd.py      # LLM-assisted command creation
    command_storage.py     # Command storage utilities
    llm_client.py          # LLM client for AI assistance
    mcp_server.py          # MCP server integration
 tests/                     # Test suite
 .venv/                     # Virtual environment (created during setup)
 pyproject.toml             # Project metadata and dependencies
 requirements.txt           # Pinned dependencies
 README.md                  # This file
```

## LLM Integration

EVAI CLI integrates with LLMs to help you:

1. Generate command metadata based on your description
2. Generate command implementation based on metadata
3. Suggest additional information needed for better command generation

## MCP Integration

EVAI CLI integrates with the Machine Control Protocol (MCP) to:

1. Expose your commands as tools in an MCP server
2. Provide built-in tools for managing commands
3. Support prompt templates for common tasks

## Development

### Setup Development Environment

```bash
# Clone the repository
git clone https://github.com/yourusername/evai-cli.git
cd evai-cli

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install development dependencies
pip install -e ".[dev]"
```

### Running Tests

```bash
pytest
```

## Troubleshooting

### Missing `__init__.py` in Commands Directory

If you encounter an error like:

```
TypeError: expected str, bytes or os.PathLike object, not NoneType
```

When running the `evai` command, ensure that there is an `__init__.py` file in the `evai/cli/commands/` directory. This file is required to make the commands directory a proper Python package.

## License

[MIT License](LICENSE)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

================
File: repomix.config.json
================
{
  "output": {
    "filePath": "repomix-output.txt",
    "style": "plain",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": true
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}

================
File: requirements.txt
================
click==8.1.8
-e file:///Users/lherron/projects/evai-cli
iniconfig==2.0.0
packaging==24.2
pip==25.0.1
pluggy==1.5.0
pytest==8.3.5
pyyaml==6.0.2
rich==13.9.4



================================================================
End of Codebase
================================================================
