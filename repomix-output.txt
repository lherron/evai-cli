This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.cursor/
  rules/
    database.mdc
    tests.mdc
~/
  .evai/
    tools/
      deploy_artifact/
        tool.py
        tool.yaml
docs-archive/
  cli_refactor_todo.md
  click_migration.md
  command_add_todo.md
  command_execution_todo.md
  command_refactor_todo.md
  command_storage_todo.md
  commands_todo.md
  deploy_artifact_todo.md
  implementation_editing_todo.md
  llm_interaction_todo.md
  llmadd_todo.md
  refactor_mcp_todo.md
  scaffolding_todo.md
  tool_storage_todo.md
evai/
  artifacts/
    CustomButton.tsx
    fancy-dashboard-card.tsx
  cli/
    commands/
      __init__.py
      cmdllmadd.py
      commands.py
      llmadd.py
      tools.py
    __init__.py
    cli.py
  docs/
    ARCHITECTURE.md
    cmd_plan.md
    evai_spec.md
    mcp_python_sdk.md
    mcp_spec.md
    openai_quickstart.md
    prompt_plan.md
    todo.md
    tool_positional_args_todo.md
  mcp/
    mcp_prompts.py
    mcp_server.py
    mcp_tools.py
  templates/
    sample_command.py
    sample_command.yaml
    sample_tool.py
    sample_tool.yaml
  __init__.py
  command_storage.py
  llm_client.py
  tool_storage.py
tests/
  test_tools/
    test_cli_integration.py
    test_positional_args.py
  __init__.py
  test_add_command.py
  test_cli.py
  test_command_add.py
  test_edit_implementation.py
  test_edit_metadata.py
  test_list_and_run.py
  test_llm.py
  test_mcp_exposure.py
  test_metadata.py
  test_user_commands.py
.cursorrules
.gitignore
.python-version
CLAUDE.md
pyproject.toml
README.md
requirements.txt
test_deploy_artifact.py
uv.lock

================================================================
Files
================================================================

================
File: .cursor/rules/database.mdc
================
---
description: Use when creating or updating database scripts or database entities
globs: *_db.py,database.py
---
## Database Rules
- Use [user_db.py](mdc:podthing/db/user_db.py) as the default reference for new database entities
- Each DB class should:
   - Take a Database instance in constructor
   - Use consistent error handling
   - Include proper type hints
   - Follow existing patterns from `user_db.py`
- Common utilities to keep in `database.py`:
   - Connection management
   - Transaction handling
   - Base query methods (fetch_one, fetch_all, etc.)
   - Row to model conversion utilities
- Error handling:
   - Use custom exceptions from `errors.py`
   - Consistent error messages
   - Proper logging
- Testing:
   - Each DB class should have its own test file
   - Follow existing test patterns
   - Include both success and error cases 

### Hints on using async postgresql
- Use asyncpg for PostgreSQL async operations
- PostgreSQL uses $1, $2 etc for parameterized queries instead of ?
- PostgreSQL has native JSON/JSONB support
- Use TIMESTAMP WITH TIME ZONE for proper timezone handling
- Use transactions for multi-statement operations
- PostgreSQL pool management is different from SQLite connections
- Always use CASCADE when dropping tables in PostgreSQL to handle foreign key constraints
- Initialize JSONB fields with default values in test fixtures

================
File: .cursor/rules/tests.mdc
================
---
description: Rules for creating new test classes
globs: test_*.py
---

# Database Test Rules

Fixtures to create other entities should use the DB library from that class.  For example, to create a test user as a fixture, use the methods in user_db.py.  Tests should not use raw SQL to create entities.

Use [test_user_db.py](mdc:tests/db/test_user_db.py) as an example of how to create tests for database classes.
Read the entity db table before beginning test creation.

================
File: ~/.evai/tools/deploy_artifact/tool.py
================
"""
Tool for deploying React component artifacts.

This tool saves React component artifacts to the artifacts directory.
"""

import os
import logging
from typing import Dict, Any

# Set up logging
logger = logging.getLogger(__name__)

def tool_deploy_artifact(artifact_name: str, source_code: str) -> Dict[str, Any]:
    """
    Deploy a React component artifact to the artifacts directory.
    
    Args:
        artifact_name: The name of the artifact (will be used as the filename)
        source_code: The source code of the React component
        
    Returns:
        A dictionary with the status of the deployment
    """
    logger.debug(f"Deploying artifact: {artifact_name}")
    
    try:
        # Ensure artifact name is valid
        if not artifact_name:
            raise ValueError("Artifact name cannot be empty")
        
        # Ensure artifact name has proper extension
        if not artifact_name.endswith('.tsx'):
            artifact_name = f"{artifact_name}.tsx"
        
        # Get the artifacts directory path
        artifacts_dir = os.path.expanduser("~/projects/evai-cli/evai/artifacts")
        
        # Create the directory if it doesn't exist
        os.makedirs(artifacts_dir, exist_ok=True)
        
        # Create the full path to the artifact file
        artifact_path = os.path.join(artifacts_dir, artifact_name)
        
        # Write the source code to the file
        with open(artifact_path, "w") as f:
            f.write(source_code)
        
        logger.info(f"Successfully deployed artifact to {artifact_path}")
        
        return {
            "status": "success",
            "message": f"Artifact '{artifact_name}' deployed successfully",
            "path": artifact_path
        }
        
    except Exception as e:
        logger.error(f"Error deploying artifact: {e}")
        return {
            "status": "error",
            "message": str(e)
        }

================
File: ~/.evai/tools/deploy_artifact/tool.yaml
================
name: deploy_artifact
description: Deploy a React component artifact to the artifacts directory
params:
  - name: artifact_name
    type: string
    description: The name of the artifact (will be used as the filename)
    required: true
  - name: source_code
    type: string
    description: The source code of the React component
    required: true
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15

================
File: docs-archive/cli_refactor_todo.md
================
# CLI Refactoring Task

## Task Description
Move all commands for the cli.group called "command" into a new command.py submodule in the commands directory.

## Steps Taken
[X] Create a new file `evai/cli/commands/command.py`
[X] Move all command functions from `cli.py` to `command.py`
[X] Update `cli.py` to remove the command functions
[X] Test the refactored code to ensure it works correctly

## Implementation Details

1. Created a new file `evai/cli/commands/command.py` with all the command functions:
   - `add`: Add a new custom command
   - `edit`: Edit an existing command
   - `list`: List available commands
   - `run`: Run a command with the given arguments

2. Removed these functions from `cli.py` and added a comment indicating where they were moved to.

3. The existing import mechanism in `cli.py` already handles importing commands from the `commands` directory, so no additional changes were needed for the import logic.

4. Tested the refactored code to ensure all commands are still accessible and working correctly.

## Result
The refactoring was successful. All commands are now properly organized in the `commands` directory, making the codebase more modular and easier to maintain.

The command group structure is preserved:
```
evai command add <command_name>
evai command edit <command_name>
evai command list
evai command run <command_name>
```

All functionality remains the same, but the code is now better organized.

================
File: docs-archive/click_migration.md
================
# Migration from argparse to click

## Task Overview
We need to switch the EVAI CLI from using argparse to click. This involves:
- Adding click as a dependency
- Refactoring the CLI implementation in cli.py
- Updating the tests to work with click
- Ensuring all existing functionality continues to work

## Progress Tracking
- [X] Add click as a dependency in pyproject.toml
- [X] Refactor cli.py to use click instead of argparse
- [X] Update tests to work with click
- [X] Verify that all functionality continues to work

## Implementation Summary
1. Added click and pyyaml as dependencies in pyproject.toml
2. Refactored cli.py to use click's decorators and command groups:
   - Created a `cli()` function decorated with `@click.group`
   - Added version option with `@click.version_option`
   - Updated `main()` to call `cli()` and handle the case when no arguments are provided
3. Updated the tests to work with click's testing utilities:
   - Used `click.testing.CliRunner` to invoke the CLI
   - Updated assertions to match click's output format
   - Added a test for the case when no arguments are provided
4. Verified that all tests pass successfully
5. Manually tested the CLI to ensure it behaves correctly

## Benefits of Using Click
- More declarative and easier to read syntax with decorators
- Built-in support for command groups and subcommands
- Better help text formatting
- Easier testing with the CliRunner utility
- More robust argument parsing and validation

## Next Steps
The migration from argparse to click is now complete. The next step would be to implement the Command Directory & YAML Management functionality as outlined in the todo.md file.

================
File: docs-archive/command_add_todo.md
================
# Command Add Workflow Implementation

## Task Description
Implement the `command add` workflow for the EVAI CLI, which allows users to create custom commands that can be executed via the CLI or exposed through the MCP server.

## Completed Steps
[X] Implemented the `evai command add <command-name>` subcommand in `cli.py`
[X] Created functionality to generate default `command.yaml` with standard fields and placeholders
[X] Created functionality to generate default `command.py` with a stub function
[X] Added tests to confirm the directory and files are created with correct content
[X] Ensured all tests pass

## Implementation Details

### CLI Subcommand
Added a new subcommand group `command` with a subcommand `add` that takes a command name as an argument. The subcommand:
1. Validates the command name (alphanumeric, hyphens, and underscores only)
2. Creates the command directory using `get_command_dir`
3. Generates default metadata and saves it to `command.yaml`
4. Creates a default implementation in `command.py`
5. Provides feedback to the user about the created files

### Default Files
- `command.yaml`: Contains metadata about the command, including name, description, parameters, and integration settings
- `command.py`: Contains a simple implementation with a `run` function that prints "Hello World" and returns a success status

### Testing
Created `test_add_command.py` with tests that:
1. Verify the command is created successfully with valid names
2. Verify the command fails with invalid names
3. Check that the directory and files are created with the correct content

## Next Steps
The next phase of the implementation will be to add interactive editing of the `command.yaml` file:
- Check for the `$EDITOR` environment variable and default to `vi` if not set
- Open the newly created `command.yaml` in the editor
- Parse the YAML after saving
- If invalid, prompt the user to fix it or abort
- Add tests to simulate user edits and validate re-editing behavior

After that, we'll implement similar functionality for editing the `command.py` file, including lint checking with `flake8`.

================
File: docs-archive/command_execution_todo.md
================
# Command Execution & MCP Exposure

## Implementation Plan

### Command Listing and Execution
- [X] Implement `evai command list` to scan the command directory and list available commands
- [X] Implement `evai command run <command-name>` to execute a command
- [X] Add parameter support for command execution

### MCP Server Integration
- [X] Create a minimal MCP server using the MCP Python SDK
- [X] Scan command directory and load command metadata
- [X] Expose commands as MCP tools
- [X] Handle requests for command execution with JSON data

### Claude Desktop Integration
- [X] Add built-in tools for command management
  - [X] `add_command`: Create a new command with metadata and implementation
  - [X] `list_commands`: List all available commands
  - [X] `edit_command_implementation`: Edit the implementation of an existing command
  - [X] `edit_command_metadata`: Edit the metadata of an existing command

### Testing
- [X] Create test file for CLI command listing and execution
- [X] Create test file for MCP server integration
- [X] Fix test mocking for MCP server integration

## Current Status

All tasks for Command Execution & MCP Exposure have been completed successfully, including enhancements for Claude Desktop integration.

## Implementation Details

### Command Listing and Execution
- Commands are listed by scanning the `~/.evai/commands` directory
- Commands are executed by dynamically importing the command module and calling its `run` function
- Parameters are passed to the command as keyword arguments

### MCP Server Integration
- The MCP server is created using the MCP Python SDK
- Commands are registered as MCP tools with their metadata
- Requests for command execution are handled by calling the command's `run` function

### Claude Desktop Integration
- Built-in tools are registered in the MCP server for command management
- The `add_command` tool creates a new command with metadata and implementation
- The `list_commands` tool lists all available commands
- The `edit_command_implementation` tool edits the implementation of an existing command
- The `edit_command_metadata` tool edits the metadata of an existing command

### Test Fixes
- Fixed mocking of the MCP SDK in the tests
- Properly mocked command directory structure and metadata loading
- Added tests for built-in tools

## Dependencies
- MCP Python SDK
- Python importlib for dynamic module loading
- JSON for metadata handling

## Summary of Changes
1. Added functions to `command_storage.py` to:
   - List available commands
   - Dynamically import command modules
   - Run commands with arguments

2. Added CLI subcommands to `cli.py`:
   - `evai command list` to list available commands
   - `evai command run <command-name>` to run a command
   - `evai server` to start the MCP server

3. Created a new `mcp_server.py` file for MCP integration:
   - `EVAIServer` class to manage the MCP server
   - Registration of commands as MCP tools
   - Error handling for command execution

4. Enhanced the MCP server with built-in tools for Claude Desktop:
   - `add_command` tool to create new commands
   - `list_commands` tool to list available commands
   - `edit_command_implementation` tool to edit command implementation
   - `edit_command_metadata` tool to edit command metadata

5. Added tests:
   - `test_list_and_run.py` for CLI command listing and execution
   - `test_mcp_exposure.py` for MCP server integration and built-in tools

All tasks for Command Execution & MCP Exposure have been completed successfully, with additional enhancements for Claude Desktop integration.

================
File: docs-archive/command_refactor_todo.md
================
# Command Refactor Task

## Current Understanding
- The `add` command currently creates a new command and then opens an editor for interactive editing
- The editing logic includes:
  - Opening the command.yaml file in an editor and validating it
  - Opening the command.py file in an editor
  - Running lint checks on command.py and allowing re-editing if there are issues
- We need to simplify it to only create the command and validate YAML
- The editing functionality should be moved to a new `edit` command

## Tasks
- [X] Examine the current `add` command implementation fully
- [X] Identify the editing logic that needs to be moved
- [X] Simplify the `add` command to only create and validate
- [X] Create a new `edit` command with the moved editing logic
- [X] Ensure both commands work properly together
- [X] Update any related documentation or help text

## Implementation Summary

### 1. Simplified the `add` command
- Removed the interactive editing logic
- Kept only the command creation and basic validation
- Updated the help text to suggest using the new edit command

### 2. Created a new `edit` command
- Created a new command function `edit` in the command group
- Implemented the editing logic that was removed from `add`
- Added options to edit either metadata or implementation or both
- Included the lint checking functionality

### 3. Key Changes
- The `add` command now only creates the command with default templates
- The `edit` command provides interactive editing with validation
- Users can choose which parts of the command to edit (metadata, implementation, or both)
- The lint checking functionality is preserved in the edit command

### 4. Benefits
- Clearer separation of concerns between command creation and editing
- More flexibility for users to edit specific parts of commands
- Simplified workflow for creating new commands
- Better adherence to the Unix philosophy of "do one thing well"

================
File: docs-archive/command_storage_todo.md
================
# Command Directory & YAML Management Implementation

## Task Description
Implement functionality to manage a command repository under the user's home directory:

1. Create a function `get_command_dir(command_name)` in a new file `evai/command_storage.py` that:
   - Returns the path: `~/.evai/commands/<command-name>`
   - Creates the directory if it doesn't exist

2. Implement `load_command_metadata(path) -> dict` and `save_command_metadata(path, data: dict) -> None` that:
   - Use PyYAML to parse/write YAML to a file named `command.yaml` in that directory
   - Handle errors gracefully

3. Create tests in `tests/test_metadata.py` to test these functions

## Implementation Plan

[X] Create `evai/command_storage.py` with the required functions
[X] Install PyYAML if not already installed
[X] Create `tests/test_metadata.py` with tests for the functions
[X] Ensure tests pass and follow best practices

## Implementation Details

### `get_command_dir(command_name)`
- Use `os.path.expanduser` to get the user's home directory
- Create the directory structure if it doesn't exist using `os.makedirs`
- Return the path as a string

### `load_command_metadata(path) -> dict`
- Open and read the YAML file at the given path
- Parse it using PyYAML
- Return the parsed data as a dictionary
- Handle file not found and YAML parsing errors

### `save_command_metadata(path, data: dict) -> None`
- Convert the dictionary to YAML using PyYAML
- Write it to the file at the given path
- Create parent directories if they don't exist
- Handle file writing errors

### Testing
- Test that `get_command_dir` creates the directory if it doesn't exist
- Test that `load_command_metadata` correctly loads YAML data
- Test that `save_command_metadata` correctly saves YAML data
- Test error handling for both functions

## Summary of Implementation

We have successfully implemented the Command Directory & YAML Management functionality for the EVAI CLI project. The implementation includes:

1. A function `get_command_dir(command_name)` that:
   - Returns the path to the command directory
   - Creates the directory if it doesn't exist
   - Validates the command name

2. Functions `load_command_metadata(path)` and `save_command_metadata(path, data)` that:
   - Load and save YAML metadata for commands
   - Handle errors gracefully
   - Provide detailed error messages

3. Comprehensive tests in `tests/test_metadata.py` that:
   - Test the functionality of all functions
   - Test error handling
   - Use temporary directories to avoid affecting the user's actual home directory

All tests are passing, and the implementation follows best practices for Python code, including:
- Type hints
- Comprehensive docstrings
- Proper error handling
- Logging
- Clean code structure

The implementation is now ready for the next step: implementing the `command add` workflow.

================
File: docs-archive/commands_todo.md
================
# Commands Submodule Implementation Plan

## Task
Create a commands submodule under evai and move llmadd from cli.py into its own file.

## Steps
[X] Create the commands directory structure
[X] Create an __init__.py file in the commands directory
[X] Create a llmadd.py file in the commands directory
[X] Move the llmadd function from cli.py to llmadd.py
[X] Update imports in llmadd.py
[X] Update cli.py to import and use the new llmadd function
[X] Test the changes
[X] Fix name conflict between imported llmadd function and llmadd command function
[X] Verify the fix by testing the command with an actual argument

## Implementation Details
1. The commands directory was created at evai/commands/
2. The llmadd.py file contains the llmadd function from cli.py
3. The __init__.py file exposes the llmadd function
4. The cli.py file was updated to import the llmadd function from the commands submodule
5. Fixed a name conflict by importing the llmadd function as llmadd_command to avoid collision with the command function

## Testing
We tested the changes by running the CLI help command for the llmadd command:
```
python -m evai.cli command llmadd --help
```

The command works as expected, showing the help message for the llmadd command.

We also tested the command with an actual argument:
```
python -m evai.cli command llmadd test_command
```

The command successfully created a new command called "test_command" that subtracts two numbers.

## Bug Fixes
- Fixed a name conflict issue where the imported llmadd function had the same name as the llmadd command function in cli.py, causing the CLI to misinterpret command arguments.

## Summary
We have successfully created a commands submodule and moved the llmadd function from cli.py to its own file. The CLI still works as expected, and the code is now better organized with the commands in their own submodule. We also fixed a name conflict issue that was causing the CLI to misinterpret command arguments.

================
File: docs-archive/deploy_artifact_todo.md
================
# Deploy Artifact Tool

## Task
Create a new MCP tool called `deploy_artifact` that will save React component artifacts to the artifacts directory.

## Requirements
- Input: Artifact Name and the source of the React Component artifact
- Output: Save the component to a .tsx file in the "artifacts" directory
- The tool should create the artifacts directory if it doesn't exist

## Steps
- [X] Understand how tools are implemented in the codebase
- [X] Create the artifacts directory if it doesn't exist
- [X] Create the deploy_artifact tool implementation
- [X] Create the deploy_artifact tool metadata
- [X] Test the tool

## Implementation Details
1. Created the artifacts directory at `evai/artifacts`
2. Created the tool directory at `~/.evai/tools/deploy_artifact`
3. Implemented the tool function in `~/.evai/tools/deploy_artifact/tool.py`
4. Created the tool metadata in `~/.evai/tools/deploy_artifact/tool.yaml`

The tool will:
- Take an artifact name and source code as input
- Ensure the artifact name has a .tsx extension
- Save the source code to a file in the artifacts directory
- Return success/failure status with the path to the saved file

================
File: docs-archive/implementation_editing_todo.md
================
# Implementation Editing and Lint Checking

## Task Description
Implement the functionality to edit the command implementation file (`command.py`) and perform lint checking using flake8.

## Steps
[X] Add function `edit_command_implementation(command_dir)` to `command_storage.py`
  - Open the command.py file in the user's preferred editor
  - Return a boolean indicating success

[X] Add function `run_lint_check(command_dir)` to `command_storage.py`
  - Run flake8 on the command.py file
  - Return a tuple with a boolean indicating success and the error output if any

[X] Update the `add` command in `cli.py` to:
  - After metadata editing is complete, open the implementation file for editing
  - Run lint check on the implementation file
  - If lint check fails, show errors and prompt user to re-edit or abort
  - Loop until lint check passes or user aborts

[X] Create test file `tests/test_edit_implementation.py` with tests for:
  - Successful editing of implementation file
  - File not found error
  - Subprocess error
  - Successful lint check
  - Failed lint check
  - flake8 not found error
  - File not found error for lint check

[X] Update `tests/test_add_command.py` to include tests for:
  - Successful editing and lint checking
  - Lint failure followed by success
  - Lint failure and user abort

[X] Update `evai/docs/todo.md` to mark the implementation editing and lint checking as completed

## Implementation Details

### Command Storage Functions
- `edit_command_implementation(command_dir)`: Opens the command.py file in the user's preferred editor and returns a boolean indicating success.
- `run_lint_check(command_dir)`: Runs flake8 on the command.py file and returns a tuple with a boolean indicating success and the error output if any.

### CLI Command
The `add` command now includes:
1. Metadata editing (existing functionality)
2. Implementation editing
3. Lint checking with re-editing loop

### Testing
- `test_edit_implementation.py`: Tests for the implementation editing and lint checking functions
- Updated `test_add_command.py`: Tests for the full workflow including implementation editing and lint checking

## Completion Status
All tasks for implementation editing and lint checking have been completed successfully.

================
File: docs-archive/llm_interaction_todo.md
================
# LLM Interaction Implementation Plan

## Task Overview
Implement todo #7 LLM Interaction for the EVAI CLI, which includes:
- Creating a new CLI command `evai command llmadd <command-name>`
- Querying the user for command description
- Calling an LLM with the description and name to generate metadata and implementation
- Building templates for metadata and implementation
- Implementing LLM-based default metadata generation
- Implementing LLM integration inside command.py logic
- Creating tests for the LLM functionality

## Implementation Steps

### 1. Create LLM Client Module
- [X] Create a new file `evai/llm_client.py` to handle LLM API interactions
- [X] Implement OpenAI API client using the OpenAI Python SDK
- [X] Create functions for generating command metadata and implementation

### 2. Add LLM Command to CLI
- [X] Add `llmadd` subcommand to the `command` group in `cli.py`
- [X] Implement user interaction to get command description
- [X] Call LLM to generate metadata and implementation

### 3. Update Command Storage
- [X] Add functions in `command_storage.py` to support LLM-generated metadata and implementation
- [X] Implement fallback mechanisms for when LLM is unavailable

### 4. Testing
- [X] Create `tests/test_llm.py` to test LLM interaction
- [X] Mock LLM API calls for testing
- [X] Test fallback mechanisms

## Implementation Details

### LLM Client
- [X] Use OpenAI API for LLM interactions
- [X] Support environment variable for API key: `OPENAI_API_KEY`
- [X] Implement retry logic for API failures
- [X] Create prompts for generating metadata and implementation

### CLI Command
- [X] `evai command llmadd <command-name>` will:
  1. Ask user for command description
  2. Call LLM to generate metadata and implementation
  3. Save generated files
  4. Allow user to edit generated files

### Testing Strategy
- [X] Mock OpenAI API responses
- [X] Test with valid and invalid inputs
- [X] Test fallback mechanisms

================
File: docs-archive/llmadd_todo.md
================
# LLMAdd Enhancement Task

## Task Description
Enhance the `llmadd` command to print the returned YAML and Python with rich formatting for better readability.

## Current Implementation
- The `llmadd` command generates metadata (YAML) and implementation (Python) using LLM
- Currently, it doesn't display the generated content to the user
- The user is only informed that the metadata and implementation were generated successfully

## Plan
1. [X] Understand the current implementation of `llmadd` command
2. [X] Check if rich is already a dependency in the project
3. [X] Add rich as a dependency if it's not already included
4. [X] Modify the `llmadd` command to:
   - [X] Display the generated YAML with rich formatting
   - [X] Display the generated Python with rich formatting
5. [X] Test the changes

## Implementation Details
- Added rich modules for syntax highlighting and console output:
  - rich.console.Console
  - rich.syntax.Syntax
  - rich.panel.Panel
- Modified the `llmadd` function in `cli.py` to display the generated content
- Used rich's syntax highlighting capabilities for YAML and Python
- Added panels to make the output more visually distinct

## Testing
To test the changes, run:
```
pip install -e .
pip install rich==13.7.1
evai command llmadd test-command
```

## Summary
The task has been completed successfully. The `llmadd` command now displays the generated YAML and Python with rich formatting, making it easier for users to review the generated content before deciding whether to edit it.

================
File: docs-archive/refactor_mcp_todo.md
================
# MCP Refactoring Plan

## Task
Refactor the MCP server code by:
1. Moving prompt registration into a new `mcp_prompts.py` file
2. Moving tool registration into a new `mcp_tools.py` file

## Current Structure
- `mcp_server.py` contains:
  - `_register_built_in_tools()` - Registers built-in tools
  - `_register_tools()` - Registers all available tools
  - `_register_tool_tool()` - Helper to register a single tool
  - `_register_prompts()` - Registers all available prompts

## Refactoring Steps
[X] Create `mcp_prompts.py` file
[X] Move prompt-related code from `mcp_server.py` to `mcp_prompts.py`
[X] Create `mcp_tools.py` file
[X] Move tool-related code from `mcp_server.py` to `mcp_tools.py`
[X] Update `mcp_server.py` to import and use the new modules
[X] Test the refactored code

## Implementation Details

### mcp_prompts.py (COMPLETED)
- Contains:
  - The `PROMPTS` dictionary
  - A function to register all prompts
  - The `analyze_file` prompt function

### mcp_tools.py (COMPLETED)
- Contains:
  - Functions to register built-in tools
  - Functions to register custom tools
  - The tool implementation functions

### mcp_server.py (UPDATED)
- Now:
  - Imports the new modules
  - Calls the registration functions from the new modules
  - Keeps the server initialization and running logic

## Summary of Changes
1. Created `mcp_prompts.py` with:
   - Moved the `PROMPTS` dictionary
   - Added a `register_prompts()` function
   - Moved the `analyze_file` prompt function

2. Created `mcp_tools.py` with:
   - Added a `register_built_in_tools()` function for built-in tools
   - Added a `register_tools()` function for all available tools
   - Added a `register_tool()` function to register a single tool
   - Moved all tool implementation functions

3. Updated `mcp_server.py`:
   - Removed the old registration methods
   - Added imports for the new modules
   - Updated the `__init__` method to use the new registration functions
   - Kept the `read_file` method needed by the prompts module

## Testing Results
The refactored code was tested by creating a server instance, which completed successfully. The warnings about tools and prompts already existing are expected because the server is initialized twice in the test script (once in the global scope and once in the create_server function).

The refactoring is complete and the code is now more modular and easier to maintain.

================
File: docs-archive/scaffolding_todo.md
================
# Project Scaffolding Implementation

## Task Overview
We need to implement the basic project structure for the EVAI CLI tool, including:
- Creating the necessary Python package structure
- Setting up a basic CLI entry point
- Implementing basic testing

## Progress Tracking
- [X] **Create Project Structure**  
  - [X] Create `__init__.py` inside `evai/`.
  - [X] Create `cli.py` with a basic `main()` function.
- [X] **Command-Line Entry Point**  
  - [X] Decide on `argparse`, `click`, or similar.
  - [X] Implement minimal CLI that prints help/version info.
- [X] **Basic Testing**  
  - [X] Create `tests/` folder with a simple test (`test_cli.py`).
  - [X] Verify that invoking CLI with `python -m evai.cli` works.

## Implementation Summary
1. Created the basic directory structure for the EVAI CLI project
2. Created the `__init__.py` file in the evai package with version information
3. Created `cli.py` with a basic CLI implementation using argparse
4. Set up a basic test structure to verify the CLI works
5. Updated pyproject.toml to include pytest as a development dependency and configure the CLI entry point
6. Ran the tests to confirm everything is working as expected
7. Installed the package in development mode to use the `evai` command directly

## Next Steps
The Project Scaffolding tasks are now complete. The next step would be to implement the Command Directory & YAML Management functionality as outlined in the todo.md file.

================
File: docs-archive/tool_storage_todo.md
================
# Tool Storage Renaming Task

## Overview
We are renaming the "command" entity to "tool" in the tool_storage.py file (previously command_storage.py). The module has already been renamed, but we need to update all references to "command" within the file and update any imports or usages in other files.

## Changes Needed in tool_storage.py

### Function Renames
- `get_command_dir` -> `get_tool_dir`
- `load_command_metadata` -> `load_tool_metadata`
- `save_command_metadata` -> `save_tool_metadata`
- `edit_command_metadata` -> `edit_tool_metadata`
- `edit_command_implementation` -> `edit_tool_implementation`
- `run_lint_check` -> No change needed (function is generic)
- `list_commands` -> `list_tools`
- `import_command_module` -> `import_tool_module`
- `run_command` -> `run_tool`

### Path Changes
- `~/.evai/commands/` -> `~/.evai/tools/`
- `command.yaml` -> `tool.yaml`
- `command.py` -> `tool.py`

### Variable/Parameter Renames
- `command_name` -> `tool_name`
- `command_dir` -> `tool_dir`
- `commands` -> `tools`
- Other similar variable names

## Files That Need Updates
1. evai/cli/commands/llmadd.py
2. evai/cli/commands/command.py (may need to be renamed to tool.py)
3. evai/cli/cli.py
4. evai/mcp/mcp_server.py
5. tests/test_list_and_run.py

## Implementation Strategy
1. First update tool_storage.py
2. Then update imports and function calls in other files
3. Rename any files that need to be renamed (like command.py -> tool.py)
4. Update tests

## Progress
- [X] Update tool_storage.py
- [X] Update tests/test_list_and_run.py
- [X] Update evai/cli/commands/llmadd.py
- [X] Create evai/cli/commands/tool.py (replacing command.py)
- [X] Update evai/cli/cli.py
- [X] Update evai/mcp/mcp_server.py
- [X] Delete the old command.py file

## Summary
We have successfully completed the renaming of the "command" entity to "tool" throughout the codebase. This included:

1. Updating all function names in tool_storage.py
2. Updating all variable names and paths in tool_storage.py
3. Creating a new tool.py file to replace command.py
4. Updating imports and function calls in all dependent files
5. Deleting the old command.py file

The renaming is now complete and the codebase should be consistent in its use of "tool" terminology. 

## Verification
We have verified that the changes work correctly by:

1. Running the CLI with the `--help` flag to confirm that the tool commands are properly registered
2. Running `tool --help` to verify that all tool subcommands are available
3. Checking that the tool directory is created correctly at `~/.evai/tools/`
4. Creating a new test tool with `tool add test-tool2` and confirming it was created successfully
5. Listing available tools with `tool list` and confirming our test tool appears
6. Running the test tool with `tool run test-tool2` and confirming it executes correctly

All tests passed successfully, confirming that the renaming from "command" to "tool" has been implemented correctly throughout the codebase. 

# Externalize Sample Tool Templates

## Task Description
Externalize the sample tool.py and tool.yaml templates that are currently hardcoded in the `evai/cli/commands/tool.py` file. This will make them easier to maintain and update.

## Current Implementation
Currently, the sample templates are hardcoded in the `add` command in `evai/cli/commands/tool.py`:

1. Default tool.yaml template:
```python
default_metadata = {
    "name": tool_name,
    "description": "Default description",
    "params": [],
    "hidden": False,
    "disabled": False,
    "mcp_integration": {
        "enabled": True,
        "metadata": {
            "endpoint": "",
            "method": "POST",
            "authentication_required": False
        }
    },
    "llm_interaction": {
        "enabled": False,
        "auto_apply": True,
        "max_llm_turns": 15
    }
}
```

2. Default tool.py template:
```python
"""Custom tool implementation."""


def run(**kwargs):
    """Run the tool with the given arguments."""
    print("Hello World")
    return {"status": "success"}
```

## Plan

1. [X] Create a templates directory in the evai package
2. [X] Create sample_tool.py and sample_tool.yaml files
3. [X] Update the tool_storage.py to load these templates
4. [X] Update the tool.py command to use the externalized templates
5. [X] Update the llmadd.py command to use the externalized templates

## Implementation Steps

1. [X] Create templates directory and files
   - Created `evai/templates` directory
   - Created `evai/templates/sample_tool.py`
   - Created `evai/templates/sample_tool.yaml`

2. [X] Add functions to tool_storage.py to load the templates
   - Added `load_sample_tool_py()` function
   - Added `load_sample_tool_yaml(tool_name)` function
   - Added `TEMPLATES_DIR` constant

3. [X] Update the add command in tool.py to use the new functions
   - Updated the `add` command to use the externalized templates
   - Added fallback to hardcoded templates if loading fails

4. [X] Update the llmadd command in llmadd.py to use the new functions
   - Updated the `llmadd` command to use the externalized templates
   - Added fallback to hardcoded templates if loading fails

## Testing

To test the changes:

1. Run `evai tool add test-tool` to create a new tool
2. Verify that the tool is created successfully
3. Run `evai tool llmadd test-llm-tool` to create a new tool with LLM assistance
4. Verify that the tool is created successfully

## Conclusion

The sample tool templates have been successfully externalized to the `evai/templates` directory. This makes them easier to maintain and update in the future. The code now loads these templates from the filesystem instead of having them hardcoded in the source code.

================
File: evai/artifacts/CustomButton.tsx
================
import React from 'react';
import { Button } from '@/components/ui/button';

interface ButtonProps {
  label: string;
  onClick: () => void;
}

export const CustomButton: React.FC<ButtonProps> = ({ label, onClick }) => {
  return (
    <Button 
      className='bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded'
      onClick={onClick}
    >
      {label}
    </Button>
  );
};

================
File: evai/artifacts/fancy-dashboard-card.tsx
================
import React, { useState } from 'react';
import { LineChart, Line, XAxis, YAxis, CartesianGrid, Tooltip, ResponsiveContainer } from 'recharts';
import { Card, CardContent, CardDescription, CardFooter, CardHeader, CardTitle } from '@/components/ui/card';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { AlertCircle, ArrowUpRight, BarChart2, Clock, Download, Users } from 'lucide-react';
import { Alert, AlertDescription, AlertTitle } from '@/components/ui/alert';

const FancyDashboardCard = () => {
  const [view, setView] = useState('week');
  
  // Sample data
  const weekData = [
    { name: 'Mon', value: 420 },
    { name: 'Tue', value: 380 },
    { name: 'Wed', value: 510 },
    { name: 'Thu', value: 580 },
    { name: 'Fri', value: 550 },
    { name: 'Sat', value: 620 },
    { name: 'Sun', value: 670 },
  ];
  
  const monthData = [
    { name: 'Week 1', value: 2800 },
    { name: 'Week 2', value: 3200 },
    { name: 'Week 3', value: 3600 },
    { name: 'Week 4', value: 3900 },
  ];
  
  const displayData = view === 'week' ? weekData : monthData;
  const currentValue = displayData[displayData.length - 1].value;
  const previousValue = displayData[displayData.length - 2].value;
  const percentChange = ((currentValue - previousValue) / previousValue * 100).toFixed(1);
  const isPositive = currentValue > previousValue;
  
  return (
    <Card className="w-full max-w-md shadow-lg">
      <CardHeader className="pb-2">
        <div className="flex justify-between items-start">
          <div>
            <CardTitle className="text-2xl font-bold">User Activity</CardTitle>
            <CardDescription className="text-gray-500">Daily active users</CardDescription>
          </div>
          <Badge variant="outline" className="flex items-center gap-1 px-2 py-1">
            <Users size={14} />
            <span>Users</span>
          </Badge>
        </div>
      </CardHeader>
      
      <CardContent className="pb-0">
        <div className="flex items-baseline justify-between mb-4">
          <div>
            <span className="text-3xl font-bold">{currentValue}</span>
            <div className="flex items-center gap-1 mt-1">
              <Badge className={`${isPositive ? 'bg-green-100 text-green-800' : 'bg-red-100 text-red-800'}`}>
                <span className="flex items-center gap-1">
                  {isPositive ? '+' : ''}{percentChange}%
                  <ArrowUpRight size={14} className={`${!isPositive && 'rotate-180'}`} />
                </span>
              </Badge>
              <span className="text-gray-500 text-sm">vs previous</span>
            </div>
          </div>
          
          <Tabs defaultValue="week" className="w-fit" onValueChange={setView}>
            <TabsList className="grid w-36 grid-cols-2">
              <TabsTrigger value="week">Week</TabsTrigger>
              <TabsTrigger value="month">Month</TabsTrigger>
            </TabsList>
          </Tabs>
        </div>
        
        <div className="h-64 w-full">
          <ResponsiveContainer width="100%" height="100%">
            <LineChart data={displayData} margin={{ top: 5, right: 5, left: 0, bottom: 5 }}>
              <CartesianGrid strokeDasharray="3 3" vertical={false} opacity={0.3} />
              <XAxis dataKey="name" axisLine={false} tickLine={false} />
              <YAxis axisLine={false} tickLine={false} width={30} />
              <Tooltip 
                contentStyle={{ 
                  borderRadius: '8px', 
                  border: 'none', 
                  boxShadow: '0 4px 12px rgba(0,0,0,0.1)',
                  padding: '8px 12px'
                }} 
              />
              <Line 
                type="monotone" 
                dataKey="value" 
                stroke="#6366F1" 
                strokeWidth={3} 
                dot={{ r: 4, strokeWidth: 2 }}
                activeDot={{ r: 6, stroke: '#6366F1', strokeWidth: 2 }}
              />
            </LineChart>
          </ResponsiveContainer>
        </div>
        
        {isPositive && percentChange > 15 && (
          <Alert className="mt-4 bg-amber-50">
            <AlertCircle className="h-4 w-4 text-amber-600" />
            <AlertTitle>Notable increase</AlertTitle>
            <AlertDescription>
              User activity has increased significantly. Check system resources.
            </AlertDescription>
          </Alert>
        )}
      </CardContent>
      
      <CardFooter className="flex justify-between pt-4">
        <Button variant="outline" size="sm" className="gap-1">
          <Download size={14} />
          Export
        </Button>
        <div className="flex items-center gap-2 text-gray-500 text-sm">
          <Clock size={14} />
          <span>Updated 2 mins ago</span>
        </div>
      </CardFooter>
    </Card>
  );
};

export default FancyDashboardCard;

================
File: evai/cli/commands/__init__.py
================
"""Command modules for EVAI CLI."""

# This file makes the commands directory a proper Python package

================
File: evai/cli/commands/cmdllmadd.py
================
"""LLM-assisted command creation for EVAI CLI."""

import sys
import os
import yaml
import click
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from evai.command_storage import (
    get_command_dir,
    save_command_metadata
)
from evai.llm_client import (
    generate_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)

# Initialize rich console
console = Console()


def generate_default_metadata_with_llm(command_name: str, description: str) -> dict:
    """
    Generate default metadata for a command using LLM.
    
    Args:
        command_name: The name of the command
        description: A description of the command
        
    Returns:
        A dictionary containing the command metadata
    """
    # Generate metadata with LLM with special instructions for commands
    prompt = f"""Generate metadata for a command (not a tool) named '{command_name}'. 
Description: {description}

The metadata should include:
1. name: {command_name}
2. description: A one-line description
3. arguments: List of command-line positional arguments (NOT parameters)
4. options: List of command-line options with flags
5. hidden: Boolean (false by default)
6. disabled: Boolean (false by default)
7. mcp_integration and llm_interaction objects (can be copy-pasted from the example below)

Each argument should have:
- name
- description  
- type (string, integer, float, boolean)

Each option should have:
- name
- description
- type (string, integer, float, boolean)  
- required (boolean)
- default (optional)

Example structure (fill in the actual details):
```yaml
name: "{command_name}"
description: "Command description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
```

Return ONLY the YAML, nothing else."""

    # Use the generic LLM client function but with our custom prompt
    try:
        from evai.llm_client import generate_content
        yaml_string = generate_content(prompt)
        
        # Try to parse the YAML
        return yaml.safe_load(yaml_string)
    except Exception as e:
        raise LLMClientError(f"Error generating command metadata: {e}")


@click.command()
@click.argument("command_name")
def llmadd(command_name):
    """Add a new custom command using LLM assistance."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)
        
        # Get a description from the user
        description = click.prompt("Enter a description for the command", type=str)
        
        # Check if additional information is needed
        try:
            additional_info = check_additional_info_needed(command_name, description)
            if additional_info:
                click.echo("\nThe LLM suggests gathering more information:")
                click.echo(additional_info)
                
                # Allow user to provide additional details
                additional_details = click.prompt(
                    "Would you like to provide additional details? (leave empty to skip)",
                    default="",
                    type=str
                )
                
                if additional_details:
                    description = f"{description}\n\nAdditional details: {additional_details}"
        except LLMClientError as e:
            click.echo(f"Warning: {e}")
            click.echo("Continuing with the provided description.")
        
        # Generate metadata with LLM
        click.echo("Generating command metadata with LLM...")
        
        try:
            metadata = generate_default_metadata_with_llm(command_name, description)
            click.echo("Metadata generated successfully.")
            
            # Display the generated YAML with rich formatting
            yaml_str = yaml.dump(metadata, default_flow_style=False)
            console.print("\n[bold blue]Generated YAML Metadata:[/bold blue]")
            console.print(Panel(Syntax(yaml_str, "yaml", theme="monokai", line_numbers=True)))
        except Exception as e:
            click.echo(f"Error generating metadata with LLM: {e}", err=True)
            click.echo("Falling back to default metadata.")
            
            # Create default metadata
            metadata = {
                "name": command_name,
                "description": description,
                "arguments": [],
                "options": [],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
        
        # Save the metadata
        save_command_metadata(cmd_dir, metadata)
        
        # Generate implementation with LLM
        click.echo("\nGenerating command implementation with LLM...")
        
        try:
            # Custom prompt for command implementation
            impl_prompt = f"""Create a Python implementation for a command-line interface command named '{command_name}'.
Description: {description}

Here is the YAML metadata for this command:
```yaml
{yaml.dump(metadata, default_flow_style=False)}
```

The implementation should:
1. Define a 'run' function that accepts all arguments and options in the metadata
2. Process the arguments and options as needed
3. Return a dictionary with the command's results

The file should include:
- A module docstring explaining the command
- Type hints for all arguments
- Proper error handling
- Informative docstrings

Example structure:
```python
\"\"\"Implementation for the {command_name} command.\"\"\"

def run(**kwargs):
    \"\"\"Execute the {command_name} command with the given arguments.\"\"\"
    # Extract arguments from kwargs
    # Process the command logic
    # Return a dictionary with results
    return {{"status": "success", "data": {...}}}
```

Return ONLY the Python code, nothing else."""

            from evai.llm_client import generate_content
            implementation = generate_content(impl_prompt)
            
            click.echo("Implementation generated successfully.")
            
            # Display the generated Python code with rich formatting
            console.print("\n[bold blue]Generated Python Implementation:[/bold blue]")
            console.print(Panel(Syntax(implementation, "python", theme="monokai", line_numbers=True)))
            
            # Save the implementation
            cmd_py_path = os.path.join(cmd_dir, "command.py")
            with open(cmd_py_path, "w") as f:
                f.write(implementation)
        except Exception as e:
            click.echo(f"Error generating implementation with LLM: {e}", err=True)
            click.echo("Falling back to default implementation.")
            
            # Create default implementation
            cmd_py_path = os.path.join(cmd_dir, "command.py")
            with open(cmd_py_path, "w") as f:
                f.write(f'"""Implementation for the {command_name} command."""\n\n\ndef run(**kwargs):\n    """Execute the {command_name} command with the given arguments."""\n    print("Hello World")\n    return {{"status": "success"}}\n')
        
        click.echo(f"\nCommand '{command_name}' created successfully.")
        click.echo(f"- Metadata: {os.path.join(cmd_dir, 'command.yaml')}")
        click.echo(f"- Implementation: {cmd_py_path}")
        click.echo(f"\nTo edit this command, run: evai commands edit {command_name}")
        
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)

================
File: evai/cli/commands/commands.py
================
"""Command management functions for EVAI CLI."""

import sys
import os
import json
import click
import yaml
from evai.command_storage import (
    get_command_dir,
    save_command_metadata,
    load_command_metadata,
    list_commands,
    import_command_module,
    run_command
)
from rich.console import Console
from rich.syntax import Syntax
from rich.panel import Panel

# Initialize rich console
console = Console()


@click.command()
@click.argument("command_name")
def add(command_name):
    """Add a new custom command."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)
            
        # Load default metadata template
        with open(os.path.join(os.path.dirname(__file__), "../../templates/sample_command.yaml"), "r") as f:
            metadata_content = f.read().replace("{command_name}", command_name)
            default_metadata = yaml.safe_load(metadata_content)
        
        # Save metadata
        save_command_metadata(cmd_dir, default_metadata)
        
        # Create default command.py
        with open(os.path.join(os.path.dirname(__file__), "../../templates/sample_command.py"), "r") as f:
            with open(cmd_dir / "command.py", "w") as py_file:
                py_file.write(f.read())
        
        click.echo(f"Command '{command_name}' created successfully.")
        click.echo(f"- Metadata: {cmd_dir / 'command.yaml'}")
        click.echo(f"- Implementation: {cmd_dir / 'command.py'}")
        click.echo(f"\nTo edit this command, run: evai commands edit {command_name}")
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
def new(command_name):
    """Alias for 'add' - Add a new custom command."""
    add.callback(command_name)


@click.command()
@click.argument("command_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit command metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit command implementation")
def edit(command_name, metadata, implementation):
    """Edit an existing command."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        # Edit metadata if requested
        if metadata:
            click.echo(f"Opening command.yaml for editing...")
            
            # Get path to metadata file
            metadata_path = cmd_dir / "command.yaml"
            if not metadata_path.exists():
                click.echo(f"Metadata file not found: {metadata_path}", err=True)
                sys.exit(1)
                
            # Open editor for user to edit the file
            click.edit(filename=str(metadata_path))
            
            # Validate YAML after editing
            try:
                with open(metadata_path, "r") as f:
                    metadata_content = yaml.safe_load(f)
                click.echo("Command metadata saved successfully.")
            except Exception as e:
                click.echo(f"Invalid YAML: {e}", err=True)
                if click.confirm("Would you like to try again?"):
                    return edit.callback(command_name, True, False)
                click.echo("Skipping metadata edit.")
        
        # Edit implementation if requested
        if implementation:
            click.echo(f"Opening command.py for editing...")
            
            # Get path to implementation file
            impl_path = cmd_dir / "command.py"
            if not impl_path.exists():
                click.echo(f"Implementation file not found: {impl_path}", err=True)
                sys.exit(1)
                
            # Open editor for user to edit the file
            click.edit(filename=str(impl_path))
            click.echo("Command implementation saved.")
        
        click.echo(f"Command '{command_name}' edited successfully.")
        
    except Exception as e:
        click.echo(f"Error editing command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit command metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit command implementation")
def e(command_name, metadata, implementation):
    """Alias for 'edit' - Edit an existing command."""
    edit.callback(command_name, metadata, implementation)


@click.command()
def list():
    """List all available commands."""
    try:
        # Get the list of commands
        commands = list_commands()
        
        if not commands:
            click.echo("No commands found.")
            return
        
        # Print the list of commands
        click.echo("Available commands:")
        for cmd in commands:
            click.echo(f"- {cmd['name']}: {cmd['description']}")
        
    except Exception as e:
        click.echo(f"Error listing commands: {e}", err=True)
        sys.exit(1)


@click.command()
def ls():
    """Alias for 'list' - List all available commands."""
    list.callback()


@click.command()
@click.argument("command_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def remove(command_name, force):
    """Remove a custom command."""
    try:
        # Get the command directory
        cmd_dir = get_command_dir(command_name)
        
        if not cmd_dir.exists():
            click.echo(f"Command '{command_name}' not found.", err=True)
            sys.exit(1)
            
        # Confirm removal unless force flag is set
        if not force and not click.confirm(f"Are you sure you want to remove command '{command_name}'?"):
            click.echo("Operation cancelled.")
            return
        
        # Remove the command directory
        import shutil
        shutil.rmtree(cmd_dir)
        
        click.echo(f"Command '{command_name}' removed successfully.")
        
    except Exception as e:
        click.echo(f"Error removing command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def rm(command_name, force):
    """Alias for 'remove' - Remove a custom command."""
    remove.callback(command_name, force)


@click.command()
@click.argument("command_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Command parameters in the format key=value")
def run(command_name, args, param):
    """Run a command with the given arguments.
    
    Arguments can be provided as positional arguments after the command name,
    or as key=value pairs with the --param/-p option.
    
    Example:
        evai commands run greet John
        evai commands run greet --param name=John --param greeting=Hello
    """
    try:
        # Parse parameters from --param options
        kwargs = {}
        for p in param:
            try:
                key, value = p.split("=", 1)
                # Try to parse the value as JSON
                try:
                    kwargs[key] = json.loads(value)
                except json.JSONDecodeError:
                    # If not valid JSON, use the raw string
                    kwargs[key] = value
            except ValueError:
                click.echo(f"Invalid parameter format: {p}. Use key=value format.", err=True)
                sys.exit(1)
        
        # Get command metadata to check parameter requirements
        cmd_dir = get_command_dir(command_name)
        metadata = load_command_metadata(cmd_dir)
        
        # If using --param options, check required options
        if not args and "options" in metadata:
            for opt_def in metadata.get("options", []):
                opt_name = opt_def.get("name")
                if opt_name and opt_def.get("required", False) and opt_name not in kwargs:
                    # If option has a default value, use it
                    if "default" in opt_def and opt_def["default"] is not None:
                        kwargs[opt_name] = opt_def["default"]
                    else:
                        click.echo(f"Missing required option: {opt_name}", err=True)
                        sys.exit(1)
        
        # Run the command with positional args if provided, otherwise use kwargs
        if args:
            result = run_command(command_name, *args, **kwargs)
        else:
            result = run_command(command_name, **kwargs)
        
        # Print the result
        if isinstance(result, dict):
            click.echo(json.dumps(result, indent=2))
        else:
            click.echo(result)
    
    except Exception as e:
        click.echo(f"Error running command: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("command_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Command parameters in the format key=value")
def r(command_name, args, param):
    """Alias for 'run' - Run a command with the given arguments."""
    run.callback(command_name, args, param)

================
File: evai/cli/commands/llmadd.py
================
"""LLM-assisted tool creation for EVAI CLI."""

import sys
import os
import yaml
import click
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from evai.tool_storage import (
    get_tool_dir, 
    save_tool_metadata,
    load_sample_tool_yaml
)
from evai.llm_client import (
    generate_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)

# Initialize rich console
console = Console()


def generate_default_metadata_with_llm(tool_name: str, description: str) -> dict:
    """
    Generate default metadata for a tool using LLM.
    
    Args:
        tool_name: The name of the tool
        description: A description of the tool
        
    Returns:
        A dictionary containing the tool metadata
    """
    # Generate metadata with LLM
    metadata = generate_metadata_with_llm(tool_name, description)
    
    # Ensure required fields are present
    if "name" not in metadata:
        metadata["name"] = tool_name
    if "description" not in metadata:
        metadata["description"] = description
    if "params" not in metadata:
        metadata["params"] = []
    if "hidden" not in metadata:
        metadata["hidden"] = False
    if "disabled" not in metadata:
        metadata["disabled"] = False
    if "mcp_integration" not in metadata:
        metadata["mcp_integration"] = {
            "enabled": True,
            "metadata": {
                "endpoint": "",
                "method": "POST",
                "authentication_required": False
            }
        }
    if "llm_interaction" not in metadata:
        metadata["llm_interaction"] = {
            "enabled": False,
            "auto_apply": True,
            "max_llm_turns": 15
        }
    
    return metadata


@click.command()
@click.argument("tool_name")
def llmadd(tool_name):
    """Add a new custom tool using LLM assistance."""
    try:
        # Get the tool directory
        tool_dir = get_tool_dir(tool_name)
        
        # Get a description from the user
        description = click.prompt("Enter a description for the tool", type=str)
        
        # Check if additional information is needed
        try:
            additional_info = check_additional_info_needed(tool_name, description)
            if additional_info:
                click.echo("\nThe LLM suggests gathering more information:")
                click.echo(additional_info)
                
                # Allow user to provide additional details
                additional_details = click.prompt(
                    "Would you like to provide additional details? (leave empty to skip)",
                    default="",
                    type=str
                )
                
                if additional_details:
                    description = f"{description}\n\nAdditional details: {additional_details}"
        except LLMClientError as e:
            click.echo(f"Warning: {e}")
            click.echo("Continuing with the provided description.")
        
        # Generate metadata with LLM
        click.echo("Generating metadata with LLM...")
        
        try:
            metadata = generate_default_metadata_with_llm(tool_name, description)
            click.echo("Metadata generated successfully.")
            
            # Display the generated YAML with rich formatting
            yaml_str = yaml.dump(metadata, default_flow_style=False)
            console.print("\n[bold blue]Generated YAML Metadata:[/bold blue]")
            console.print(Panel(Syntax(yaml_str, "yaml", theme="monokai", line_numbers=True)))
        except Exception as e:
            click.echo(f"Error generating metadata with LLM: {e}", err=True)
            click.echo("Falling back to default metadata.")
            
            # Try to load the sample template
            try:
                metadata = load_sample_tool_yaml(tool_name)
                metadata["description"] = description
            except Exception as template_error:
                click.echo(f"Error loading sample template: {template_error}", err=True)
                
                # Create default metadata
                metadata = {
                    "name": tool_name,
                    "description": description,
                    "params": [],
                    "hidden": False,
                    "disabled": False,
                    "mcp_integration": {
                        "enabled": True,
                        "metadata": {
                            "endpoint": "",
                            "method": "POST",
                            "authentication_required": False
                        }
                    },
                    "llm_interaction": {
                        "enabled": False,
                        "auto_apply": True,
                        "max_llm_turns": 15
                    }
                }
        
        # Save the metadata
        save_tool_metadata(tool_dir, metadata)
        
        # Generate implementation with LLM
        click.echo("\nGenerating tool implementation with LLM...")
        
        try:
            implementation = generate_implementation_with_llm(tool_name, metadata)
            click.echo("Implementation generated successfully.")
            
            # Display the generated Python code with rich formatting
            console.print("\n[bold blue]Generated Python Implementation:[/bold blue]")
            console.print(Panel(Syntax(implementation, "python", theme="monokai", line_numbers=True)))
            
            # Save the implementation
            tool_py_path = os.path.join(tool_dir, "tool.py")
            with open(tool_py_path, "w") as f:
                f.write(implementation)
        except Exception as e:
            click.echo(f"Error generating implementation with LLM: {e}", err=True)
            click.echo("Falling back to default implementation.")
            
            # Create default implementation
            tool_py_path = os.path.join(tool_dir, "tool.py")
            with open(tool_py_path, "w") as f:
                f.write(f'"""Custom tool implementation for {tool_name}."""\n\n\ndef run(**kwargs):\n    """Run the tool with the given arguments."""\n    print("Hello World")\n    return {{"status": "success"}}\n')
        
        click.echo(f"\nTool '{tool_name}' created successfully.")
        click.echo(f"- Metadata: {os.path.join(tool_dir, 'tool.yaml')}")
        click.echo(f"- Implementation: {tool_py_path}")
        click.echo(f"\nTo edit this tool, run: evai tool edit {tool_name}")
        
    except Exception as e:
        click.echo(f"Error creating tool: {e}", err=True)
        sys.exit(1)

================
File: evai/cli/commands/tools.py
================
"""Tool management functions for EVAI CLI."""

import sys
import os
import json
import click
from evai.tool_storage import (
    get_tool_dir, 
    save_tool_metadata, 
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    list_tools,
    run_tool,
    load_tool_metadata,
    load_sample_tool_py,
    load_sample_tool_yaml,
    remove_tool
)
from rich.console import Console

# Initialize rich console
console = Console()


@click.command()
@click.argument("tool_name")
def add(tool_name):
    """Add a new custom tool."""
    try:
        # Get the tool directory
        tool_dir = get_tool_dir(tool_name)
        
        # Load the sample tool.yaml template
        try:
            default_metadata = load_sample_tool_yaml(tool_name)
        except Exception as e:
            click.echo(f"Error loading sample tool.yaml template: {e}", err=True)
            click.echo("Falling back to default metadata.")
            
            # Create default metadata
            default_metadata = {
                "name": tool_name,
                "description": "Default description",
                "params": [],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
        
        # Save the metadata
        save_tool_metadata(tool_dir, default_metadata)
        
        # Create default tool.py
        tool_py_path = os.path.join(tool_dir, "tool.py")
        try:
            tool_py_content = load_sample_tool_py()
            with open(tool_py_path, "w") as f:
                f.write(tool_py_content)
        except Exception as e:
            click.echo(f"Error loading sample tool.py template: {e}", err=True)
            click.echo("Falling back to default implementation.")
            
            # Create default tool.py with a simple function (not using run(**kwargs))
            with open(tool_py_path, "w") as f:
                f.write('"""Custom tool implementation."""\n\n\ndef tool_echo(echo_string: str) -> str:\n    """Echo the input string."""\n    return echo_string\n')
        
        click.echo(f"Tool '{tool_name}' created successfully.")
        click.echo(f"- Metadata: {os.path.join(tool_dir, 'tool.yaml')}")
        click.echo(f"- Implementation: {tool_py_path}")
        click.echo(f"\nTo edit this tool, run: evai tool edit {tool_name}")
        
    except Exception as e:
        click.echo(f"Error creating tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
def new(tool_name):
    """Alias for 'add' - Add a new custom tool."""
    add.callback(tool_name)


@click.command()
@click.argument("tool_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit tool metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit tool implementation")
def edit(tool_name, metadata, implementation):
    """Edit an existing tool."""
    try:
        # Get the tool directory
        tool_dir = get_tool_dir(tool_name)
        
        # Edit metadata if requested
        if metadata:
            click.echo(f"Opening tool.yaml for editing...")
            
            # Loop until the user provides valid YAML or chooses to abort
            while True:
                success, metadata_content = edit_tool_metadata(tool_dir)
                
                if success:
                    click.echo("Tool metadata saved successfully.")
                    break
                else:
                    if not click.confirm("Invalid YAML. Would you like to try again?"):
                        click.echo("Aborting metadata edit.")
                        break
                    click.echo("Opening tool.yaml for editing again...")
        
        # Edit implementation if requested
        if implementation:
            click.echo(f"Opening tool.py for editing...")
            
            # Open the editor for the user to edit the file
            edit_tool_implementation(tool_dir)
            
            # Run a lint check on the edited file
            lint_success, lint_output = run_lint_check(tool_dir)
            
            if not lint_success:
                click.echo("Lint check failed with the following errors:")
                click.echo(lint_output)
                
                if click.confirm("Would you like to fix the lint errors?"):
                    # Loop until the user fixes the lint errors or chooses to abort
                    while True:
                        click.echo(f"Opening tool.py for editing...")
                        
                        # Open the editor for the user to edit the file
                        edit_tool_implementation(tool_dir)
                        
                        # Run a lint check on the edited file
                        lint_success, lint_output = run_lint_check(tool_dir)
                        
                        if lint_success:
                            click.echo("Lint check passed.")
                            break
                        else:
                            click.echo("Lint check failed with the following errors:")
                            click.echo(lint_output)
                            
                            if not click.confirm("Would you like to try again?"):
                                click.echo("Skipping lint errors.")
                                break
            else:
                click.echo("Lint check passed.")
        
        click.echo(f"Tool '{tool_name}' edited successfully.")
        
    except Exception as e:
        click.echo(f"Error editing tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
@click.option("--metadata/--no-metadata", default=True, help="Edit tool metadata")
@click.option("--implementation/--no-implementation", default=True, help="Edit tool implementation")
def e(tool_name, metadata, implementation):
    """Alias for 'edit' - Edit an existing tool."""
    edit.callback(tool_name, metadata, implementation)


@click.command()
def list():
    """List all available tools."""
    try:
        # Get the list of tools
        tools = list_tools()
        
        if not tools:
            click.echo("No tools found.")
            return
        
        # Print the list of tools
        click.echo("Available tools:")
        for tool in tools:
            click.echo(f"- {tool['name']}: {tool['description']}")
        
    except Exception as e:
        click.echo(f"Error listing tools: {e}", err=True)
        sys.exit(1)


@click.command()
def ls():
    """Alias for 'list' - List all available tools."""
    list.callback()


@click.command()
@click.argument("tool_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Tool parameters in the format key=value (for backward compatibility)")
def run(tool_name, args, param):
    """Run a tool with the given arguments.
    
    Arguments can be provided as positional arguments after the tool name,
    or as key=value pairs with the --param/-p option for backward compatibility.
    
    Example:
        evai tools run subtract 8 5
        evai tools run subtract --param minuend=8 --param subtrahend=5
    """
    try:
        # Parse parameters from --param options (backward compatibility)
        kwargs = {}
        for p in param:
            try:
                key, value = p.split("=", 1)
                # Try to parse the value as JSON
                try:
                    kwargs[key] = json.loads(value)
                except json.JSONDecodeError:
                    # If not valid JSON, use the raw string
                    kwargs[key] = value
            except ValueError:
                click.echo(f"Invalid parameter format: {p}. Use key=value format.", err=True)
                sys.exit(1)
        
        # Get tool metadata to check parameter requirements
        tool_dir = get_tool_dir(tool_name)
        metadata = load_tool_metadata(tool_dir)
        
        # If using --param options, check required parameters
        if not args and "params" in metadata:
            for param_def in metadata.get("params", []):
                param_name = param_def.get("name")
                if param_name and param_def.get("required", True) and param_name not in kwargs:
                    # If parameter has a default value, use it
                    if "default" in param_def and param_def["default"] is not None:
                        kwargs[param_name] = param_def["default"]
                    else:
                        click.echo(f"Missing required parameter: {param_name}", err=True)
                        sys.exit(1)
        
        # Run the tool with positional args if provided, otherwise use kwargs
        if args:
            result = run_tool(tool_name, *args)
        else:
            result = run_tool(tool_name, **kwargs)
        
        # Print the result
        if isinstance(result, dict):
            click.echo(json.dumps(result, indent=2))
        else:
            click.echo(result)
    
    except Exception as e:
        click.echo(f"Error running tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
@click.argument("args", nargs=-1)
@click.option("--param", "-p", multiple=True, help="Tool parameters in the format key=value (for backward compatibility)")
def r(tool_name, args, param):
    """Alias for 'run' - Run a tool with the given arguments."""
    run.callback(tool_name, args, param)


@click.command()
@click.argument("tool_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def remove(tool_name, force):
    """Remove a custom tool."""
    try:
        # Confirm removal unless force flag is set
        if not force and not click.confirm(f"Are you sure you want to remove tool '{tool_name}'?"):
            click.echo("Operation cancelled.")
            return
        
        # Remove the tool
        remove_tool(tool_name)
        
        click.echo(f"Tool '{tool_name}' removed successfully.")
        
    except FileNotFoundError:
        click.echo(f"Tool '{tool_name}' not found.", err=True)
        sys.exit(1)
    except Exception as e:
        click.echo(f"Error removing tool: {e}", err=True)
        sys.exit(1)


@click.command()
@click.argument("tool_name")
@click.option("--force", "-f", is_flag=True, help="Force removal without confirmation")
def rm(tool_name, force):
    """Alias for 'remove' - Remove a custom tool."""
    remove.callback(tool_name, force)

================
File: evai/cli/__init__.py
================
"""Commands submodule for EVAI CLI."""

================
File: evai/cli/cli.py
================
"""Command-line interface for EVAI."""

import sys
import os
import json
import click
import importlib
import pkgutil
from evai import __version__
from evai.tool_storage import (
    get_tool_dir, 
    save_tool_metadata, 
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    list_tools,
    run_tool,
    load_tool_metadata
)
from evai.command_storage import get_command_dir, save_command_metadata, list_commands, import_command_module
from evai.llm_client import (
    generate_default_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)
from rich.console import Console
from rich.syntax import Syntax
from rich.panel import Panel
import yaml
import logging

# Initialize rich console
console = Console()

# Type mapping for Click parameter types
TYPE_MAP = {
    "string": click.STRING,
    "integer": click.INT,
    "float": click.FLOAT,
    "boolean": click.BOOL,
}

# Create an AliasedGroup class to support command aliases
class AliasedGroup(click.Group):
    def get_command(self, ctx, cmd_name):
        # Try to get command by name
        rv = click.Group.get_command(self, ctx, cmd_name)
        if rv is not None:
            return rv
        
        # Try to match aliases
        matches = [x for x in self.list_commands(ctx) if x.startswith(cmd_name)]
        if not matches:
            return None
        elif len(matches) == 1:
            return click.Group.get_command(self, ctx, matches[0])
        
        ctx.fail(f"Too many matches: {', '.join(sorted(matches))}")


@click.group(help="EVAI CLI - Command-line interface for EVAI")
@click.version_option(version=__version__, prog_name="evai")
def cli():
    """EVAI CLI - Command-line interface for EVAI."""
    pass


@cli.group(cls=AliasedGroup)
def tools():
    """Manage custom tools."""
    pass

# Tool functions have been moved to evai/cli/commands/tool.py

@cli.group(cls=AliasedGroup)
def commands():
    """Manage user-defined commands."""
    pass

@cli.group(cls=AliasedGroup)
def user():
    """User-defined commands."""
    pass

def create_user_command(command_metadata: dict):
    """Create a Click command from command metadata."""
    command_name = command_metadata["name"]
    description = command_metadata.get("description", "")
    arg_names = [arg["name"] for arg in command_metadata.get("arguments", [])]

    def callback(*args, **kwargs):
        module = import_command_module(command_name)
        run_func = getattr(module, "run")
        params = dict(zip(arg_names, args))
        params.update(kwargs)
        result = run_func(**params)
        click.echo(json.dumps(result, indent=2))
        return result

    command = click.command(name=command_name, help=description)(callback)

    # Add arguments
    for arg in command_metadata.get("arguments", []):
        command = click.argument(
            arg["name"],
            type=TYPE_MAP.get(arg.get("type", "string"), click.STRING)
        )(command)

    # Add options
    for opt in command_metadata.get("options", []):
        command = click.option(
            f"--{opt['name']}",
            type=TYPE_MAP.get(opt.get("type", "string"), click.STRING),
            help=opt.get("description", ""),
            required=opt.get("required", False),
            default=opt.get("default", None)
        )(command)

    return command

def load_user_commands():
    """Load and register user-defined commands from ~/.evai/commands."""
    commands_list = list_commands()
    for cmd_meta in commands_list:
        try:
            command = create_user_command(cmd_meta)
            user.add_command(command)
        except Exception as e:
            logger.warning(f"Failed to load command {cmd_meta['name']}: {e}")

@cli.command()
@click.option("--name", "-n", default="EVAI Tools", help="Name of the MCP server")
def server(name):
    """Start an MCP server exposing all tools."""
    try:
        # Import here to avoid dependency issues if MCP is not installed
        from .mcp_server import run_server
        
        click.echo(f"Starting MCP server '{name}'...")
        click.echo("Press Ctrl+C to stop the server.")
        
        # Run the server
        run_server(name)
    except ImportError as e:
        click.echo(f"Error: {e}", err=True)
        click.echo("Please install the MCP Python SDK with: pip install mcp", err=True)
        sys.exit(1)
    except Exception as e:
        click.echo(f"Error starting MCP server: {e}", err=True)
        sys.exit(1)


# Automatically add all commands from the commands submodule
def import_commands():
    """Import all commands from the commands submodule and add them to the appropriate groups."""
    from evai.cli import commands as commands_module
    
    # Get the package path
    package_path = os.path.dirname(commands_module.__file__)
    
    # Iterate through all modules in the commands package
    for _, module_name, _ in pkgutil.iter_modules([package_path]):
        # Import the module
        module = importlib.import_module(f"evai.cli.commands.{module_name}")
        
        # Find all Click commands in the module
        for attr_name in dir(module):
            attr = getattr(module, attr_name)
            
            # Check if it's a Click command
            if isinstance(attr, click.Command):
                # Determine which group to add the command to
                if module_name == "tools" or module_name == "llmadd":
                    # Add tool-related commands to the tools group
                    tools.add_command(attr)
                elif module_name == "commands" or module_name == "cmdllmadd":
                    # Add command-related commands to the commands group
                    commands.add_command(attr)
                else:
                    # Default to tools group for anything else
                    tools.add_command(attr)


# Import commands
import_commands()

# Load user-defined commands
load_user_commands()


def main():
    """Run the EVAI CLI."""
    # If no arguments are provided, show help
    if len(sys.argv) == 1:
        sys.argv.append("--help")
    return cli()


if __name__ == "__main__":
    sys.exit(main())

================
File: evai/docs/ARCHITECTURE.md
================
```mermaid

flowchart TB
    %% External Systems
    User([User])
    Claude["Claude Desktop\n(MCP Client)"]
    Terminal["Terminal/iTerm\n(CLI Interface)"]
    AnthropicAPI["Anthropic API\n(LLM Service)"]
    
    %% Main evai CLI Application
    subgraph evai["evai CLI Application"]
        direction TB
        
        Core["CLI Core\nCommand Parsing, Execution, I/O"]
        
        subgraph Components["Core Components"]
            direction LR
            MCPServer["Embedded MCP Server\n(stdio Transport)"]
            LLMClient["LLM Client\n(Anthropic API Interface)"]
            SelfMod["Self-Modification System\nCode Analysis, Generation, Integration"]
        end
        
        subgraph Storage["Storage"]
            Commands["Command Repository\nDefinitions, Metadata"]
            CodeStore["Code Storage\nCommand Implementations"]
        end
        
        Core --> Components
        Core --> Storage
        SelfMod --> CodeStore
        SelfMod --> Commands
        
        %% Dynamic tool registration pathway
        Commands -- "New/Modified\nCommands" --> MCPServer
    end
    
    %% Relations/Connections
    User --> Claude
    User --> Terminal
    Claude -- "MCP stdio Protocol" --> MCPServer
    Terminal -- "CLI Commands" --> Core
    LLMClient -- "API Calls" --> AnthropicAPI
    
    %% MCP Tools
    subgraph MCPTools["MCP Tools Exposed"]
        ListCommands["list-commands"]
        AddCommand["add-command"]
        ExecuteCommand["execute-command"]
        ModifyCommand["modify-command"]
        DynamicTools["Dynamic Command Tools\n(Auto-registered from Command Repository)"]
    end
    
    MCPServer --> MCPTools
    Commands -- "Dynamic Registration" --> MCPServer
    
    classDef externalSystems fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef evaiCore fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef storage fill:#f1f8e9,stroke:#558b2f,stroke-width:2px
    classDef selfMod fill:#ffebee,stroke:#c62828,stroke-width:2px
    classDef component fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef tools fill:#f3e5f5,stroke:#7b1fa2,stroke-width:1px
    
    class User,Claude,Terminal,AnthropicAPI externalSystems
    class Core evaiCore
    class Commands,CodeStore storage
    class SelfMod selfMod
    class MCPServer,LLMClient component
    class ListCommands,AddCommand,ExecuteCommand,ModifyCommand tools
```

================
File: evai/docs/cmd_plan.md
================
Below is a detailed proposal to add functionality for creating new commands in the EVAI CLI, mirroring the existing tool creation process. The new commands will reside in ~/.evai/commands, be loaded on EVAI startup, and be registered in Click like the existing command structure. This proposal includes a step-by-step approach with specific instructions, code snippets, and a final prompt tailored for a less intelligent sub-LLM to implement incrementally, stopping for feedback at each step.
Proposal to Add Command Creation Functionality to EVAI CLI
General Approach
To meet your requirements, we'll extend the EVAI CLI by:
Mirroring the Tool Structure: Commands will follow a similar directory and file structure to tools (~/.evai/commands/<command_name>/ with command.yaml and command.py), leveraging existing utilities where possible.
Adding Command Storage: Create a new command_storage.py module to handle command-specific file operations, parallel to tool_storage.py.
Enhancing CLI: Introduce a commands group with an add subcommand, similar to tools add, to create new commands interactively.
Dynamic Registration: Modify cli.py to load and register user-defined commands in Click at startup, ensuring they integrate seamlessly with the existing structure.
Ensuring Specificity: Each step will be discrete, with clear instructions and code, stopping for feedback to keep the implementation on track.
This approach ensures that commands are treated as first-class citizens in the CLI, with a creation process akin to tools, and are automatically available upon startup.
Discrete Steps for Implementation
Step 1: Create command_storage.py Module
Purpose: Establish a dedicated module for command file operations, mirroring tool_storage.py.
Instructions:
Create a new file evai/command_storage.py.
Define functions to manage the ~/.evai/commands directory, load/save metadata, list commands, and import command modules.
Use pathlib.Path for cross-platform compatibility, similar to tool_storage.py.
Code:
python


"""Command storage utilities for EVAI CLI."""

import os
import yaml
from pathlib import Path
import importlib.util
import logging

logger = logging.getLogger(__name__)

COMMANDS_DIR = Path.home() / ".evai" / "commands"

def get_command_dir(command_name: str) -> Path:
    """Get the directory path for a command and create it if it doesn't exist."""
    command_dir = COMMANDS_DIR / command_name
    command_dir.mkdir(parents=True, exist_ok=True)
    return command_dir

def load_command_metadata(path: Path) -> dict:
    """Load command metadata from command.yaml."""
    yaml_path = path / "command.yaml"
    if not yaml_path.exists():
        raise FileNotFoundError(f"Command metadata file not found: {yaml_path}")
    with yaml_path.open("r") as f:
        return yaml.safe_load(f) or {}

def save_command_metadata(path: Path, data: dict) -> None:
    """Save command metadata to command.yaml."""
    yaml_path = path / "command.yaml"
    with yaml_path.open("w") as f:
        yaml.dump(data, f, default_flow_style=False)

def list_commands() -> list[dict]:
    """List all available commands."""
    if not COMMANDS_DIR.exists():
        return []
    commands = []
    for cmd_dir in COMMANDS_DIR.iterdir():
        if cmd_dir.is_dir():
            try:
                metadata = load_command_metadata(cmd_dir)
                if not metadata.get("disabled", False):
                    commands.append({
                        "name": metadata.get("name", cmd_dir.name),
                        "description": metadata.get("description", "No description"),
                        "path": cmd_dir
                    })
            except Exception as e:
                logger.warning(f"Error loading command {cmd_dir.name}: {e}")
    return commands

def import_command_module(command_name: str):
    """Dynamically import a command module."""
    cmd_dir = get_command_dir(command_name)
    py_path = cmd_dir / "command.py"
    if not py_path.exists():
        raise FileNotFoundError(f"Command implementation file not found: {py_path}")
    spec = importlib.util.spec_from_file_location(f"evai.commands.{command_name}", py_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module
Stop for Feedback: After implementing this step, verify that the module functions work as expected (e.g., get_command_dir creates directories, load_command_metadata reads YAML files correctly). Provide feedback before proceeding.
Step 2: Create Command Templates
Purpose: Define default templates for command.yaml and command.py, stored in evai/templates/, to streamline command creation.
Instructions:
Create evai/templates/sample_command.yaml with a metadata structure matching the tools' format but tailored for commands.
Create evai/templates/sample_command.py with a run(**kwargs) function, as required for dynamic registration.
Code:
evai/templates/sample_command.yaml:
yaml
name: "{command_name}"
description: "Default description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
evai/templates/sample_command.py:
python
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}
Stop for Feedback: Confirm that the templates are correctly placed and formatted. Test loading them manually to ensure they parse without errors. Provide feedback before moving on.
Step 3: Add commands Group and add Subcommand to cli.py
Purpose: Extend the CLI with a commands group and an add subcommand to create new commands, similar to tools add.
Instructions:
In evai/cli/cli.py, add a commands group using AliasedGroup.
Define an add subcommand that creates the command directory, saves default metadata, and writes the implementation file.
Reuse existing utilities from command_storage.py and adapt logic from tools.py.
Code:
Update evai/cli/cli.py:
python
# ... (existing imports remain unchanged)
from evai.command_storage import get_command_dir, save_command_metadata

# ... (existing cli group remains unchanged)

@cli.group(cls=AliasedGroup)
def commands():
    """Manage user-defined commands."""
    pass

@commands.command()
@click.argument("command_name")
def add(command_name):
    """Add a new custom command."""
    try:
        cmd_dir = get_command_dir(command_name)
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)

        # Load default metadata template
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.yaml"), "r") as f:
            metadata_content = f.read().replace("{command_name}", command_name)
            default_metadata = yaml.safe_load(metadata_content)

        # Save metadata
        save_command_metadata(cmd_dir, default_metadata)

        # Create default command.py
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.py"), "r") as f:
            with open(cmd_dir / "command.py", "w") as py_file:
                py_file.write(f.read())

        click.echo(f"Command '{command_name}' created successfully.")
        click.echo(f"- Metadata: {cmd_dir / 'command.yaml'}")
        click.echo(f"- Implementation: {cmd_dir / 'command.py'}")
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)

# ... (rest of cli.py remains unchanged, including import_commands and main)
Stop for Feedback: Test the evai commands add <command_name> command to ensure it creates the directory and files as expected. Verify the output messages and file contents. Provide feedback before proceeding.
Step 4: Dynamically Load and Register Commands in cli.py
Purpose: Load commands from ~/.evai/commands at startup and register them in Click under a user group.
Instructions:
Add a user group to cli.py for user-defined commands.
Implement a load_user_commands() function to scan ~/.evai/commands, create Click commands dynamically, and add them to the user group.
Use command_storage.py functions and Click's dynamic command creation capabilities.
Code:
Update evai/cli/cli.py:
python
# ... (add to existing imports)
from evai.command_storage import list_commands, import_command_module

# Type mapping for Click parameter types
TYPE_MAP = {
    "string": click.STRING,
    "integer": click.INT,
    "float": click.FLOAT,
    "boolean": click.BOOL,
}

# ... (existing cli and tools groups remain unchanged)

@cli.group(cls=AliasedGroup)
def user():
    """User-defined commands."""
    pass

def create_user_command(command_metadata: dict):
    """Create a Click command from command metadata."""
    command_name = command_metadata["name"]
    description = command_metadata.get("description", "")
    arg_names = [arg["name"] for arg in command_metadata.get("arguments", [])]

    def callback(*args, **kwargs):
        module = import_command_module(command_name)
        run_func = getattr(module, "run")
        params = dict(zip(arg_names, args))
        params.update(kwargs)
        result = run_func(**params)
        click.echo(result)

    command = click.command(name=command_name, help=description)(callback)

    # Add arguments
    for arg in command_metadata.get("arguments", []):
        command = click.argument(
            arg["name"],
            type=TYPE_MAP.get(arg.get("type", "string"), click.STRING)
        )(command)

    # Add options
    for opt in command_metadata.get("options", []):
        command = click.option(
            f"--{opt['name']}",
            type=TYPE_MAP.get(opt.get("type", "string"), click.STRING),
            help=opt.get("description", ""),
            required=opt.get("required", False),
            default=opt.get("default", None)
        )(command)

    return command

def load_user_commands():
    """Load and register user-defined commands from ~/.evai/commands."""
    commands_list = list_commands()
    for cmd_meta in commands_list:
        try:
            command = create_user_command(cmd_meta)
            user.add_command(command)
        except Exception as e:
            logger.warning(f"Failed to load command {cmd_meta['name']}: {e}")

# Call load_user_commands after defining groups
load_user_commands()

# ... (rest of cli.py remains unchanged)
Stop for Feedback: Run evai and check if the user group appears in the help output (evai --help). Create a test command and verify its listed under evai user --help and executable (e.g., evai user testcommand). Provide feedback before final integration.
Final Prompt for Sub-LLM
Below is a comprehensive prompt combining all steps, designed for a less intelligent sub-LLM. It includes explicit instructions, stresses specificity, and enforces discrete steps with feedback stops.
markdown
You are tasked with adding functionality to the EVAI CLI to allow users to create new commands, mirroring the existing tool creation process. The new commands will reside in `~/.evai/commands` and be loaded on EVAI startup, registered in Click like the existing command structure. Follow these **specific** steps EXACTLY as outlined, stopping after each step for feedback before proceeding. Do not skip steps or combine them. Assume the existing codebase (provided in context) is correct and must be extended without altering unrelated parts unless specified.

---

### Step 1: Create `command_storage.py` Module
- **Task**: Create a new file `evai/command_storage.py` to handle command file operations, similar to `tool_storage.py`.
- **Instructions**:
  - Use `pathlib.Path` for paths.
  - Define `COMMANDS_DIR = Path.home() / ".evai" / "commands"`.
  - Implement these functions:
    - `get_command_dir(command_name: str) -> Path`: Returns `COMMANDS_DIR / command_name`, creating it if needed.
    - `load_command_metadata(path: Path) -> dict`: Loads `command.yaml` from the path, raises FileNotFoundError if missing.
    - `save_command_metadata(path: Path, data: dict) -> None`: Saves dict to `command.yaml`.
    - `list_commands() -> list[dict]`: Lists all commands, skipping disabled ones, returns list of dicts with "name", "description", "path".
    - `import_command_module(command_name: str)`: Imports `command.py` from the command directory.
  - Include basic logging with `logger = logging.getLogger(__name__)`.
- **Code**:
```python
"""Command storage utilities for EVAI CLI."""

import os
import yaml
from pathlib import Path
import importlib.util
import logging

logger = logging.getLogger(__name__)

COMMANDS_DIR = Path.home() / ".evai" / "commands"

def get_command_dir(command_name: str) -> Path:
    """Get the directory path for a command and create it if it doesn't exist."""
    command_dir = COMMANDS_DIR / command_name
    command_dir.mkdir(parents=True, exist_ok=True)
    return command_dir

def load_command_metadata(path: Path) -> dict:
    """Load command metadata from command.yaml."""
    yaml_path = path / "command.yaml"
    if not yaml_path.exists():
        raise FileNotFoundError(f"Command metadata file not found: {yaml_path}")
    with yaml_path.open("r") as f:
        return yaml.safe_load(f) or {}

def save_command_metadata(path: Path, data: dict) -> None:
    """Save command metadata to command.yaml."""
    yaml_path = path / "command.yaml"
    with yaml_path.open("w") as f:
        yaml.dump(data, f, default_flow_style=False)

def list_commands() -> list[dict]:
    """List all available commands."""
    if not COMMANDS_DIR.exists():
        return []
    commands = []
    for cmd_dir in COMMANDS_DIR.iterdir():
        if cmd_dir.is_dir():
            try:
                metadata = load_command_metadata(cmd_dir)
                if not metadata.get("disabled", False):
                    commands.append({
                        "name": metadata.get("name", cmd_dir.name),
                        "description": metadata.get("description", "No description"),
                        "path": cmd_dir
                    })
            except Exception as e:
                logger.warning(f"Error loading command {cmd_dir.name}: {e}")
    return commands

def import_command_module(command_name: str):
    """Dynamically import a command module."""
    cmd_dir = get_command_dir(command_name)
    py_path = cmd_dir / "command.py"
    if not py_path.exists():
        raise FileNotFoundError(f"Command implementation file not found: {py_path}")
    spec = importlib.util.spec_from_file_location(f"evai.commands.{command_name}", py_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module
Stop: After implementing this, stop and wait for feedback. Test each function manually (e.g., create a directory, save/load YAML, list commands) and report any issues.
Step 2: Create Command Templates
Task: Add sample_command.yaml and sample_command.py to evai/templates/ for default command files.
Instructions:
Create evai/templates/sample_command.yaml with placeholders {command_name} where needed.
Create evai/templates/sample_command.py with a run(**kwargs) function.
Ensure the YAML includes arguments and options fields instead of params to distinguish from tools.
Code:
evai/templates/sample_command.yaml:
yaml
name: "{command_name}"
description: "Default description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
evai/templates/sample_command.py:
python
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}
Stop: Verify the templates are in evai/templates/ and can be read correctly. Provide feedback on their contents and placement.
Step 3: Add commands Group and add Subcommand to cli.py
Task: Extend evai/cli/cli.py with a commands group and add subcommand.
Instructions:
Add @cli.group(cls=AliasedGroup) for commands.
Add @commands.command() for add, taking a command_name argument.
Use get_command_dir, save_command_metadata, and file operations to create the command.
Check for existing commands and exit with an error if found.
Code:
python
# In evai/cli/cli.py, add to imports:
from evai.command_storage import get_command_dir, save_command_metadata

# Add after existing groups:
@cli.group(cls=AliasedGroup)
def commands():
    """Manage user-defined commands."""
    pass

@commands.command()
@click.argument("command_name")
def add(command_name):
    """Add a new custom command."""
    try:
        cmd_dir = get_command_dir(command_name)
        if list(cmd_dir.iterdir()):  # Check if directory is non-empty
            click.echo(f"Command '{command_name}' already exists.", err=True)
            sys.exit(1)

        # Load default metadata template
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.yaml"), "r") as f:
            metadata_content = f.read().replace("{command_name}", command_name)
            default_metadata = yaml.safe_load(metadata_content)

        # Save metadata
        save_command_metadata(cmd_dir, default_metadata)

        # Create default command.py
        with open(os.path.join(os.path.dirname(__file__), "../templates/sample_command.py"), "r") as f:
            with open(cmd_dir / "command.py", "w") as py_file:
                py_file.write(f.read())

        click.echo(f"Command '{command_name}' created successfully.")
        click.echo(f"- Metadata: {cmd_dir / 'command.yaml'}")
        click.echo(f"- Implementation: {cmd_dir / 'command.py'}")
    except Exception as e:
        click.echo(f"Error creating command: {e}", err=True)
        sys.exit(1)
Stop: Test evai commands add testcommand and check if the files are created in ~/.evai/commands/testcommand/. Provide feedback on functionality and output.
Step 4: Dynamically Load and Register Commands in cli.py
Task: Update evai/cli/cli.py to load and register commands at startup under a user group.
Instructions:
Add a user group with @cli.group(cls=AliasedGroup).
Define TYPE_MAP for parameter types.
Implement create_user_command to generate Click commands from metadata.
Implement load_user_commands to register all commands.
Call load_user_commands() after group definitions.
Code:
python
# In evai/cli/cli.py, add to imports:
from evai.command_storage import list_commands, import_command_module

# Add before groups:
TYPE_MAP = {
    "string": click.STRING,
    "integer": click.INT,
    "float": click.FLOAT,
    "boolean": click.BOOL,
}

# Add after 'commands' group:
@cli.group(cls=AliasedGroup)
def user():
    """User-defined commands."""
    pass

def create_user_command(command_metadata: dict):
    """Create a Click command from command metadata."""
    command_name = command_metadata["name"]
    description = command_metadata.get("description", "")
    arg_names = [arg["name"] for arg in command_metadata.get("arguments", [])]

    def callback(*args, **kwargs):
        module = import_command_module(command_name)
        run_func = getattr(module, "run")
        params = dict(zip(arg_names, args))
        params.update(kwargs)
        result = run_func(**params)
        click.echo(result)

    command = click.command(name=command_name, help=description)(callback)

    # Add arguments
    for arg in command_metadata.get("arguments", []):
        command = click.argument(
            arg["name"],
            type=TYPE_MAP.get(arg.get("type", "string"), click.STRING)
        )(command)

    # Add options
    for opt in command_metadata.get("options", []):
        command = click.option(
            f"--{opt['name']}",
            type=TYPE_MAP.get(opt.get("type", "string"), click.STRING),
            help=opt.get("description", ""),
            required=opt.get("required", False),
            default=opt.get("default", None)
        )(command)

    return command

def load_user_commands():
    """Load and register user-defined commands from ~/.evai/commands."""
    commands_list = list_commands()
    for cmd_meta in commands_list:
        try:
            command = create_user_command(cmd_meta)
            user.add_command(command)
        except Exception as e:
            logger.warning(f"Failed to load command {cmd_meta['name']}: {e}")

# Add after group definitions, before import_commands():
load_user_commands()
Stop: Run evai --help and evai user --help to verify the user group and loaded commands. Test a command (e.g., evai user testcommand) and provide feedback on registration and execution.
Final Notes: After completing all steps, the EVAI CLI will support command creation via evai commands add and load them at startup under the user group. Provide final feedback to confirm the implementation meets all requirements.
Please proceed with Step 1 and stop for feedback.
```

================
File: evai/docs/evai_spec.md
================
# Comprehensive Specification for EVAI CLI Custom Commands Integration

## Overview
This specification outlines the requirements, architecture decisions, data management strategies, error handling, and testing guidelines necessary for integrating user-created commands via the EVAI CLI, exposing them through the MCP server, and optionally interacting with LLM calls.

## Requirements

### Functional Requirements
- Users must be able to define custom commands through the EVAI CLI.
- Custom commands should automatically be exposed through the MCP server.
- Commands can optionally invoke LLM interactions.
- Commands must be executable from two entry points:
  - Claude Desktop (interactive)
  - Terminal CLI (`evai`)

### Technical Requirements
- Commands are stateless and independently executable.
- Multi-line editing support through the user's preferred `$EDITOR`.
- LLM provides reasonable defaults for command metadata, which users can manually review and iteratively refine.
- Command metadata and implementation are stored persistently under:
  ```
  ~/.evai/commands/<command-name>/
  ```

### Metadata Management
- Metadata must be YAML-formatted:

```yaml
name: string (required)
description: string (required)
params:
  - name: string (required)
    type: string (default: "string")
    description: string (optional, default: "")
    required: boolean (default: true)
    default: any (optional, default: null)
hidden: boolean (default: false)
disabled: boolean (default: false)
mcp_integration:
  enabled: boolean (default: true)
  metadata:
    endpoint: string (default auto-generated)
    method: string (default: "POST")
    authentication_required: boolean (default: false)
llm_interaction:
  enabled: boolean (default: false)
  auto_apply: boolean (default: true)
  max_llm_turns: integer (default: 15)
```

### File Structure
Each command has the following standardized files:
```
~/.evai/commands/<command-name>/command.yaml
~/.evai/commands/<command-name>/command.py
```

## Architecture
- EVAI CLI handles command creation and editing.
- Commands exposed via MCP through automatic YAML metadata parsing.
- Stateless runtime ensures consistent behavior between Claude Desktop and CLI invocations.

### Command Creation Workflow
1. User initiates command creation via CLI:
   ```
   evai command add <command-name>
   ```
2. LLM generates default metadata.
3. Metadata reviewed and edited by the user through interactive CLI session.
4. User edits implementation code via `$EDITOR`.
5. Post-editing validation:
   - YAML syntax validation.
   - Python linting (flake8 default minimal rules).
   - Iterative LLM fixes (auto-applied, max 15 iterations).

## Data Handling
- Commands are stateless by default, no persistent command runtime data storage.
- Persistent storage limited strictly to metadata and command implementation files under user-managed `~/.evai` directory.

## Error Handling Strategy
- YAML Syntax Errors:
  - Immediate validation feedback after editing.
- Python Implementation Errors:
  - Auto-linting via `flake8`.
  - Iterative LLM-aided error correction.
  - Informative terminal messaging between LLM iterations to notify user of issues and progress.
- Runtime Execution Errors:
  - Clear, descriptive terminal messages provided upon exceptions.
  - Errors during LLM interaction clearly indicated with retry guidance.

## Testing Plan

### Unit Tests
- Validate YAML parsing.
- Verify command metadata correctly translates to MCP exposure.
- Command implementation execution correctness via isolated unit tests.

### Integration Tests
- Test commands via both CLI and Claude Desktop execution paths.
- MCP exposure tests verifying endpoint availability and authentication handling.

### End-to-End (E2E) Tests
- Command creation workflow (LLM metadata generation, user review/edit, file creation).
- Full-cycle test from command creation to execution via CLI and Claude Desktop.

### Linting and Validation Tests
- Automated `flake8` checks in CI/CD pipeline.
- YAML schema validation integrated into command creation workflow.

### User Acceptance Tests (UAT)
- Manual user workflow tests for command creation/editing.
- Verify user clarity and ease of understanding of error messaging and correction workflows.

## Deployment
- No explicit deployment steps required beyond ensuring `~/.evai` directory is correctly structured.
- Automatic MCP exposure upon command file creation and metadata validation.

## Security Considerations
- Commands execute under the user's environment.
- Default no-authentication model for MCP endpoints, configurable if required.
- Clearly documented guidance to users regarding secure practices for handling sensitive command parameters and implementations.

This specification provides developers with all details required to commence immediate and confident implementation of the EVAI CLI Custom Commands Integration.

================
File: evai/docs/mcp_python_sdk.md
================
# MCP Python SDK

<div align="center">

<strong>Python implementation of the Model Context Protocol (MCP)</strong>

[![PyPI][pypi-badge]][pypi-url]
[![MIT licensed][mit-badge]][mit-url]
[![Python Version][python-badge]][python-url]
[![Documentation][docs-badge]][docs-url]
[![Specification][spec-badge]][spec-url]
[![GitHub Discussions][discussions-badge]][discussions-url]

</div>

<!-- omit in toc -->
## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Quickstart](#quickstart)
- [What is MCP?](#what-is-mcp)
- [Core Concepts](#core-concepts)
  - [Server](#server)
  - [Resources](#resources)
  - [Tools](#tools)
  - [Prompts](#prompts)
  - [Images](#images)
  - [Context](#context)
- [Running Your Server](#running-your-server)
  - [Development Mode](#development-mode)
  - [Claude Desktop Integration](#claude-desktop-integration)
  - [Direct Execution](#direct-execution)
- [Examples](#examples)
  - [Echo Server](#echo-server)
  - [SQLite Explorer](#sqlite-explorer)
- [Advanced Usage](#advanced-usage)
  - [Low-Level Server](#low-level-server)
  - [Writing MCP Clients](#writing-mcp-clients)
  - [MCP Primitives](#mcp-primitives)
  - [Server Capabilities](#server-capabilities)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [License](#license)

[pypi-badge]: https://img.shields.io/pypi/v/mcp.svg
[pypi-url]: https://pypi.org/project/mcp/
[mit-badge]: https://img.shields.io/pypi/l/mcp.svg
[mit-url]: https://github.com/modelcontextprotocol/python-sdk/blob/main/LICENSE
[python-badge]: https://img.shields.io/pypi/pyversions/mcp.svg
[python-url]: https://www.python.org/downloads/
[docs-badge]: https://img.shields.io/badge/docs-modelcontextprotocol.io-blue.svg
[docs-url]: https://modelcontextprotocol.io
[spec-badge]: https://img.shields.io/badge/spec-spec.modelcontextprotocol.io-blue.svg
[spec-url]: https://spec.modelcontextprotocol.io
[discussions-badge]: https://img.shields.io/github/discussions/modelcontextprotocol/python-sdk
[discussions-url]: https://github.com/modelcontextprotocol/python-sdk/discussions

## Overview

The Model Context Protocol allows applications to provide context for LLMs in a standardized way, separating the concerns of providing context from the actual LLM interaction. This Python SDK implements the full MCP specification, making it easy to:

- Build MCP clients that can connect to any MCP server
- Create MCP servers that expose resources, prompts and tools
- Use standard transports like stdio and SSE
- Handle all MCP protocol messages and lifecycle events

## Installation

We recommend using [uv](https://docs.astral.sh/uv/) to manage your Python projects:

```bash
uv add "mcp[cli]"
```

Alternatively:
```bash
pip install mcp
```

## Quickstart

Let's create a simple MCP server that exposes a calculator tool and some data:

```python
# server.py
from mcp.server.fastmcp import FastMCP

# Create an MCP server
mcp = FastMCP("Demo")

# Add an addition tool
@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

# Add a dynamic greeting resource
@mcp.resource("greeting://{name}")
def get_greeting(name: str) -> str:
    """Get a personalized greeting"""
    return f"Hello, {name}!"
```

You can install this server in [Claude Desktop](https://claude.ai/download) and interact with it right away by running:
```bash
mcp install server.py
```

Alternatively, you can test it with the MCP Inspector:
```bash
mcp dev server.py
```

## What is MCP?

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io) lets you build servers that expose data and functionality to LLM applications in a secure, standardized way. Think of it like a web API, but specifically designed for LLM interactions. MCP servers can:

- Expose data through **Resources** (think of these sort of like GET endpoints; they are used to load information into the LLM's context)
- Provide functionality through **Tools** (sort of like POST endpoints; they are used to execute code or otherwise produce a side effect)
- Define interaction patterns through **Prompts** (reusable templates for LLM interactions)
- And more!

## Core Concepts

### Server

The FastMCP server is your core interface to the MCP protocol. It handles connection management, protocol compliance, and message routing:

```python
# Add lifespan support for startup/shutdown with strong typing
from dataclasses import dataclass
from typing import AsyncIterator
from mcp.server.fastmcp import FastMCP

# Create a named server
mcp = FastMCP("My App")

# Specify dependencies for deployment and development
mcp = FastMCP("My App", dependencies=["pandas", "numpy"])

@dataclass
class AppContext:
    db: Database  # Replace with your actual DB type

@asynccontextmanager
async def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:
    """Manage application lifecycle with type-safe context"""
    try:
        # Initialize on startup
        await db.connect()
        yield AppContext(db=db)
    finally:
        # Cleanup on shutdown
        await db.disconnect()

# Pass lifespan to server
mcp = FastMCP("My App", lifespan=app_lifespan)

# Access type-safe lifespan context in tools
@mcp.tool()
def query_db(ctx: Context) -> str:
    """Tool that uses initialized resources"""
    db = ctx.request_context.lifespan_context["db"]
    return db.query()
```

### Resources

Resources are how you expose data to LLMs. They're similar to GET endpoints in a REST API - they provide data but shouldn't perform significant computation or have side effects:

```python
@mcp.resource("config://app")
def get_config() -> str:
    """Static configuration data"""
    return "App configuration here"

@mcp.resource("users://{user_id}/profile")
def get_user_profile(user_id: str) -> str:
    """Dynamic user data"""
    return f"Profile data for user {user_id}"
```

### Tools

Tools let LLMs take actions through your server. Unlike resources, tools are expected to perform computation and have side effects:

```python
@mcp.tool()
def calculate_bmi(weight_kg: float, height_m: float) -> float:
    """Calculate BMI given weight in kg and height in meters"""
    return weight_kg / (height_m ** 2)

@mcp.tool()
async def fetch_weather(city: str) -> str:
    """Fetch current weather for a city"""
    async with httpx.AsyncClient() as client:
        response = await client.get(f"https://api.weather.com/{city}")
        return response.text
```

### Prompts

Prompts are reusable templates that help LLMs interact with your server effectively:

```python
@mcp.prompt()
def review_code(code: str) -> str:
    return f"Please review this code:\n\n{code}"

@mcp.prompt()
def debug_error(error: str) -> list[Message]:
    return [
        UserMessage("I'm seeing this error:"),
        UserMessage(error),
        AssistantMessage("I'll help debug that. What have you tried so far?")
    ]
```

### Images

FastMCP provides an `Image` class that automatically handles image data:

```python
from mcp.server.fastmcp import FastMCP, Image
from PIL import Image as PILImage

@mcp.tool()
def create_thumbnail(image_path: str) -> Image:
    """Create a thumbnail from an image"""
    img = PILImage.open(image_path)
    img.thumbnail((100, 100))
    return Image(data=img.tobytes(), format="png")
```

### Context

The Context object gives your tools and resources access to MCP capabilities:

```python
from mcp.server.fastmcp import FastMCP, Context

@mcp.tool()
async def long_task(files: list[str], ctx: Context) -> str:
    """Process multiple files with progress tracking"""
    for i, file in enumerate(files):
        ctx.info(f"Processing {file}")
        await ctx.report_progress(i, len(files))
        data, mime_type = await ctx.read_resource(f"file://{file}")
    return "Processing complete"
```

## Running Your Server

### Development Mode

The fastest way to test and debug your server is with the MCP Inspector:

```bash
mcp dev server.py

# Add dependencies
mcp dev server.py --with pandas --with numpy

# Mount local code
mcp dev server.py --with-editable .
```

### Claude Desktop Integration

Once your server is ready, install it in Claude Desktop:

```bash
mcp install server.py

# Custom name
mcp install server.py --name "My Analytics Server"

# Environment variables
mcp install server.py -v API_KEY=abc123 -v DB_URL=postgres://...
mcp install server.py -f .env
```

### Direct Execution

For advanced scenarios like custom deployments:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("My App")

if __name__ == "__main__":
    mcp.run()
```

Run it with:
```bash
python server.py
# or
mcp run server.py
```

## Examples

### Echo Server

A simple server demonstrating resources, tools, and prompts:

```python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Echo")

@mcp.resource("echo://{message}")
def echo_resource(message: str) -> str:
    """Echo a message as a resource"""
    return f"Resource echo: {message}"

@mcp.tool()
def echo_tool(message: str) -> str:
    """Echo a message as a tool"""
    return f"Tool echo: {message}"

@mcp.prompt()
def echo_prompt(message: str) -> str:
    """Create an echo prompt"""
    return f"Please process this message: {message}"
```

### SQLite Explorer

A more complex example showing database integration:

```python
from mcp.server.fastmcp import FastMCP
import sqlite3

mcp = FastMCP("SQLite Explorer")

@mcp.resource("schema://main")
def get_schema() -> str:
    """Provide the database schema as a resource"""
    conn = sqlite3.connect("database.db")
    schema = conn.execute(
        "SELECT sql FROM sqlite_master WHERE type='table'"
    ).fetchall()
    return "\n".join(sql[0] for sql in schema if sql[0])

@mcp.tool()
def query_data(sql: str) -> str:
    """Execute SQL queries safely"""
    conn = sqlite3.connect("database.db")
    try:
        result = conn.execute(sql).fetchall()
        return "\n".join(str(row) for row in result)
    except Exception as e:
        return f"Error: {str(e)}"
```

## Advanced Usage

### Low-Level Server

For more control, you can use the low-level server implementation directly. This gives you full access to the protocol and allows you to customize every aspect of your server, including lifecycle management through the lifespan API:

```python
from contextlib import asynccontextmanager
from typing import AsyncIterator

@asynccontextmanager
async def server_lifespan(server: Server) -> AsyncIterator[dict]:
    """Manage server startup and shutdown lifecycle."""
    try:
        # Initialize resources on startup
        await db.connect()
        yield {"db": db}
    finally:
        # Clean up on shutdown
        await db.disconnect()

# Pass lifespan to server
server = Server("example-server", lifespan=server_lifespan)

# Access lifespan context in handlers
@server.call_tool()
async def query_db(name: str, arguments: dict) -> list:
    ctx = server.request_context
    db = ctx.lifespan_context["db"]
    return await db.query(arguments["query"])
```

The lifespan API provides:
- A way to initialize resources when the server starts and clean them up when it stops
- Access to initialized resources through the request context in handlers
- Type-safe context passing between lifespan and request handlers

```python
from mcp.server.lowlevel import Server, NotificationOptions
from mcp.server.models import InitializationOptions
import mcp.server.stdio
import mcp.types as types

# Create a server instance
server = Server("example-server")

@server.list_prompts()
async def handle_list_prompts() -> list[types.Prompt]:
    return [
        types.Prompt(
            name="example-prompt",
            description="An example prompt template",
            arguments=[
                types.PromptArgument(
                    name="arg1",
                    description="Example argument",
                    required=True
                )
            ]
        )
    ]

@server.get_prompt()
async def handle_get_prompt(
    name: str,
    arguments: dict[str, str] | None
) -> types.GetPromptResult:
    if name != "example-prompt":
        raise ValueError(f"Unknown prompt: {name}")

    return types.GetPromptResult(
        description="Example prompt",
        messages=[
            types.PromptMessage(
                role="user",
                content=types.TextContent(
                    type="text",
                    text="Example prompt text"
                )
            )
        ]
    )

async def run():
    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            InitializationOptions(
                server_name="example",
                server_version="0.1.0",
                capabilities=server.get_capabilities(
                    notification_options=NotificationOptions(),
                    experimental_capabilities={},
                )
            )
        )

if __name__ == "__main__":
    import asyncio
    asyncio.run(run())
```

### Writing MCP Clients

The SDK provides a high-level client interface for connecting to MCP servers:

```python
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# Create server parameters for stdio connection
server_params = StdioServerParameters(
    command="python", # Executable
    args=["example_server.py"], # Optional command line arguments
    env=None # Optional environment variables
)

# Optional: create a sampling callback
async def handle_sampling_message(message: types.CreateMessageRequestParams) -> types.CreateMessageResult:
    return types.CreateMessageResult(
        role="assistant",
        content=types.TextContent(
            type="text",
            text="Hello, world! from model",
        ),
        model="gpt-3.5-turbo",
        stopReason="endTurn",
    )

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write, sampling_callback=handle_sampling_message) as session:
            # Initialize the connection
            await session.initialize()

            # List available prompts
            prompts = await session.list_prompts()

            # Get a prompt
            prompt = await session.get_prompt("example-prompt", arguments={"arg1": "value"})

            # List available resources
            resources = await session.list_resources()

            # List available tools
            tools = await session.list_tools()

            # Read a resource
            content, mime_type = await session.read_resource("file://some/path")

            # Call a tool
            result = await session.call_tool("tool-name", arguments={"arg1": "value"})

if __name__ == "__main__":
    import asyncio
    asyncio.run(run())
```

### MCP Primitives

The MCP protocol defines three core primitives that servers can implement:

| Primitive | Control               | Description                                         | Example Use                  |
|-----------|-----------------------|-----------------------------------------------------|------------------------------|
| Prompts   | User-controlled       | Interactive templates invoked by user choice        | Slash commands, menu options |
| Resources | Application-controlled| Contextual data managed by the client application   | File contents, API responses |
| Tools     | Model-controlled      | Functions exposed to the LLM to take actions        | API calls, data updates      |

### Server Capabilities

MCP servers declare capabilities during initialization:

| Capability  | Feature Flag                 | Description                        |
|-------------|------------------------------|------------------------------------|
| `prompts`   | `listChanged`                | Prompt template management         |
| `resources` | `subscribe`<br/>`listChanged`| Resource exposure and updates      |
| `tools`     | `listChanged`                | Tool discovery and execution       |
| `logging`   | -                            | Server logging configuration       |
| `completion`| -                            | Argument completion suggestions    |

## Documentation

- [Model Context Protocol documentation](https://modelcontextprotocol.io)
- [Model Context Protocol specification](https://spec.modelcontextprotocol.io)
- [Officially supported servers](https://github.com/modelcontextprotocol/servers)

## Contributing

We are passionate about supporting contributors of all levels of experience and would love to see you get involved in the project. See the [contributing guide](CONTRIBUTING.md) to get started.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: evai/docs/mcp_spec.md
================
# Example Clients
Source: https://modelcontextprotocol.io/clients

A list of applications that support MCP integrations

This page provides an overview of applications that support the Model Context Protocol (MCP). Each client may support different MCP features, allowing for varying levels of integration with MCP servers.

## Feature support matrix

| Client                               | [Resources] | [Prompts] | [Tools] | [Sampling] | Roots | Notes                                                              |
| ------------------------------------ | ----------- | --------- | ------- | ---------- | ----- | ------------------------------------------------------------------ |
| [Claude Desktop App][Claude]         |            |          |        |           |      | Full support for all MCP features                                  |
| [5ire][5ire]                         |            |          |        |           |      | Supports tools.                                                    |
| [BeeAI Framework][BeeAI Framework]   |            |          |        |           |      | Supports tools in agentic workflows.                               |
| [Cline][Cline]                       |            |          |        |           |      | Supports tools and resources.                                      |
| [Continue][Continue]                 |            |          |        |           |      | Full support for all MCP features                                  |
| [Cursor][Cursor]                     |            |          |        |           |      | Supports tools.                                                    |
| [Emacs Mcp][Mcp.el]                  |            |          |        |           |      | Supports tools in Emacs.                                           |
| [Firebase Genkit][Genkit]            |           |          |        |           |      | Supports resource list and lookup through tools.                   |
| [GenAIScript][GenAIScript]           |            |          |        |           |      | Supports tools.                                                    |
| [Goose][Goose]                       |            |          |        |           |      | Supports tools.                                                    |
| [LibreChat][LibreChat]               |            |          |        |           |      | Supports tools for Agents                                          |
| [mcp-agent][mcp-agent]               |            |          |        |          |      | Supports tools, server connection management, and agent workflows. |
| [Roo Code][Roo Code]                 |            |          |        |           |      | Supports tools and resources.                                      |
| [Sourcegraph Cody][Cody]             |            |          |        |           |      | Supports resources through OpenCTX                                 |
| [Superinterface][Superinterface]     |            |          |        |           |      | Supports tools                                                     |
| [TheiaAI/TheiaIDE][TheiaAI/TheiaIDE] |            |          |        |           |      | Supports tools for Agents in Theia AI and the AI-powered Theia IDE |
| [Windsurf Editor][Windsurf]          |            |          |        |           |      | Supports tools with AI Flow for collaborative development.         |
| [Zed][Zed]                           |            |          |        |           |      | Prompts appear as slash commands                                   |
| \[OpenSumi]\[OpenSumi]               |            |          |        |           |      | Supports tools in OpenSumi                                         |

[Claude]: https://claude.ai/download

[Cursor]: https://cursor.com

[Zed]: https://zed.dev

[Cody]: https://sourcegraph.com/cody

[Genkit]: https://github.com/firebase/genkit

[Continue]: https://github.com/continuedev/continue

[GenAIScript]: https://microsoft.github.io/genaiscript/reference/scripts/mcp-tools/

[Cline]: https://github.com/cline/cline

[LibreChat]: https://github.com/danny-avila/LibreChat

[TheiaAI/TheiaIDE]: https://eclipsesource.com/blogs/2024/12/19/theia-ide-and-theia-ai-support-mcp/

[Superinterface]: https://superinterface.ai

[5ire]: https://github.com/nanbingxyz/5ire

[BeeAI Framework]: https://i-am-bee.github.io/beeai-framework

[mcp-agent]: https://github.com/lastmile-ai/mcp-agent

[Mcp.el]: https://github.com/lizqwerscott/mcp.el

[Roo Code]: https://roocode.com

[Goose]: https://block.github.io/goose/docs/goose-architecture/#interoperability-with-extensions

[Windsurf]: https://codeium.com/windsurf

[Resources]: https://modelcontextprotocol.io/docs/concepts/resources

[Prompts]: https://modelcontextprotocol.io/docs/concepts/prompts

[Tools]: https://modelcontextprotocol.io/docs/concepts/tools

[Sampling]: https://modelcontextprotocol.io/docs/concepts/sampling

## Client details

### Claude Desktop App

The Claude desktop application provides comprehensive support for MCP, enabling deep integration with local tools and data sources.

**Key features:**

* Full support for resources, allowing attachment of local files and data
* Support for prompt templates
* Tool integration for executing commands and scripts
* Local server connections for enhanced privacy and security

>  Note: The Claude.ai web application does not currently support MCP. MCP features are only available in the desktop application.

### 5ire

[5ire](https://github.com/nanbingxyz/5ire) is an open source cross-platform desktop AI assistant that supports tools through MCP servers.

**Key features:**

* Built-in MCP servers can be quickly enabled and disabled.
* Users can add more servers by modifying the configuration file.
* It is open-source and user-friendly, suitable for beginners.
* Future support for MCP will be continuously improved.

### BeeAI Framework

[BeeAI Framework](https://i-am-bee.github.io/beeai-framework) is an open-source framework for building, deploying, and serving powerful agentic workflows at scale. The framework includes the **MCP Tool**, a native feature that simplifies the integration of MCP servers into agentic workflows.

**Key features:**

* Seamlessly incorporate MCP tools into agentic workflows.
* Quickly instantiate framework-native tools from connected MCP client(s).
* Planned future support for agentic MCP capabilities.

**Learn more:**

* [Example of using MCP tools in agentic workflow](https://i-am-bee.github.io/beeai-framework/#/typescript/tools?id=using-the-mcptool-class)

### Cline

[Cline](https://github.com/cline/cline) is an autonomous coding agent in VS Code that edits files, runs commands, uses a browser, and morewith your permission at each step.

**Key features:**

* Create and add tools through natural language (e.g. "add a tool that searches the web")
* Share custom MCP servers Cline creates with others via the `~/Documents/Cline/MCP` directory
* Displays configured MCP servers along with their tools, resources, and any error logs

### Continue

[Continue](https://github.com/continuedev/continue) is an open-source AI code assistant, with built-in support for all MCP features.

**Key features**

* Type "@" to mention MCP resources
* Prompt templates surface as slash commands
* Use both built-in and MCP tools directly in chat
* Supports VS Code and JetBrains IDEs, with any LLM

### Cursor

[Cursor](https://docs.cursor.com/advanced/model-context-protocol) is an AI code editor.

**Key Features**:

* Support for MCP tools in Cursor Composer
* Support for both STDIO and SSE

### Emacs Mcp

[Emacs Mcp](https://github.com/lizqwerscott/mcp.el) is an Emacs client designed to interface with MCP servers, enabling seamless connections and interactions. It provides MCP tool invocation support for AI plugins like [gptel](https://github.com/karthink/gptel) and [llm](https://github.com/ahyatt/llm), adhering to Emacs' standard tool invocation format. This integration enhances the functionality of AI tools within the Emacs ecosystem.

**Key features:**

* Provides MCP tool support for Emacs.

### Firebase Genkit

[Genkit](https://github.com/firebase/genkit) is Firebase's SDK for building and integrating GenAI features into applications. The [genkitx-mcp](https://github.com/firebase/genkit/tree/main/js/plugins/mcp) plugin enables consuming MCP servers as a client or creating MCP servers from Genkit tools and prompts.

**Key features:**

* Client support for tools and prompts (resources partially supported)
* Rich discovery with support in Genkit's Dev UI playground
* Seamless interoperability with Genkit's existing tools and prompts
* Works across a wide variety of GenAI models from top providers

### GenAIScript

Programmatically assemble prompts for LLMs using [GenAIScript](https://microsoft.github.io/genaiscript/) (in JavaScript). Orchestrate LLMs, tools, and data in JavaScript.

**Key features:**

* JavaScript toolbox to work with prompts
* Abstraction to make it easy and productive
* Seamless Visual Studio Code integration

### Goose

[Goose](https://github.com/block/goose) is an open source AI agent that supercharges your software development by automating coding tasks.

**Key features:**

* Expose MCP functionality to Goose through tools.
* MCPs can be installed directly via the [extensions directory](https://block.github.io/goose/v1/extensions/), CLI, or UI.
* Goose allows you to extend its functionality by [building your own MCP servers](https://block.github.io/goose/docs/tutorials/custom-extensions).
* Includes built-in tools for development, web scraping, automation, memory, and integrations with JetBrains and Google Drive.

### LibreChat

[LibreChat](https://github.com/danny-avila/LibreChat) is an open-source, customizable AI chat UI that supports multiple AI providers, now including MCP integration.

**Key features:**

* Extend current tool ecosystem, including [Code Interpreter](https://www.librechat.ai/docs/features/code_interpreter) and Image generation tools, through MCP servers
* Add tools to customizable [Agents](https://www.librechat.ai/docs/features/agents), using a variety of LLMs from top providers
* Open-source and self-hostable, with secure multi-user support
* Future roadmap includes expanded MCP feature support

### mcp-agent

[mcp-agent] is a simple, composable framework to build agents using Model Context Protocol.

**Key features:**

* Automatic connection management of MCP servers.
* Expose tools from multiple servers to an LLM.
* Implements every pattern defined in [Building Effective Agents](https://www.anthropic.com/research/building-effective-agents).
* Supports workflow pause/resume signals, such as waiting for human feedback.

### Roo Code

[Roo Code](https://roocode.com) enables AI coding assistance via MCP.

**Key features:**

* Support for MCP tools and resources
* Integration with development workflows
* Extensible AI capabilities

### Sourcegraph Cody

[Cody](https://openctx.org/docs/providers/modelcontextprotocol) is Sourcegraph's AI coding assistant, which implements MCP through OpenCTX.

**Key features:**

* Support for MCP resources
* Integration with Sourcegraph's code intelligence
* Uses OpenCTX as an abstraction layer
* Future support planned for additional MCP features

### Superinterface

[Superinterface](https://superinterface.ai) is AI infrastructure and a developer platform to build in-app AI assistants with support for MCP, interactive components, client-side function calling and more.

**Key features:**

* Use tools from MCP servers in assistants embedded via React components or script tags
* SSE transport support
* Use any AI model from any AI provider (OpenAI, Anthropic, Ollama, others)

### TheiaAI/TheiaIDE

[Theia AI](https://eclipsesource.com/blogs/2024/10/07/introducing-theia-ai/) is a framework for building AI-enhanced tools and IDEs. The [AI-powered Theia IDE](https://eclipsesource.com/blogs/2024/10/08/introducting-ai-theia-ide/) is an open and flexible development environment built on Theia AI.

**Key features:**

* **Tool Integration**: Theia AI enables AI agents, including those in the Theia IDE, to utilize MCP servers for seamless tool interaction.
* **Customizable Prompts**: The Theia IDE allows users to define and adapt prompts, dynamically integrating MCP servers for tailored workflows.
* **Custom agents**: The Theia IDE supports creating custom agents that leverage MCP capabilities, enabling users to design dedicated workflows on the fly.

Theia AI and Theia IDE's MCP integration provide users with flexibility, making them powerful platforms for exploring and adapting MCP.

**Learn more:**

* [Theia IDE and Theia AI MCP Announcement](https://eclipsesource.com/blogs/2024/12/19/theia-ide-and-theia-ai-support-mcp/)
* [Download the AI-powered Theia IDE](https://theia-ide.org/)

### Windsurf Editor

[Windsurf Editor](https://codeium.com/windsurf) is an agentic IDE that combines AI assistance with developer workflows. It features an innovative AI Flow system that enables both collaborative and independent AI interactions while maintaining developer control.

**Key features:**

* Revolutionary AI Flow paradigm for human-AI collaboration
* Intelligent code generation and understanding
* Rich development tools with multi-model support

### Zed

[Zed](https://zed.dev/docs/assistant/model-context-protocol) is a high-performance code editor with built-in MCP support, focusing on prompt templates and tool integration.

**Key features:**

* Prompt templates surface as slash commands in the editor
* Tool integration for enhanced coding workflows
* Tight integration with editor features and workspace context
* Does not support MCP resources

### OpenSumi

[OpenSumi](https://github.com/opensumi/core) is a framework helps you quickly build AI Native IDE products.

**Key features:**

* Supports MCP tools in OpenSumi
* Supports built-in IDE MCP servers and custom MCP servers

## Adding MCP support to your application

If you've added MCP support to your application, we encourage you to submit a pull request to add it to this list. MCP integration can provide your users with powerful contextual AI capabilities and make your application part of the growing MCP ecosystem.

Benefits of adding MCP support:

* Enable users to bring their own context and tools
* Join a growing ecosystem of interoperable AI applications
* Provide users with flexible integration options
* Support local-first AI workflows

To get started with implementing MCP in your application, check out our [Python](https://github.com/modelcontextprotocol/python-sdk) or [TypeScript SDK Documentation](https://github.com/modelcontextprotocol/typescript-sdk)

## Updates and corrections

This list is maintained by the community. If you notice any inaccuracies or would like to update information about MCP support in your application, please submit a pull request or [open an issue in our documentation repository](https://github.com/modelcontextprotocol/docs/issues).


# Contributing
Source: https://modelcontextprotocol.io/development/contributing

How to participate in Model Context Protocol development

We welcome contributions from the community! Please review our [contributing guidelines](https://github.com/modelcontextprotocol/.github/blob/main/CONTRIBUTING.md) for details on how to submit changes.

All contributors must adhere to our [Code of Conduct](https://github.com/modelcontextprotocol/.github/blob/main/CODE_OF_CONDUCT.md).

For questions and discussions, please use [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions).


# Roadmap
Source: https://modelcontextprotocol.io/development/roadmap

Our plans for evolving Model Context Protocol (H1 2025)

The Model Context Protocol is rapidly evolving. This page outlines our current thinking on key priorities and future direction for **the first half of 2025**, though these may change significantly as the project develops.

<Note>The ideas presented here are not commitmentswe may solve these challenges differently than described, or some may not materialize at all. This is also not an *exhaustive* list; we may incorporate work that isn't mentioned here.</Note>

We encourage community participation! Each section links to relevant discussions where you can learn more and contribute your thoughts.

## Remote MCP Support

Our top priority is enabling [remote MCP connections](https://github.com/modelcontextprotocol/specification/discussions/102), allowing clients to securely connect to MCP servers over the internet. Key initiatives include:

*   [**Authentication & Authorization**](https://github.com/modelcontextprotocol/specification/discussions/64): Adding standardized auth capabilities, particularly focused on OAuth 2.0 support.

*   [**Service Discovery**](https://github.com/modelcontextprotocol/specification/discussions/69): Defining how clients can discover and connect to remote MCP servers.

*   [**Stateless Operations**](https://github.com/modelcontextprotocol/specification/discussions/102): Thinking about whether MCP could encompass serverless environments too, where they will need to be mostly stateless.

## Reference Implementations

To help developers build with MCP, we want to offer documentation for:

*   **Client Examples**: Comprehensive reference client implementation(s), demonstrating all protocol features
*   **Protocol Drafting**: Streamlined process for proposing and incorporating new protocol features

## Distribution & Discovery

Looking ahead, we're exploring ways to make MCP servers more accessible. Some areas we may investigate include:

*   **Package Management**: Standardized packaging format for MCP servers
*   **Installation Tools**: Simplified server installation across MCP clients
*   **Sandboxing**: Improved security through server isolation
*   **Server Registry**: A common directory for discovering available MCP servers

## Agent Support

We're expanding MCP's capabilities for [complex agentic workflows](https://github.com/modelcontextprotocol/specification/discussions/111), particularly focusing on:

*   [**Hierarchical Agent Systems**](https://github.com/modelcontextprotocol/specification/discussions/94): Improved support for trees of agents through namespacing and topology awareness.

*   [**Interactive Workflows**](https://github.com/modelcontextprotocol/specification/issues/97): Better handling of user permissions and information requests across agent hierarchies, and ways to send output to users instead of models.

*   [**Streaming Results**](https://github.com/modelcontextprotocol/specification/issues/117): Real-time updates from long-running agent operations.

## Broader Ecosystem

We're also invested in:

*   **Community-Led Standards Development**: Fostering a collaborative ecosystem where all AI providers can help shape MCP as an open standard through equal participation and shared governance, ensuring it meets the needs of diverse AI applications and use cases.
*   [**Additional Modalities**](https://github.com/modelcontextprotocol/specification/discussions/88): Expanding beyond text to support audio, video, and other formats.
*   \[**Standardization**] Considering standardization through a standardization body.

## Get Involved

We welcome community participation in shaping MCP's future. Visit our [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions) to join the conversation and contribute your ideas.


# What's New
Source: https://modelcontextprotocol.io/development/updates

The latest updates and improvements to MCP

<Update label="2025-02-14" description="Java SDK released">
  * We're excited to announce that the Java SDK developed by Spring AI at VMware Tanzu is now
    the official [Java SDK](https://github.com/modelcontextprotocol/java-sdk) for MCP.
    This joins our existing Kotlin SDK in our growing list of supported languages.
    The Spring AI team will maintain the SDK as an integral part of the Model Context Protocol
    organization. We're thrilled to welcome them to the MCP community!
</Update>

<Update label="2025-01-27" description="Python SDK 1.2.1">
  * Version [1.2.1](https://github.com/modelcontextprotocol/python-sdk/releases/tag/v1.2.1) of the MCP Python SDK has been released,
    delivering important stability improvements and bug fixes.
</Update>

<Update label="2025-01-18" description="SDK and Server Improvements">
  * Simplified, express-like API in the [TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)
  * Added 8 new clients to the [clients page](https://modelcontextprotocol.io/clients)
</Update>

<Update label="2025-01-03" description="SDK and Server Improvements">
  * FastMCP API in the [Python SDK](https://github.com/modelcontextprotocol/python-sdk)
  * Dockerized MCP servers in the [servers repo](https://github.com/modelcontextprotocol/servers)
</Update>

<Update label="2024-12-21" description="Kotlin SDK released">
  * Jetbrains released a Kotlin SDK for MCP!
  * For a sample MCP Kotlin server, check out [this repository](https://github.com/modelcontextprotocol/kotlin-sdk/tree/main/samples/kotlin-mcp-server)
</Update>


# Core architecture
Source: https://modelcontextprotocol.io/docs/concepts/architecture

Understand how MCP connects clients, servers, and LLMs

The Model Context Protocol (MCP) is built on a flexible, extensible architecture that enables seamless communication between LLM applications and integrations. This document covers the core architectural components and concepts.

## Overview

MCP follows a client-server architecture where:

*   **Hosts** are LLM applications (like Claude Desktop or IDEs) that initiate connections
*   **Clients** maintain 1:1 connections with servers, inside the host application
*   **Servers** provide context, tools, and prompts to clients

```mermaid
flowchart LR
    subgraph "&nbsp;Host (e.g., Claude Desktop)&nbsp;"
        client1[MCP Client]
        client2[MCP Client]
    end
    subgraph "Server Process"
        server1[MCP Server]
    end
    subgraph "Server Process"
        server2[MCP Server]
    end

    client1 <-->|Transport Layer| server1
    client2 <-->|Transport Layer| server2
```

## Core components

### Protocol layer

The protocol layer handles message framing, request/response linking, and high-level communication patterns.

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    class Protocol<Request, Notification, Result> {
        // Handle incoming requests
        setRequestHandler<T>(schema: T, handler: (request: T, extra: RequestHandlerExtra) => Promise<Result>): void

        // Handle incoming notifications
        setNotificationHandler<T>(schema: T, handler: (notification: T) => Promise<void>): void

        // Send requests and await responses
        request<T>(request: Request, schema: T, options?: RequestOptions): Promise<T>

        // Send one-way notifications
        notification(notification: Notification): Promise<void>
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
    class Session(BaseSession[RequestT, NotificationT, ResultT]):
        async def send_request(
            self,
            request: RequestT,
            result_type: type[Result]
        ) -> Result:
            """
            Send request and wait for response. Raises McpError if response contains error.
            """
            # Request handling implementation

        async def send_notification(
            self,
            notification: NotificationT
        ) -> None:
            """Send one-way notification that doesn't expect response."""
            # Notification handling implementation

        async def _received_request(
            self,
            responder: RequestResponder[ReceiveRequestT, ResultT]
        ) -> None:
            """Handle incoming request from other side."""
            # Request handling implementation

        async def _received_notification(
            self,
            notification: ReceiveNotificationT
        ) -> None:
            """Handle incoming notification from other side."""
            # Notification handling implementation
    ```
  </Tab>
</Tabs>

Key classes include:

*   `Protocol`
*   `Client`
*   `Server`

### Transport layer

The transport layer handles the actual communication between clients and servers. MCP supports multiple transport mechanisms:

1.  **Stdio transport**
    *   Uses standard input/output for communication
    *   Ideal for local processes

2.  **HTTP with SSE transport**
    *   Uses Server-Sent Events for server-to-client messages
    *   HTTP POST for client-to-server messages

All transports use [JSON-RPC](https://www.jsonrpc.org/) 2.0 to exchange messages. See the [specification](https://spec.modelcontextprotocol.io) for detailed information about the Model Context Protocol message format.

### Message types

MCP has these main types of messages:

1.  **Requests** expect a response from the other side:
    ```typescript
    interface Request {
      method: string;
      params?: { ... };
    }
    ```

2.  **Results** are successful responses to requests:
    ```typescript
    interface Result {
      [key: string]: unknown;
    }
    ```

3.  **Errors** indicate that a request failed:
    ```typescript
    interface Error {
      code: number;
      message: string;
      data?: unknown;
    }
    ```

4.  **Notifications** are one-way messages that don't expect a response:
    ```typescript
    interface Notification {
      method: string;
      params?: { ... };
    }
    ```

## Connection lifecycle

### 1. Initialization

```mermaid
sequenceDiagram
    participant Client
    participant Server

    Client->>Server: initialize request
    Server->>Client: initialize response
    Client->>Server: initialized notification

    Note over Client,Server: Connection ready for use
```

1.  Client sends `initialize` request with protocol version and capabilities
2.  Server responds with its protocol version and capabilities
3.  Client sends `initialized` notification as acknowledgment
4.  Normal message exchange begins

### 2. Message exchange

After initialization, the following patterns are supported:

*   **Request-Response**: Client or server sends requests, the other responds
*   **Notifications**: Either party sends one-way messages

### 3. Termination

Either party can terminate the connection:

*   Clean shutdown via `close()`
*   Transport disconnection
*   Error conditions

## Error handling

MCP defines these standard error codes:

```typescript
enum ErrorCode {
  // Standard JSON-RPC error codes
  ParseError = -32700,
  InvalidRequest = -32600,
  MethodNotFound = -32601,
  InvalidParams = -32602,
  InternalError = -32603
}
```

SDKs and applications can define their own error codes above -32000.

Errors are propagated through:

*   Error responses to requests
*   Error events on transports
*   Protocol-level error handlers

## Implementation example

Here's a basic example of implementing an MCP server:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import { Server } from "@modelcontextprotocol/sdk/server/index.js";
    import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";

    const server = new Server({
      name: "example-server",
      version: "1.0.0"
    }, {
      capabilities: {
        resources: {}
      }
    });

    // Handle requests
    server.setRequestHandler(ListResourcesRequestSchema, async () => {
      return {
        resources: [
          {
            uri: "example://resource",
            name: "Example Resource"
          }
        ]
      };
    });

    // Connect transport
    const transport = new StdioServerTransport();
    await server.connect(transport);
    ```
  </Tab>

  <Tab title="Python">
    ```python
    import asyncio
    import mcp.types as types
    from mcp.server import Server
    from mcp.server.stdio import stdio_server

    app = Server("example-server")

    @app.list_resources()
    async def list_resources() -> list[types.Resource]:
        return [
            types.Resource(
                uri="example://resource",
                name="Example Resource"
            )
        ]

    async def main():
        async with stdio_server() as streams:
            await app.run(
                streams[0],
                streams[1],
                app.create_initialization_options()
            )

    if __name__ == "__main__":
        asyncio.run(main)
    ```
  </Tab>
</Tabs>

## Best practices

### Transport selection

1.  **Local communication**
    *   Use stdio transport for local processes
    *   Efficient for same-machine communication
    *   Simple process management

2.  **Remote communication**
    *   Use SSE for scenarios requiring HTTP compatibility
    *   Consider security implications including authentication and authorization

### Message handling

1.  **Request processing**
    *   Validate inputs thoroughly
    *   Use type-safe schemas
    *   Handle errors gracefully
    *   Implement timeouts

2.  **Progress reporting**
    *   Use progress tokens for long operations
    *   Report progress incrementally
    *   Include total progress when known

3.  **Error management**
    *   Use appropriate error codes
    *   Include helpful error messages
    *   Clean up resources on errors

## Security considerations

1.  **Transport security**
    *   Use TLS for remote connections
    *   Validate connection origins
    *   Implement authentication when needed

2.  **Message validation**
    *   Validate all incoming messages
    *   Sanitize inputs
    *   Check message size limits
    *   Verify JSON-RPC format

3.  **Resource protection**
    *   Implement access controls
    *   Validate resource paths
    *   Monitor resource usage
    *   Rate limit requests

4.  **Error handling**
    *   Don't leak sensitive information
    *   Log security-relevant errors
    *   Implement proper cleanup
    *   Handle DoS scenarios

## Debugging and monitoring

1.  **Logging**
    *   Log protocol events
    *   Track message flow
    *   Monitor performance
    *   Record errors

2.  **Diagnostics**
    *   Implement health checks
    *   Monitor connection state
    *   Track resource usage
    *   Profile performance

3.  **Testing**
    *   Test different transports
    *   Verify error handling
    *   Check edge cases
    *   Load test servers


# Prompts
Source: https://modelcontextprotocol.io/docs/concepts/prompts

Create reusable prompt templates and workflows

Prompts enable servers to define reusable prompt templates and workflows that clients can easily surface to users and LLMs. They provide a powerful way to standardize and share common LLM interactions.

<Note>
  Prompts are designed to be **user-controlled**, meaning they are exposed from servers to clients with the intention of the user being able to explicitly select them for use.
</Note>

## Overview

Prompts in MCP are predefined templates that can:

*   Accept dynamic arguments
*   Include context from resources
*   Chain multiple interactions
*   Guide specific workflows
*   Surface as UI elements (like slash commands)

## Prompt structure

Each prompt is defined with:

```typescript
{
  name: string;              // Unique identifier for the prompt
  description?: string;      // Human-readable description
  arguments?: [              // Optional list of arguments
    {
      name: string;          // Argument identifier
      description?: string;  // Argument description
      required?: boolean;    // Whether argument is required
    }
  ]
}
```

## Discovering prompts

Clients can discover available prompts through the `prompts/list` endpoint:

```typescript
// Request
{
  method: "prompts/list"
}

// Response
{
  prompts: [
    {
      name: "analyze-code",
      description: "Analyze code for potential improvements",
      arguments: [
        {
          name: "language",
          description: "Programming language",
          required: true
        }
      ]
    }
  ]
}
```

## Using prompts

To use a prompt, clients make a `prompts/get` request:

````typescript
// Request
{
  method: "prompts/get",
  params: {
    name: "analyze-code",
    arguments: {
      language: "python"
    }
  }
}

// Response
{
  description: "Analyze Python code for potential improvements",
  messages: [
    {
      role: "user",
      content: {
        type: "text",
        text: "Please analyze the following Python code for potential improvements:\n\n```python\ndef calculate_sum(numbers):\n    total = 0\n    for num in numbers:\n        total = total + num\n    return total\n\nresult = calculate_sum([1, 2, 3, 4, 5])\nprint(result)\n```"
      }
    }
  ]
}
````

## Dynamic prompts

Prompts can be dynamic and include:

### Embedded resource context

```json
{
  "name": "analyze-project",
  "description": "Analyze project logs and code",
  "arguments": [
    {
      "name": "timeframe",
      "description": "Time period to analyze logs",
      "required": true
    },
    {
      "name": "fileUri",
      "description": "URI of code file to review",
      "required": true
    }
  ]
}
```

When handling the `prompts/get` request:

```json
{
  "messages": [
    {
      "role": "user",
      "content": {
        "type": "text",
        "text": "Analyze these system logs and the code file for any issues:"
      }
    },
    {
      "role": "user",
      "content": {
        "type": "resource",
        "resource": {
          "uri": "logs://recent?timeframe=1h",
          "text": "[2024-03-14 15:32:11] ERROR: Connection timeout in network.py:127\n[2024-03-14 15:32:15] WARN: Retrying connection (attempt 2/3)\n[2024-03-14 15:32:20] ERROR: Max retries exceeded",
          "mimeType": "text/plain"
        }
      }
    },
    {
      "role": "user",
      "content": {
        "type": "resource",
        "resource": {
          "uri": "file:///path/to/code.py",
          "text": "def connect_to_service(timeout=30):\n    retries = 3\n    for attempt in range(retries):\n        try:\n            return establish_connection(timeout)\n        except TimeoutError:\n            if attempt == retries - 1:\n                raise\n            time.sleep(5)\n\ndef establish_connection(timeout):\n    # Connection implementation\n    pass",
          "mimeType": "text/x-python"
        }
      }
    }
  ]
}
```

### Multi-step workflows

```typescript
const debugWorkflow = {
  name: "debug-error",
  async getMessages(error: string) {
    return [
      {
        role: "user",
        content: {
          type: "text",
          text: `Here's an error I'm seeing: ${error}`
        }
      },
      {
        role: "assistant",
        content: {
          type: "text",
          text: "I'll help analyze this error. What have you tried so far?"
        }
      },
      {
        role: "user",
        content: {
          type: "text",
          text: "I've tried restarting the service, but the error persists."
        }
      }
    ];
  }
};
```

## Example implementation

Here's a complete example of implementing prompts in an MCP server:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    import { Server } from "@modelcontextprotocol/sdk/server";
    import {
      ListPromptsRequestSchema,
      GetPromptRequestSchema
    } from "@modelcontextprotocol/sdk/types";

    const PROMPTS = {
      "git-commit": {
        name: "git-commit",
        description: "Generate a Git commit message",
        arguments: [
          {
            name: "changes",
            description: "Git diff or description of changes",
            required: true
          }
        ]
      },
      "explain-code": {
        name: "explain-code",
        description: "Explain how code works",
        arguments: [
          {
            name: "code",
            description: "Code to explain",
            required: true
          },
          {
            name: "language",
            description: "Programming language",
            required: false
          }
        ]
      }
    };

    const server = new Server({
      name: "example-prompts-server",
      version: "1.0.0"
    }, {
      capabilities: {
        prompts: {}
      }
    });

    // List available prompts
    server.setRequestHandler(ListPromptsRequestSchema, async () => {
      return {
        prompts: Object.values(PROMPTS)
      };
    });

    // Get specific prompt
    server.setRequestHandler(GetPromptRequestSchema, async (request) => {
      const prompt = PROMPTS[request.params.name];
      if (!prompt) {
        throw new Error(`Prompt not found: ${request.params.name}`);
      }

      if (request.params.name === "git-commit") {
        return {
          messages: [
            {
              role: "user",
              content: {
                type: "text",
                text: `Generate a concise but descriptive commit message for these changes:\n\n${request.params.arguments?.changes}`
              }
            }
          ]
        };
      }

      if (request.params.name === "explain-code") {
        const language = request.params.arguments?.language || "Unknown";
        return {
          messages: [
            {
              role: "user",
              content: {
                type: "text",
                text: `Explain how this ${language} code works:\n\n${request.params.arguments?.code}`
              }
            }
          ]
        };
      }

      throw new Error("Prompt implementation not found");
    });
    ```
  </Tab>

  <Tab title="Python">
    ```python
    from mcp.server import Server
    import mcp.types as types

    # Define available prompts
    PROMPTS = {
        "git-commit": types.Prompt(
            name="git-commit",
            description="Generate a Git commit message",
            arguments=[
                types.PromptArgument(
                    name="changes",
                    description="Git diff or description of changes",
                    required=True
                )
            ],
        ),
        "explain-code": types.Prompt(
            name="explain-code",
            description="Explain how code works",
            arguments=[
                types.PromptArgument(
                    name="code",
                    description="Code to explain",
                    required=True
                ),
                types.PromptArgument(
                    name="language",
                    description="Programming language",
                    required=False
                )
            ],
        )
    }

    # Initialize server
    app = Server("example-prompts-server")

    @app.list_prompts()
    async def list_prompts() -> list[types.Prompt]:
        return list(PROMPTS.values())

    @app.get_prompt()
    async def get_prompt(
        name: str, arguments: dict[str, str] | None = None
    ) -> types.GetPromptResult:
        if name not in PROMPTS:
            raise ValueError(f"Prompt not found: {name}")

        if name == "git-commit":
            changes = arguments.get("changes") if arguments else ""
            return types.GetPromptResult(
                messages=[
                    types.PromptMessage(
                        role="user",
                        content=types.TextContent(
                            type="text",
                            text=f"Generate a concise but descriptive commit message "
                            f"for these changes:\n\n{changes}"
                        )
                    )
                ]
            )

        if name == "explain-code":
            code = arguments.get("code") if arguments else ""
            language = arguments.get("language", "Unknown") if arguments else "Unknown"
            return types.GetPromptResult(
                messages=[
                    types.PromptMessage(
                        role="user",
                        content=types.TextContent(
                            type="text",
                            text=f"Explain how this {language} code works:\n\n{code}"
                        )
                    )
                ]
            )

        raise ValueError("Prompt implementation not found")
    ```
  </Tab>
</Tabs>

## Best practices

When implementing prompts:

1.  Use clear, descriptive prompt names
2.  Provide detailed descriptions for prompts and arguments
3.  Validate all required arguments
4.  Handle missing arguments gracefully
5.  Consider versioning for prompt templates
6.  Cache dynamic content when appropriate
7.  Implement error handling
8.  Document expected argument formats
9.  Consider prompt composability
10. Test prompts with various inputs

## UI integration

Prompts can be surfaced in client UIs as:

*   Slash commands
*   Quick actions
*   Context menu items
*   Command palette entries
*   Guided workflows
*   Interactive forms

## Updates and changes

Servers can notify clients about prompt changes:

1.  Server capability: `prompts.listChanged`
2.  Notification: `notifications/prompts/list_changed`
3.  Client re-fetches prompt list

## Security considerations

When implementing prompts:

*   Validate all arguments
*   Sanitize user input
*   Consider rate limiting
*   Implement access controls
*   Audit prompt usage
*   Handle sensitive data appropriately
*   Validate generated content
*   Implement timeouts
*   Consider prompt injection risks
*   Document security requirements


# Resources
Source: https://modelcontextprotocol.io/docs/concepts/resources

Expose data and content from your servers to LLMs

Resources are a core primitive in the Model Context Protocol (MCP) that allow servers to expose data and content that can be read by clients and used as context for LLM interactions.

<Note>
  Resources are designed to be **application-controlled**, meaning that the client application can decide how and when they should be used.
  Different MCP clients may handle resources differently. For example:

  *   Claude Desktop currently requires users to explicitly select resources before they can be used
  *   Other clients might automatically select resources based on heuristics
  *   Some implementations may even allow the AI model itself to determine which resources to use

  Server authors should be prepared to handle any of these interaction patterns when implementing resource support. In order to expose data to models automatically, server authors should use a **model-controlled** primitive such as [Tools](./tools).
</Note>

## Overview

Resources represent any kind of data that an MCP server wants to make available to clients. This can include:

*   File contents
*   Database records
*   API responses
*   Live system data
*   Screenshots and images
*   Log files
*   And more

Each resource is identified by a unique URI and can contain either text or binary data.

## Resource URIs

Resources are identified using URIs that follow this format:

```
[protocol]://[host]/[path]
```

For example:

*   `file:///home/user/documents/report.pdf`
*   `postgres://database/customers/schema`
*   `screen://localhost/display1`

The protocol and path structure is defined by the MCP server implementation. Servers can define their own custom URI schemes.

## Resource types

Resources can contain two types of content:

### Text resources

Text resources contain UTF-8 encoded text data. These are suitable for:

*   Source code
*   Configuration files
*   Log files
*   JSON/XML data
*   Plain text

### Binary resources

Binary resources contain raw binary data encoded in base64. These are suitable for:

*   Images
*   PDFs
*   Audio files
*   Video files
*   Other non-text formats

## Resource discovery

Clients can discover available resources through two main methods:

### Direct resources

Servers expose a list of concrete resources via the `resources/list` endpoint. Each resource includes:

```typescript
{
  uri: string;           // Unique identifier for the resource
  name: string;          // Human-readable name
  description?: string;  // Optional description
  mimeType?: string;     // Optional MIME type
}
```

### Resource templates

For dynamic resources, servers can expose [URI templates](https://datatracker.ietf.org/doc/html/rfc6570) that clients can use to construct valid resource URIs:

```typescript
{
  uriTemplate: string;   // URI template following RFC 6570
  name: string;          // Human-readable name for this type
  description?: string;  // Optional description
  mimeType?: string;     // Optional MIME type for all matching resources
}
```

## Reading resources

To read a resource, clients make a `resources/read` request with the resource URI.

The server responds with a list of resource contents:

```typescript
{
  contents: [
    {
      uri: string;        // The URI of the resource
      mimeType?: string;  // Optional MIME type

      // One of:
      text?: string;      // For text resources
      blob?: string;      // For binary resources (base64 encoded)
    }
  ]
}
```

<Tip>
  Servers may return multiple resources in response to one `resources/read` request. This could be used, for example, to return a list of files inside a directory when the directory is read.
</Tip>

## Resource updates

MCP supports real-time updates for resources through two mechanisms:

### List changes

Servers can notify clients when their list of available resources changes via the `notifications/resources/list_changed` notification.

### Content changes

Clients can subscribe to updates for specific resources:

1.  Client sends `resources/subscribe` with resource URI
2.  Server sends `notifications/resources/updated` when the resource changes
3.  Client can fetch latest content with `resources/read`
4.  Client can unsubscribe with `resources/unsubscribe`

## Example implementation

Here's a simple example of implementing resource support in an MCP server:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    const server = new Server({
      name: "example-server",
      version: "1.0.0"
    }, {
      capabilities: {
        resources: {}
      }
    });

    // List available resources
    server.setRequestHandler(ListResourcesRequestSchema, async () => {
      return {
        resources: [
          {
            uri: "file:///logs/app.log",
            name: "Application Logs",
            mimeType: "text/plain"
          }
        ]
      };
    });

    // Read resource contents
    server.setRequestHandler(ReadResourceRequestSchema, async (request) => {
      const uri = request.params.uri;

      if (uri === "file:///logs/app.log") {
        const logContents = await readLogFile();
        return {
          contents: [
            {
              uri,
              mimeType: "text/plain",
              text: logContents
            }
          ]
        };
      }

      throw new Error("Resource not found");
    });
    ```
  </Tab>

  <Tab title="Python">
    ```python
    app = Server("example-server")

    @app.list_resources()
    async def list_resources() -> list[types.Resource]:
        return [
            types.Resource(
                uri="file:///logs/app.log",
                name="Application Logs",
                mimeType="text/plain"
            )
        ]

    @app.read_resource()
    async def read_resource(uri: AnyUrl) -> str:
        if str(uri) == "file:///logs/app.log":
            log_contents = await read_log_file()
            return log_contents

        raise ValueError("Resource not found")

    # Start server
    async with stdio_server() as streams:
        await app.run(
            streams[0],
            streams[1],
            app.create_initialization_options()
        )
    ```
  </Tab>
</Tabs>

## Best practices

When implementing resource support:

1.  Use clear, descriptive resource names and URIs
2.  Include helpful descriptions to guide LLM understanding
3.  Set appropriate MIME types when known
4.  Implement resource templates for dynamic content
5.  Use subscriptions for frequently changing resources
6.  Handle errors gracefully with clear error messages
7.  Consider pagination for large resource lists
8.  Cache resource contents when appropriate
9.  Validate URIs before processing
10. Document your custom URI schemes

## Security considerations

When exposing resources:

*   Validate all resource URIs
*   Implement appropriate access controls
*   Sanitize file paths to prevent directory traversal
*   Be cautious with binary data handling
*   Consider rate limiting for resource reads
*   Audit resource access
*   Encrypt sensitive data in transit
*   Validate MIME types
*   Implement timeouts for long-running reads
*   Handle resource cleanup appropriately


# Roots
Source: https://modelcontextprotocol.io/docs/concepts/roots

Understanding roots in MCP

Roots are a concept in MCP that define the boundaries where servers can operate. They provide a way for clients to inform servers about relevant resources and their locations.

## What are Roots?

A root is a URI that a client suggests a server should focus on. When a client connects to a server, it declares which roots the server should work with. While primarily used for filesystem paths, roots can be any valid URI including HTTP URLs.

For example, roots could be:

```
file:///home/user/projects/myapp
https://api.example.com/v1
```

## Why Use Roots?

Roots serve several important purposes:

1.  **Guidance**: They inform servers about relevant resources and locations
2.  **Clarity**: Roots make it clear which resources are part of your workspace
3.  **Organization**: Multiple roots let you work with different resources simultaneously

## How Roots Work

When a client supports roots, it:

1.  Declares the `roots` capability during connection
2.  Provides a list of suggested roots to the server
3.  Notifies the server when roots change (if supported)

While roots are informational and not strictly enforcing, servers should:

1.  Respect the provided roots
2.  Use root URIs to locate and access resources
3.  Prioritize operations within root boundaries

## Common Use Cases

Roots are commonly used to define:

*   Project directories
*   Repository locations
*   API endpoints
*   Configuration locations
*   Resource boundaries

## Best Practices

When working with roots:

1.  Only suggest necessary resources
2.  Use clear, descriptive names for roots
3.  Monitor root accessibility
4.  Handle root changes gracefully

## Example

Here's how a typical MCP client might expose roots:

```json
{
  "roots": [
    {
      "uri": "file:///home/user/projects/frontend",
      "name": "Frontend Repository"
    },
    {
      "uri": "https://api.example.com/v1",
      "name": "API Endpoint"
    }
  ]
}
```

This configuration suggests the server focus on both a local repository and an API endpoint while keeping them logically separated.


# Sampling
Source: https://modelcontextprotocol.io/docs/concepts/sampling

Let your servers request completions from LLMs

Sampling is a powerful MCP feature that allows servers to request LLM completions through the client, enabling sophisticated agentic behaviors while maintaining security and privacy.

<Info>
  This feature of MCP is not yet supported in the Claude Desktop client.
</Info>

## How sampling works

The sampling flow follows these steps:

1.  Server sends a `sampling/createMessage` request to the client
2.  Client reviews the request and can modify it
3.  Client samples from an LLM
4.  Client reviews the completion
5.  Client returns the result to the server

This human-in-the-loop design ensures users maintain control over what the LLM sees and generates.

## Message format

Sampling requests use a standardized message format:

```typescript
{
  messages: [
    {
      role: "user" | "assistant",
      content: {
        type: "text" | "image",

        // For text:
        text?: string,

        // For images:
        data?: string,             // base64 encoded
        mimeType?: string
      }
    }
  ],
  modelPreferences?: {
    hints?: [{
      name?: string                // Suggested model name/family
    }],
    costPriority?: number,         // 0-1, importance of minimizing cost
    speedPriority?: number,        // 0-1, importance of low latency
    intelligencePriority?: number  // 0-1, importance of capabilities
  },
  systemPrompt?: string,
  includeContext?: "none" | "thisServer" | "allServers",
  temperature?: number,
  maxTokens: number,
  stopSequences?: string[],
  metadata?: Record<string, unknown>
}
```

## Request parameters

### Messages

The `messages` array contains the conversation history to send to the LLM. Each message has:

*   `role`: Either "user" or "assistant"
*   `content`: The message content, which can be:
    *   Text content with a `text` field
    *   Image content with `data` (base64) and `mimeType` fields

### Model preferences

The `modelPreferences` object allows servers to specify their model selection preferences:

*   `hints`: Array of model name suggestions that clients can use to select an appropriate model:
    *   `name`: String that can match full or partial model names (e.g. "claude-3", "sonnet")
    *   Clients may map hints to equivalent models from different providers
    *   Multiple hints are evaluated in preference order

*   Priority values (0-1 normalized):
    *   `costPriority`: Importance of minimizing costs
    *   `speedPriority`: Importance of low latency response
    *   `intelligencePriority`: Importance of advanced model capabilities

Clients make the final model selection based on these preferences and their available models.

### System prompt

An optional `systemPrompt` field allows servers to request a specific system prompt. The client may modify or ignore this.

### Context inclusion

The `includeContext` parameter specifies what MCP context to include:

*   `"none"`: No additional context
*   `"thisServer"`: Include context from the requesting server
*   `"allServers"`: Include context from all connected MCP servers

The client controls what context is actually included.

### Sampling parameters

Fine-tune the LLM sampling with:

*   `temperature`: Controls randomness (0.0 to 1.0)
*   `maxTokens`: Maximum tokens to generate
*   `stopSequences`: Array of sequences that stop generation
*   `metadata`: Additional provider-specific parameters

## Response format

The client returns a completion result:

```typescript
{
  model: string,  // Name of the model used
  stopReason?: "endTurn" | "stopSequence" | "maxTokens" | string,
  role: "user" | "assistant",
  content: {
    type: "text" | "image",
    text?: string,
    data?: string,
    mimeType?: string
  }
}
```

## Example request

Here's an example of requesting sampling from a client:

```json
{
  "method": "sampling/createMessage",
  "params": {
    "messages": [
      {
        "role": "user",
        "content": {
          "type": "text",
          "text": "What files are in the current directory?"
        }
      }
    ],
    "systemPrompt": "You are a helpful file system assistant.",
    "includeContext": "thisServer",
    "maxTokens": 100
  }
}
```

## Best practices

When implementing sampling:

1.  Always provide clear, well-structured prompts
2.  Handle both text and image content appropriately
3.  Set reasonable token limits
4.  Include relevant context through `includeContext`
5.  Validate responses before using them
6.  Handle errors gracefully
7.  Consider rate limiting sampling requests
8.  Document expected sampling behavior
9.  Test with various model parameters
10. Monitor sampling costs

## Human in the loop controls

Sampling is designed with human oversight in mind:

### For prompts

*   Clients should show users the proposed prompt
*   Users should be able to modify or reject prompts
*   System prompts can be filtered or modified
*   Context inclusion is controlled by the client

### For completions

*   Clients should show users the completion
*   Users should be able to modify or reject completions
*   Clients can filter or modify completions
*   Users control which model is used

## Security considerations

When implementing sampling:

*   Validate all message content
*   Sanitize sensitive information
*   Implement appropriate rate limits
*   Monitor sampling usage
*   Encrypt data in transit
*   Handle user data privacy
*   Audit sampling requests
*   Control cost exposure
*   Implement timeouts
*   Handle model errors gracefully

## Common patterns

### Agentic workflows

Sampling enables agentic patterns like:

*   Reading and analyzing resources
*   Making decisions based on context
*   Generating structured data
*   Handling multi-step tasks
*   Providing interactive assistance

### Context management

Best practices for context:

*   Request minimal necessary context
*   Structure context clearly
*   Handle context size limits
*   Update context as needed
*   Clean up stale context

### Error handling

Robust error handling should:

*   Catch sampling failures
*   Handle timeout errors
*   Manage rate limits
*   Validate responses
*   Provide fallback behaviors
*   Log errors appropriately

## Limitations

Be aware of these limitations:

*   Sampling depends on client capabilities
*   Users control sampling behavior
*   Context size has limits
*   Rate limits may apply
*   Costs should be considered
*   Model availability varies
*   Response times vary
*   Not all content types supported


# Tools
Source: https://modelcontextprotocol.io/docs/concepts/tools

Enable LLMs to perform actions through your server

Tools are a powerful primitive in the Model Context Protocol (MCP) that enable servers to expose executable functionality to clients. Through tools, LLMs can interact with external systems, perform computations, and take actions in the real world.

<Note>
  Tools are designed to be **model-controlled**, meaning that tools are exposed from servers to clients with the intention of the AI model being able to automatically invoke them (with a human in the loop to grant approval).
</Note>

## Overview

Tools in MCP allow servers to expose executable functions that can be invoked by clients and used by LLMs to perform actions. Key aspects of tools include:

*   **Discovery**: Clients can list available tools through the `tools/list` endpoint
*   **Invocation**: Tools are called using the `tools/call` endpoint, where servers perform the requested operation and return results
*   **Flexibility**: Tools can range from simple calculations to complex API interactions

Like [resources](/docs/concepts/resources), tools are identified by unique names and can include descriptions to guide their usage. However, unlike resources, tools represent dynamic operations that can modify state or interact with external systems.

## Tool definition structure

Each tool is defined with the following structure:

```typescript
{
  name: string;          // Unique identifier for the tool
  description?: string;  // Human-readable description
  inputSchema: {         // JSON Schema for the tool's parameters
    type: "object",
    properties: { ... }  // Tool-specific parameters
  }
}
```

## Implementing tools

Here's an example of implementing a basic tool in an MCP server:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    const server = new Server({
      name: "example-server",
      version: "1.0.0"
    }, {
      capabilities: {
        tools: {}
      }
    });

    // Define available tools
    server.setRequestHandler(ListToolsRequestSchema, async () => {
      return {
        tools: [{
          name: "calculate_sum",
          description: "Add two numbers together",
          inputSchema: {
            type: "object",
            properties: {
              a: { type: "number" },
              b: { type: "number" }
            },
            required: ["a", "b"]
          }
        }]
      };
    });

    // Handle tool execution
    server.setRequestHandler(CallToolRequestSchema, async (request) => {
      if (request.params.name === "calculate_sum") {
        const { a, b } = request.params.arguments;
        return {
          content: [
            {
              type: "text",
              text: String(a + b)
            }
          ]
        };
      }
      throw new Error("Tool not found");
    });
    ```
  </Tab>

  <Tab title="Python">
    ```python
    app = Server("example-server")

    @app.list_tools()
    async def list_tools() -> list[types.Tool]:
        return [
            types.Tool(
                name="calculate_sum",
                description="Add two numbers together",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "a": {"type": "number"},
                        "b": {"type": "number"}
                    },
                    "required": ["a", "b"]
                }
            )
        ]

    @app.call_tool()
    async def call_tool(
        name: str,
        arguments: dict
    ) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:
        if name == "calculate_sum":
            a = arguments["a"]
            b = arguments["b"]
            result = a + b
            return [types.TextContent(type="text", text=str(result))]
        raise ValueError(f"Tool not found: {name}")
    ```
  </Tab>
</Tabs>

## Example tool patterns

Here are some examples of types of tools that a server could provide:

### System operations

Tools that interact with the local system:

```typescript
{
  name: "execute_command",
  description: "Run a shell command",
  inputSchema: {
    type: "object",
    properties: {
      command: { type: "string" },
      args: { type: "array", items: { type: "string" } }
    }
  }
}
```

### API integrations

Tools that wrap external APIs:

```typescript
{
  name: "github_create_issue",
  description: "Create a GitHub issue",
  inputSchema: {
    type: "object",
    properties: {
      title: { type: "string" },
      body: { type: "string" },
      labels: { type: "array", items: { type: "string" } }
    }
  }
}
```

### Data processing

Tools that transform or analyze data:

```typescript
{
  name: "analyze_csv",
  description: "Analyze a CSV file",
  inputSchema: {
    type: "object",
    properties: {
      filepath: { type: "string" },
      operations: {
        type: "array",
        items: {
          enum: ["sum", "average", "count"]
        }
      }
    }
  }
}
```

## Best practices

When implementing tools:

1.  Provide clear, descriptive names and descriptions
2.  Use detailed JSON Schema definitions for parameters
3.  Include examples in tool descriptions to demonstrate how the model should use them
4.  Implement proper error handling and validation
5.  Use progress reporting for long operations
6.  Keep tool operations focused and atomic
7.  Document expected return value structures
8.  Implement proper timeouts
9.  Consider rate limiting for resource-intensive operations
10. Log tool usage for debugging and monitoring

## Security considerations

When exposing tools:

### Input validation

*   Validate all parameters against the schema
*   Sanitize file paths and system commands
*   Validate URLs and external identifiers
*   Check parameter sizes and ranges
*   Prevent command injection

### Access control

*   Implement authentication where needed
*   Use appropriate authorization checks
*   Audit tool usage
*   Rate limit requests
*   Monitor for abuse

### Error handling

*   Don't expose internal errors to clients
*   Log security-relevant errors
*   Handle timeouts appropriately
*   Clean up resources after errors
*   Validate return values

## Tool discovery and updates

MCP supports dynamic tool discovery:

1.  Clients can list available tools at any time
2.  Servers can notify clients when tools change using `notifications/tools/list_changed`
3.  Tools can be added or removed during runtime
4.  Tool definitions can be updated (though this should be done carefully)

## Error handling

Tool errors should be reported within the result object, not as MCP protocol-level errors. This allows the LLM to see and potentially handle the error. When a tool encounters an error:

1.  Set `isError` to `true` in the result
2.  Include error details in the `content` array

Here's an example of proper error handling for tools:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    try {
      // Tool operation
      const result = performOperation();
      return {
        content: [
          {
            type: "text",
            text: `Operation successful: ${result}`
          }
        ]
      };
    } catch (error) {
      return {
        isError: true,
        content: [
          {
            type: "text",
            text: `Error: ${error.message}`
          }
        ]
      };
    }
    ```
  </Tab>

  <Tab title="Python">
    ```python
    try:
        # Tool operation
        result = perform_operation()
        return types.CallToolResult(
            content=[
                types.TextContent(
                    type="text",
                    text=f"Operation successful: {result}"
                )
            ]
        )
    except Exception as error:
        return types.CallToolResult(
            isError=True,
            content=[
                types.TextContent(
                    type="text",
                    text=f"Error: {str(error)}"
                )
            ]
        )
    ```
  </Tab>
</Tabs>

This approach allows the LLM to see that an error occurred and potentially take corrective action or request human intervention.

## Testing tools

A comprehensive testing strategy for MCP tools should cover:

*   **Functional testing**: Verify tools execute correctly with valid inputs and handle invalid inputs appropriately
*   **Integration testing**: Test tool interaction with external systems using both real and mocked dependencies
*   **Security testing**: Validate authentication, authorization, input sanitization, and rate limiting
*   **Performance testing**: Check behavior under load, timeout handling, and resource cleanup
*   **Error handling**: Ensure tools properly report errors through the MCP protocol and clean up resources


# Transports
Source: https://modelcontextprotocol.io/docs/concepts/transports

Learn about MCP's communication mechanisms

Transports in the Model Context Protocol (MCP) provide the foundation for communication between clients and servers. A transport handles the underlying mechanics of how messages are sent and received.

## Message Format

MCP uses [JSON-RPC](https://www.jsonrpc.org/) 2.0 as its wire format. The transport layer is responsible for converting MCP protocol messages into JSON-RPC format for transmission and converting received JSON-RPC messages back into MCP protocol messages.

There are three types of JSON-RPC messages used:

### Requests

```typescript
{
  jsonrpc: "2.0",
  id: number | string,
  method: string,
  params?: object
}
```

### Responses

```typescript
{
  jsonrpc: "2.0",
  id: number | string,
  result?: object,
  error?: {
    code: number,
    message: string,
    data?: unknown
  }
}
```

### Notifications

```typescript
{
  jsonrpc: "2.0",
  method: string,
  params?: object
}
```

## Built-in Transport Types

MCP includes two standard transport implementations:

### Standard Input/Output (stdio)

The stdio transport enables communication through standard input and output streams. This is particularly useful for local integrations and command-line tools.

Use stdio when:

*   Building command-line tools
*   Implementing local integrations
*   Needing simple process communication
*   Working with shell scripts

<Tabs>
  <Tab title="TypeScript (Server)">
    ```typescript
    const server = new Server({
      name: "example-server",
      version: "1.0.0"
    }, {
      capabilities: {}
    });

    const transport = new StdioServerTransport();
    await server.connect(transport);
    ```
  </Tab>

  <Tab title="TypeScript (Client)">
    ```typescript
    const client = new Client({
      name: "example-client",
      version: "1.0.0"
    }, {
      capabilities: {}
    });

    const transport = new StdioClientTransport({
      command: "./server",
      args: ["--option", "value"]
    });
    await client.connect(transport);
    ```
  </Tab>

  <Tab title="Python (Server)">
    ```python
    app = Server("example-server")

    async with stdio_server() as streams:
        await app.run(
            streams[0],
            streams[1],
            app.create_initialization_options()
        )
    ```
  </Tab>

  <Tab title="Python (Client)">
    ```python
    params = StdioServerParameters(
        command="./server",
        args=["--option", "value"]
    )

    async with stdio_client(params) as streams:
        async with ClientSession(streams[0], streams[1]) as session:
            await session.initialize()
    ```
  </Tab>
</Tabs>

### Server-Sent Events (SSE)

SSE transport enables server-to-client streaming with HTTP POST requests for client-to-server communication.

Use SSE when:

*   Only server-to-client streaming is needed
*   Working with restricted networks
*   Implementing simple updates

<Tabs>
  <Tab title="TypeScript (Server)">
    ```typescript
    import express from "express";

    const app = express();

    const server = new Server({
      name: "example-server",
      version: "1.0.0"
    }, {
      capabilities: {}
    });

    let transport: SSEServerTransport | null = null;

    app.get("/sse", (req, res) => {
      transport = new SSEServerTransport("/messages", res);
      server.connect(transport);
    });

    app.post("/messages", (req, res) => {
      if (transport) {
        transport.handlePostMessage(req, res);
      }
    });

    app.listen(3000);
    ```
  </Tab>

  <Tab title="TypeScript (Client)">
    ```typescript
    const client = new Client({
      name: "example-client",
      version: "1.0.0"
    }, {
      capabilities: {}
    });

    const transport = new SSEClientTransport(
      new URL("http://localhost:3000/sse")
    );
    await client.connect(transport);
    ```
  </Tab>

  <Tab title="Python (Server)">
    ```python
    from mcp.server.sse import SseServerTransport
    from starlette.applications import Starlette
    from starlette.routing import Route

    app = Server("example-server")
    sse = SseServerTransport("/messages")

    async def handle_sse(scope, receive, send):
        async with sse.connect_sse(scope, receive, send) as streams:
            await app.run(streams[0], streams[1], app.create_initialization_options())

    async def handle_messages(scope, receive, send):
        await sse.handle_post_message(scope, receive, send)

    starlette_app = Starlette(
        routes=[
            Route("/sse", endpoint=handle_sse),
            Route("/messages", endpoint=handle_messages, methods=["POST"]),
        ]
    )
    ```
  </Tab>

  <Tab title="Python (Client)">
    ```python
    async with sse_client("http://localhost:8000/sse") as streams:
        async with ClientSession(streams[0], streams[1]) as session:
            await session.initialize()
    ```
  </Tab>
</Tabs>

## Custom Transports

MCP makes it easy to implement custom transports for specific needs. Any transport implementation just needs to conform to the Transport interface:

You can implement custom transports for:

*   Custom network protocols
*   Specialized communication channels
*   Integration with existing systems
*   Performance optimization

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    interface Transport {
      // Start processing messages
      start(): Promise<void>;

      // Send a JSON-RPC message
      send(message: JSONRPCMessage): Promise<void>;

      // Close the connection
      close(): Promise<void>;

      // Callbacks
      onclose?: () => void;
      onerror?: (error: Error) => void;
      onmessage?: (message: JSONRPCMessage) => void;
    }
    ```
  </Tab>

  <Tab title="Python">
    Note that while MCP Servers are often implemented with asyncio, we recommend
    implementing low-level interfaces like transports with `anyio` for wider compatibility.

    ```python
    @contextmanager
    async def create_transport(
        read_stream: MemoryObjectReceiveStream[JSONRPCMessage | Exception],
        write_stream: MemoryObjectSendStream[JSONRPCMessage]
    ):
        """
        Transport interface for MCP.

        Args:
            read_stream: Stream to read incoming messages from
            write_stream: Stream to write outgoing messages to
        """
        async with anyio.create_task_group() as tg:
            try:
                # Start processing messages
                tg.start_soon(lambda: process_messages(read_stream))

                # Send messages
                async with write_stream:
                    yield write_stream

            except Exception as exc:
                # Handle errors
                raise exc
            finally:
                # Clean up
                tg.cancel_scope.cancel()
                await write_stream.aclose()
                await read_stream.aclose()
    ```
  </Tab>
</Tabs>

## Error Handling

Transport implementations should handle various error scenarios:

1.  Connection errors
2.  Message parsing errors
3.  Protocol errors
4.  Network timeouts
5.  Resource cleanup

Example error handling:

<Tabs>
  <Tab title="TypeScript">
    ```typescript
    class ExampleTransport implements Transport {
      async start() {
        try {
          // Connection logic
        } catch (error) {
          this.onerror?.(new Error(`Failed to connect: ${error}`));
          throw error;
        }
      }

      async send(message: JSONRPCMessage) {
        try {
          // Sending logic
        } catch (error) {
          this.onerror?.(new Error(`Failed to send message: ${error}`));
          throw error;
        }
      }
    }
    ```
  </Tab>

  <Tab title="Python">
    Note that while MCP Servers are often implemented with asyncio, we recommend
    implementing low-level interfaces like transports with `anyio` for wider compatibility.

    ```python
    @contextmanager
    async def example_transport(scope: Scope, receive: Receive, send: Send):
        try:
            # Create streams for bidirectional communication
            read_stream_writer, read_stream = anyio.create_memory_object_stream(0)
            write_stream, write_stream_reader = anyio.create_memory_object_stream(0)

            async def message_handler():
                try:
                    async with read_stream_writer:
                        # Message handling logic
                        pass
                except Exception as exc:
                    logger.error(f"Failed to handle message: {exc}")
                    raise exc

            async with anyio.create_task_group() as tg:
                tg.start_soon(message_handler)
                try:
                    # Yield streams for communication
                    yield read_stream, write_stream
                except Exception as exc:
                    logger.error(f"Transport error: {exc}")
                    raise exc
                finally:
                    tg.cancel_scope.cancel()
                    await write_stream.aclose()
                    await read_stream.aclose()
        except Exception as exc:
            logger.error(f"Failed to initialize transport: {exc}")
            raise exc
    ```
  </Tab>
</Tabs>

## Best Practices

When implementing or using MCP transport:

1.  Handle connection lifecycle properly
2.  Implement proper error handling
3.  Clean up resources on connection close
4.  Use appropriate timeouts
5.  Validate messages before sending
6.  Log transport events for debugging
7.  Implement reconnection logic when appropriate
8.  Handle backpressure in message queues
9.  Monitor connection health
10. Implement proper security measures

## Security Considerations

When implementing transport:

### Authentication and Authorization

*   Implement proper authentication mechanisms
*   Validate client credentials
*   Use secure token handling
*   Implement authorization checks

### Data Security

*   Use TLS for network transport
*   Encrypt sensitive data
*   Validate message integrity
*   Implement message size limits
*   Sanitize input data

### Network Security

*   Implement rate limiting
*   Use appropriate timeouts
*   Handle denial of service scenarios
*   Monitor for unusual patterns
*   Implement proper firewall rules

## Debugging Transport

Tips for debugging transport issues:

1.  Enable debug logging
2.  Monitor message flow
3.  Check connection states
4.  Validate message formats
5.  Test error scenarios
6.  Use network analysis tools
7.  Implement health checks
8.  Monitor resource usage
9.  Test edge cases
10. Use proper error tracking


# Debugging
Source: https://modelcontextprotocol.io/docs/tools/debugging

A comprehensive guide to debugging Model Context Protocol (MCP) integrations

Effective debugging is essential when developing MCP servers or integrating them with applications. This guide covers the debugging tools and approaches available in the MCP ecosystem.

<Info>
  This guide is for macOS. Guides for other platforms are coming soon.
</Info>

## Debugging tools overview

MCP provides several tools for debugging at different levels:

1.  **MCP Inspector**
    *   Interactive debugging interface
    *   Direct server testing
    *   See the [Inspector guide](/docs/tools/inspector) for details

2.  **Claude Desktop Developer Tools**
    *   Integration testing
    *   Log collection
    *   Chrome DevTools integration

3.  **Server Logging**
    *   Custom logging implementations
    *   Error tracking
    *   Performance monitoring

## Debugging in Claude Desktop

### Checking server status

The Claude.app interface provides basic server status information:

1.  Click the <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-plug-icon.svg" style={{display: 'inline', margin: 0, height: '1.3em'}} /> icon to view:
    *   Connected servers
    *   Available prompts and resources

2.  Click the <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg" style={{display: 'inline', margin: 0, height: '1.3em'}} /> icon to view:
    *   Tools made available to the model

### Viewing logs

Review detailed MCP logs from Claude Desktop:

```bash
# Follow logs in real-time
tail -n 20 -F ~/Library/Logs/Claude/mcp*.log
```

The logs capture:

*   Server connection events
*   Configuration issues
*   Runtime errors
*   Message exchanges

### Using Chrome DevTools

Access Chrome's developer tools inside Claude Desktop to investigate client-side errors:

1.  Create a `developer_settings.json` file with `allowDevTools` set to true:

```bash
echo '{"allowDevTools": true}' > ~/Library/Application\ Support/Claude/developer_settings.json
```

2.  Open DevTools: `Command-Option-Shift-i`

Note: You'll see two DevTools windows:

*   Main content window
*   App title bar window

Use the Console panel to inspect client-side errors.

Use the Network panel to inspect:

*   Message payloads
*   Connection timing

## Common issues

### Working directory

When using MCP servers with Claude Desktop:

*   The working directory for servers launched via `claude_desktop_config.json` may be undefined (like `/` on macOS) since Claude Desktop could be started from anywhere
*   Always use absolute paths in your configuration and `.env` files to ensure reliable operation
*   For testing servers directly via command line, the working directory will be where you run the command

For example in `claude_desktop_config.json`, use:

```json
{
  "command": "npx",
  "args": ["-y", "@modelcontextprotocol/server-filesystem", "/Users/username/data"]
}
```

Instead of relative paths like `./data`

### Environment variables

MCP servers inherit only a subset of environment variables automatically, like `USER`, `HOME`, and `PATH`.

To override the default variables or provide your own, you can specify an `env` key in `claude_desktop_config.json`:

```json
{
  "myserver": {
    "command": "mcp-server-myapp",
    "env": {
      "MYAPP_API_KEY": "some_key",
    }
  }
}
```

### Server initialization

Common initialization problems:

1.  **Path Issues**
    *   Incorrect server executable path
    *   Missing required files
    *   Permission problems
    *   Try using an absolute path for `command`

2.  **Configuration Errors**
    *   Invalid JSON syntax
    *   Missing required fields
    *   Type mismatches

3.  **Environment Problems**
    *   Missing environment variables
    *   Incorrect variable values
    *   Permission restrictions

### Connection problems

When servers fail to connect:

1.  Check Claude Desktop logs
2.  Verify server process is running
3.  Test standalone with [Inspector](/docs/tools/inspector)
4.  Verify protocol compatibility

## Implementing logging

### Server-side logging

When building a server that uses the local stdio [transport](/docs/concepts/transports), all messages logged to stderr (standard error) will be captured by the host application (e.g., Claude Desktop) automatically.

<Warning>
  Local MCP servers should not log messages to stdout (standard out), as this will interfere with protocol operation.
</Warning>

For all [transports](/docs/concepts/transports), you can also provide logging to the client by sending a log message notification:

<Tabs>
  <Tab title="Python">
    ```python
    server.request_context.session.send_log_message(
      level="info",
      data="Server started successfully",
    )
    ```
  </Tab>

  <Tab title="TypeScript">
    ```typescript
    server.sendLoggingMessage({
      level: "info",
      data: "Server started successfully",
    });
    ```
  </Tab>
</Tabs>

Important events to log:

*   Initialization steps
*   Resource access
*   Tool execution
*   Error conditions
*   Performance metrics

### Client-side logging

In client applications:

1.  Enable debug logging
2.  Monitor network traffic
3.  Track message exchanges
4.  Record error states

## Debugging workflow

### Development cycle

1.  Initial Development
    *   Use [Inspector](/docs/tools/inspector) for basic testing
    *   Implement core functionality
    *   Add logging points

2.  Integration Testing
    *   Test in Claude Desktop
    *   Monitor logs
    *   Check error handling

### Testing changes

To test changes efficiently:

*   **Configuration changes**: Restart Claude Desktop
*   **Server code changes**: Use Command-R to reload
*   **Quick iteration**: Use [Inspector](/docs/tools/inspector) during development

## Best practices

### Logging strategy

1.  **Structured Logging**
    *   Use consistent formats
    *   Include context
    *   Add timestamps
    *   Track request IDs

2.  **Error Handling**
    *   Log stack traces
    *   Include error context
    *   Track error patterns
    *   Monitor recovery

3.  **Performance Tracking**
    *   Log operation timing
    *   Monitor resource usage
    *   Track message sizes
    *   Measure latency

### Security considerations

When debugging:

1.  **Sensitive Data**
    *   Sanitize logs
    *   Protect credentials
    *   Mask personal information

2.  **Access Control**
    *   Verify permissions
    *   Check authentication
    *   Monitor access patterns

## Getting help

When encountering issues:

1.  **First Steps**
    *   Check server logs
    *   Test with [Inspector](/docs/tools/inspector)
    *   Review configuration
    *   Verify environment

2.  **Support Channels**
    *   GitHub issues
    *   GitHub discussions

3.  **Providing Information**
    *   Log excerpts
    *   Configuration files
    *   Steps to reproduce
    *   Environment details

## Next steps

<CardGroup cols={2}>
  <Card title="MCP Inspector" icon="magnifying-glass" href="/docs/tools/inspector">
    Learn to use the MCP Inspector
  </Card>
</CardGroup>


# Inspector
Source: https://modelcontextprotocol.io/docs/tools/inspector

In-depth guide to using the MCP Inspector for testing and debugging Model Context Protocol servers

The [MCP Inspector](https://github.com/modelcontextprotocol/inspector) is an interactive developer tool for testing and debugging MCP servers. While the [Debugging Guide](/docs/tools/debugging) covers the Inspector as part of the overall debugging toolkit, this document provides a detailed exploration of the Inspector's features and capabilities.

## Getting started

### Installation and basic usage

The Inspector runs directly through `npx` without requiring installation:

```bash
npx @modelcontextprotocol/inspector <command>
```

```bash
npx @modelcontextprotocol/inspector <command> <arg1> <arg2>
```

#### Inspecting servers from NPM or PyPi

A common way to start server packages from [NPM](https://npmjs.com) or [PyPi](https://pypi.com).

<Tabs>
  <Tab title="NPM package">
    ```bash
    npx -y @modelcontextprotocol/inspector npx <package-name> <args>
    # For example
    npx -y @modelcontextprotocol/inspector npx server-postgres postgres://127.0.0.1/testdb
    ```
  </Tab>

  <Tab title="PyPi package">
    ```bash
    npx @modelcontextprotocol/inspector uvx <package-name> <args>
    # For example
    npx @modelcontextprotocol/inspector uvx mcp-server-git --repository ~/code/mcp/servers.git
    ```
  </Tab>
</Tabs>

#### Inspecting locally developed servers

To inspect servers locally developed or downloaded as a repository, the most common
way is:

<Tabs>
  <Tab title="TypeScript">
    ```bash
    npx @modelcontextprotocol/inspector node path/to/server/index.js args...
    ```
  </Tab>

  <Tab title="Python">
    ```bash
    npx @modelcontextprotocol/inspector \
      uv \
      --directory path/to/server \
      run \
      package-name \
      args...
    ```
  </Tab>
</Tabs>

Please carefully read any attached README for the most accurate instructions.

## Feature overview

<Frame caption="The MCP Inspector interface">
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/mcp-inspector.png" />
</Frame>

The Inspector provides several features for interacting with your MCP server:

### Server connection pane

*   Allows selecting the [transport](/docs/concepts/transports) for connecting to the server
*   For local servers, supports customizing the command-line arguments and environment

### Resources tab

*   Lists all available resources
*   Shows resource metadata (MIME types, descriptions)
*   Allows resource content inspection
*   Supports subscription testing

### Prompts tab

*   Displays available prompt templates
*   Shows prompt arguments and descriptions
*   Enables prompt testing with custom arguments
*   Previews generated messages

### Tools tab

*   Lists available tools
*   Shows tool schemas and descriptions
*   Enables tool testing with custom inputs
*   Displays tool execution results

### Notifications pane

*   Presents all logs recorded from the server
*   Shows notifications received from the server

## Best practices

### Development workflow

1.  Start Development
    *   Launch Inspector with your server
    *   Verify basic connectivity
    *   Check capability negotiation

2.  Iterative testing
    *   Make server changes
    *   Rebuild the server
    *   Reconnect the Inspector
    *   Test affected features
    *   Monitor messages

3.  Test edge cases
    *   Invalid inputs
    *   Missing prompt arguments
    *   Concurrent operations
    *   Verify error handling and error responses

## Next steps

<CardGroup cols={2}>
  <Card title="Inspector Repository" icon="github" href="https://github.com/modelcontextprotocol/inspector">
    Check out the MCP Inspector source code
  </Card>

  <Card title="Debugging Guide" icon="bug" href="/docs/tools/debugging">
    Learn about broader debugging strategies
  </Card>
</CardGroup>


# Example Servers
Source: https://modelcontextprotocol.io/examples

A list of example servers and implementations

This page showcases various Model Context Protocol (MCP) servers that demonstrate the protocol's capabilities and versatility. These servers enable Large Language Models (LLMs) to securely access tools and data sources.

## Reference implementations

These official reference servers demonstrate core MCP features and SDK usage:

### Data and file systems

* **[Filesystem](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem)** - Secure file operations with configurable access controls
* **[PostgreSQL](https://github.com/modelcontextprotocol/servers/tree/main/src/postgres)** - Read-only database access with schema inspection capabilities
* **[SQLite](https://github.com/modelcontextprotocol/servers/tree/main/src/sqlite)** - Database interaction and business intelligence features
* **[Google Drive](https://github.com/modelcontextprotocol/servers/tree/main/src/gdrive)** - File access and search capabilities for Google Drive

### Development tools

* **[Git](https://github.com/modelcontextprotocol/servers/tree/main/src/git)** - Tools to read, search, and manipulate Git repositories
* **[GitHub](https://github.com/modelcontextprotocol/servers/tree/main/src/github)** - Repository management, file operations, and GitHub API integration
* **[GitLab](https://github.com/modelcontextprotocol/servers/tree/main/src/gitlab)** - GitLab API integration enabling project management
* **[Sentry](https://github.com/modelcontextprotocol/servers/tree/main/src/sentry)** - Retrieving and analyzing issues from Sentry.io

### Web and browser automation

* **[Brave Search](https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search)** - Web and local search using Brave's Search API
* **[Fetch](https://github.com/modelcontextprotocol/servers/tree/main/src/fetch)** - Web content fetching and conversion optimized for LLM usage
* **[Puppeteer](https://github.com/modelcontextprotocol/servers/tree/main/src/puppeteer)** - Browser automation and web scraping capabilities

### Productivity and communication

* **[Slack](https://github.com/modelcontextprotocol/servers/tree/main/src/slack)** - Channel management and messaging capabilities
* **[Google Maps](https://github.com/modelcontextprotocol/servers/tree/main/src/google-maps)** - Location services, directions, and place details
* **[Memory](https://github.com/modelcontextprotocol/servers/tree/main/src/memory)** - Knowledge graph-based persistent memory system

### AI and specialized tools

* **[EverArt](https://github.com/modelcontextprotocol/servers/tree/main/src/everart)** - AI image generation using various models
* **[Sequential Thinking](https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking)** - Dynamic problem-solving through thought sequences
* **[AWS KB Retrieval](https://github.com/modelcontextprotocol/servers/tree/main/src/aws-kb-retrieval-server)** - Retrieval from AWS Knowledge Base using Bedrock Agent Runtime

## Official integrations

These MCP servers are maintained by companies for their platforms:

* **[Axiom](https://github.com/axiomhq/mcp-server-axiom)** - Query and analyze logs, traces, and event data using natural language
* **[Browserbase](https://github.com/browserbase/mcp-server-browserbase)** - Automate browser interactions in the cloud
* **[Cloudflare](https://github.com/cloudflare/mcp-server-cloudflare)** - Deploy and manage resources on the Cloudflare developer platform
* **[E2B](https://github.com/e2b-dev/mcp-server)** - Execute code in secure cloud sandboxes
* **[Neon](https://github.com/neondatabase/mcp-server-neon)** - Interact with the Neon serverless Postgres platform
* **[Obsidian Markdown Notes](https://github.com/calclavia/mcp-obsidian)** - Read and search through Markdown notes in Obsidian vaults
* **[Qdrant](https://github.com/qdrant/mcp-server-qdrant/)** - Implement semantic memory using the Qdrant vector search engine
* **[Raygun](https://github.com/MindscapeHQ/mcp-server-raygun)** - Access crash reporting and monitoring data
* **[Search1API](https://github.com/fatwang2/search1api-mcp)** - Unified API for search, crawling, and sitemaps
* **[Stripe](https://github.com/stripe/agent-toolkit)** - Interact with the Stripe API
* **[Tinybird](https://github.com/tinybirdco/mcp-tinybird)** - Interface with the Tinybird serverless ClickHouse platform

## Community highlights

A growing ecosystem of community-developed servers extends MCP's capabilities:

* **[Docker](https://github.com/ckreiling/mcp-server-docker)** - Manage containers, images, volumes, and networks
* **[Kubernetes](https://github.com/Flux159/mcp-server-kubernetes)** - Manage pods, deployments, and services
* **[Linear](https://github.com/jerhadf/linear-mcp-server)** - Project management and issue tracking
* **[Snowflake](https://github.com/datawiz168/mcp-snowflake-service)** - Interact with Snowflake databases
* **[Spotify](https://github.com/varunneal/spotify-mcp)** - Control Spotify playback and manage playlists
* **[Todoist](https://github.com/abhiz123/todoist-mcp-server)** - Task management integration

> **Note:** Community servers are untested and should be used at your own risk. They are not affiliated with or endorsed by Anthropic.

For a complete list of community servers, visit the [MCP Servers Repository](https://github.com/modelcontextprotocol/servers).

## Getting started

### Using reference servers

TypeScript-based servers can be used directly with `npx`:

```bash
npx -y @modelcontextprotocol/server-memory
```

Python-based servers can be used with `uvx` (recommended) or `pip`:

```bash
# Using uvx
uvx mcp-server-git

# Using pip
pip install mcp-server-git
python -m mcp_server_git
```

### Configuring with Claude

To use an MCP server with Claude, add it to your configuration:

```json
{
  "mcpServers": {
    "memory": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-memory"]
    },
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/files"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "<YOUR_TOKEN>"
      }
    }
  }
}
```

## Additional resources

* [MCP Servers Repository](https://github.com/modelcontextprotocol/servers) - Complete collection of reference implementations and community servers
* [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers) - Curated list of MCP servers
* [MCP CLI](https://github.com/wong2/mcp-cli) - Command-line inspector for testing MCP servers
* [MCP Get](https://mcp-get.com) - Tool for installing and managing MCP servers
* [Supergateway](https://github.com/supercorp-ai/supergateway) - Run MCP stdio servers over SSE

Visit our [GitHub Discussions](https://github.com/orgs/modelcontextprotocol/discussions) to engage with the MCP community.


# Introduction
Source: https://modelcontextprotocol.io/introduction

Get started with the Model Context Protocol (MCP)

<Note>Java SDK released! Check out [what else is new.](/development/updates)</Note>

MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.

## Why MCP?

MCP helps you build agents and complex workflows on top of LLMs. LLMs frequently need to integrate with data and tools, and MCP provides:

* A growing list of pre-built integrations that your LLM can directly plug into
* The flexibility to switch between LLM providers and vendors
* Best practices for securing your data within your infrastructure

### General architecture

At its core, MCP follows a client-server architecture where a host application can connect to multiple servers:

```mermaid
flowchart LR
    subgraph "Your Computer"
        Host["Host with MCP Client\n(Claude, IDEs, Tools)"]
        S1["MCP Server A"]
        S2["MCP Server B"]
        S3["MCP Server C"]
        Host <-->|"MCP Protocol"| S1
        Host <-->|"MCP Protocol"| S2
        Host <-->|"MCP Protocol"| S3
        S1 <--> D1[("Local\nData Source A")]
        S2 <--> D2[("Local\nData Source B")]
    end
    subgraph "Internet"
        S3 <-->|"Web APIs"| D3[("Remote\nService C")]
    end
```

* **MCP Hosts**: Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP
* **MCP Clients**: Protocol clients that maintain 1:1 connections with servers
* **MCP Servers**: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol
* **Local Data Sources**: Your computer's files, databases, and services that MCP servers can securely access
* **Remote Services**: External systems available over the internet (e.g., through APIs) that MCP servers can connect to

## Get started

Choose the path that best fits your needs:

#### Quick Starts

<CardGroup cols={2}>
  <Card title="For Server Developers" icon="bolt" href="/quickstart/server">
    Get started building your own server to use in Claude for Desktop and other clients
  </Card>

  <Card title="For Client Developers" icon="bolt" href="/quickstart/client">
    Get started building your own client that can integrate with all MCP servers
  </Card>

  <Card title="For Claude Desktop Users" icon="bolt" href="/quickstart/user">
    Get started using pre-built servers in Claude for Desktop
  </Card>
</CardGroup>

#### Examples

<CardGroup cols={2}>
  <Card title="Example Servers" icon="grid" href="/examples">
    Check out our gallery of official MCP servers and implementations
  </Card>

  <Card title="Example Clients" icon="cubes" href="/clients">
    View the list of clients that support MCP integrations
  </Card>
</CardGroup>

## Tutorials

<CardGroup cols={2}>
  <Card title="Building MCP with LLMs" icon="comments" href="/tutorials/building-mcp-with-llms">
    Learn how to use LLMs like Claude to speed up your MCP development
  </Card>

  <Card title="Debugging Guide" icon="bug" href="/docs/tools/debugging">
    Learn how to effectively debug MCP servers and integrations
  </Card>

  <Card title="MCP Inspector" icon="magnifying-glass" href="/docs/tools/inspector">
    Test and inspect your MCP servers with our interactive debugging tool
  </Card>
</CardGroup>

## Explore MCP

Dive deeper into MCP's core concepts and capabilities:

<CardGroup cols={2}>
  <Card title="Core architecture" icon="sitemap" href="/docs/concepts/architecture">
    Understand how MCP connects clients, servers, and LLMs
  </Card>

  <Card title="Resources" icon="database" href="/docs/concepts/resources">
    Expose data and content from your servers to LLMs
  </Card>

  <Card title="Prompts" icon="message" href="/docs/concepts/prompts">
    Create reusable prompt templates and workflows
  </Card>

  <Card title="Tools" icon="wrench" href="/docs/concepts/tools">
    Enable LLMs to perform actions through your server
  </Card>

  <Card title="Sampling" icon="robot" href="/docs/concepts/sampling">
    Let your servers request completions from LLMs
  </Card>

  <Card title="Transports" icon="network-wired" href="/docs/concepts/transports">
    Learn about MCP's communication mechanism
  </Card>
</CardGroup>

## Contributing

Want to contribute? Check out our [Contributing Guide](/development/contributing) to learn how you can help improve MCP.

## Support and Feedback

Here's how to get help or provide feedback:

* For bug reports and feature requests related to the MCP specification, SDKs, or documentation (open source), please [create a GitHub issue](https://github.com/modelcontextprotocol)
* For discussions or Q\&A about the MCP specification, use the [specification discussions](https://github.com/modelcontextprotocol/specification/discussions)
* For discussions or Q\&A about other MCP open source components, use the [organization discussions](https://github.com/orgs/modelcontextprotocol/discussions)
* For bug reports, feature requests, and questions related to Claude.app and claude.ai's MCP integration, please email [mcp-support@anthropic.com](mailto:mcp-support@anthropic.com)


# For Client Developers
Source: https://modelcontextprotocol.io/quickstart/client

Get started building your own client that can integrate with all MCP servers.

In this tutorial, you'll learn how to build a LLM-powered chatbot client that connects to MCP servers. It helps to have gone through the [Server quickstart](/quickstart/server) that guides you through the basic of building your first server.

<Tabs>
  <Tab title="Python">
    [You can find the complete code for this tutorial here.](https://github.com/modelcontextprotocol/quickstart-resources/tree/main/mcp-client)

    ## System Requirements

    Before starting, ensure your system meets these requirements:

    * Mac or Windows computer
    * Latest Python version installed
    * Latest version of `uv` installed

    ## Setting Up Your Environment

    First, create a new Python project with `uv`:

    ```bash
    # Create project directory
    uv init mcp-client
    cd mcp-client

    # Create virtual environment
    uv venv

    # Activate virtual environment
    # On Windows:
    .venv\Scripts\activate
    # On Unix or MacOS:
    source .venv/bin/activate

    # Install required packages
    uv add mcp anthropic python-dotenv

    # Remove boilerplate files
    rm hello.py

    # Create our main file
    touch client.py
    ```

    ## Setting Up Your API Key

    You'll need an Anthropic API key from the [Anthropic Console](https://console.anthropic.com/settings/keys).

    Create a `.env` file to store it:

    ```bash
    # Create .env file
    touch .env
    ```

    Add your key to the `.env` file:

    ```bash
    ANTHROPIC_API_KEY=<your key here>
    ```

    Add `.env` to your `.gitignore`:

    ```bash
    echo ".env" >> .gitignore
    ```

    <Warning>
      Make sure you keep your `ANTHROPIC_API_KEY` secure!
    </Warning>

    ## Creating the Client

    ### Basic Client Structure

    First, let's set up our imports and create the basic client class:

    ```python
    import asyncio
    from typing import Optional
    from contextlib import AsyncExitStack

    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client

    from anthropic import Anthropic
    from dotenv import load_dotenv

    load_dotenv()  # load environment variables from .env

    class MCPClient:
        def __init__(self):
            # Initialize session and client objects
            self.session: Optional[ClientSession] = None
            self.exit_stack = AsyncExitStack()
            self.anthropic = Anthropic()
        # methods will go here
    ```

    ### Server Connection Management

    Next, we'll implement the method to connect to an MCP server:

    ```python
    async def connect_to_server(self, server_script_path: str):
        """Connect to an MCP server

        Args:
            server_script_path: Path to the server script (.py or .js)
        """
        is_python = server_script_path.endswith('.py')
        is_js = server_script_path.endswith('.js')
        if not (is_python or is_js):
            raise ValueError("Server script must be a .py or .js file")

        command = "python" if is_python else "node"
        server_params = StdioServerParameters(
            command=command,
            args=[server_script_path],
            env=None
        )

        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))

        await self.session.initialize()

        # List available tools
        response = await self.session.list_tools()
        tools = response.tools
        print("\nConnected to server with tools:", [tool.name for tool in tools])
    ```

    ### Query Processing Logic

    Now let's add the core functionality for processing queries and handling tool calls:

    ```python
    async def process_query(self, query: str) -> str:
        """Process a query using Claude and available tools"""
        messages = [
            {
                "role": "user",
                "content": query
            }
        ]

        response = await self.session.list_tools()
        available_tools = [{
            "name": tool.name,
            "description": tool.description,
            "input_schema": tool.inputSchema
        } for tool in response.tools]

        # Initial Claude API call
        response = self.anthropic.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            messages=messages,
            tools=available_tools
        )

        # Process response and handle tool calls
        tool_results = []
        final_text = []

        assistant_message_content = []
        for content in response.content:
            if content.type == 'text':
                final_text.append(content.text)
                assistant_message_content.append(content)
            elif content.type == 'tool_use':
                tool_name = content.name
                tool_args = content.input

                # Execute tool call
                result = await self.session.call_tool(tool_name, tool_args)
                tool_results.append({"call": tool_name, "result": result})
                final_text.append(f"[Calling tool {tool_name} with args {tool_args}]")

                assistant_message_content.append(content)
                messages.append({
                    "role": "assistant",
                    "content": assistant_message_content
                })
                messages.append({
                    "role": "user",
                    "content": [
                        {
                            "type": "tool_result",
                            "tool_use_id": content.id,
                            "content": result.content
                        }
                    ]
                })

                # Get next response from Claude
                response = self.anthropic.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=1000,
                    messages=messages,
                    tools=available_tools
                )

                final_text.append(response.content[0].text)

        return "\n".join(final_text)
    ```

    ### Interactive Chat Interface

    Now we'll add the chat loop and cleanup functionality:

    ```python
    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Client Started!")
        print("Type your queries or 'quit' to exit.")

        while True:
            try:
                query = input("\nQuery: ").strip()

                if query.lower() == 'quit':
                    break

                response = await self.process_query(query)
                print("\n" + response)

            except Exception as e:
                print(f"\nError: {str(e)}")

    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()
    ```

    ### Main Entry Point

    Finally, we'll add the main execution logic:

    ```python
    async def main():
        if len(sys.argv) < 2:
            print("Usage: python client.py <path_to_server_script>")
            sys.exit(1)

        client = MCPClient()
        try:
            await client.connect_to_server(sys.argv[1])
            await client.chat_loop()
        finally:
            await client.cleanup()

    if __name__ == "__main__":
        import sys
        asyncio.run(main())
    ```

    You can find the complete `client.py` file [here.](https://gist.github.com/zckly/f3f28ea731e096e53b39b47bf0a2d4b1)

    ## Key Components Explained

    ### 1. Client Initialization

    * The `MCPClient` class initializes with session management and API clients
    * Uses `AsyncExitStack` for proper resource management
    * Configures the Anthropic client for Claude interactions

    ### 2. Server Connection

    * Supports both Python and Node.js servers
    * Validates server script type
    * Sets up proper communication channels
    * Initializes the session and lists available tools

    ### 3. Query Processing

    * Maintains conversation context
    * Handles Claude's responses and tool calls
    * Manages the message flow between Claude and tools
    * Combines results into a coherent response

    ### 4. Interactive Interface

    * Provides a simple command-line interface
    * Handles user input and displays responses
    * Includes basic error handling
    * Allows graceful exit

    ### 5. Resource Management

    * Proper cleanup of resources
    * Error handling for connection issues
    * Graceful shutdown procedures

    ## Common Customization Points

    1. **Tool Handling**
       * Modify `process_query()` to handle specific tool types
       * Add custom error handling for tool calls
       * Implement tool-specific response formatting

    2. **Response Processing**
       * Customize how tool results are formatted
       * Add response filtering or transformation
       * Implement custom logging

    3. **User Interface**
       * Add a GUI or web interface
       * Implement rich console output
       * Add command history or auto-completion

    ## Running the Client

    To run your client with any MCP server:

    ```bash
    uv run client.py path/to/server.py # python server
    uv run client.py path/to/build/index.js # node server
    ```

    <Note>
      If you're continuing the weather tutorial from the server quickstart, your command might look something like this: `python client.py .../weather/src/weather/server.py`
    </Note>

    The client will:

    1. Connect to the specified server
    2. List available tools
    3. Start an interactive chat session where you can:
       * Enter queries
       * See tool executions
       * Get responses from Claude

    Here's an example of what it should look like if connected to the weather server from the server quickstart:

    <Frame>
      <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/client-claude-cli-python.png" />
    </Frame>

    ## How It Works

    When you submit a query:

    1. The client gets the list of available tools from the server
    2. Your query is sent to Claude along with tool descriptions
    3. Claude decides which tools (if any) to use
    4. The client executes any requested tool calls through the server
    5. Results are sent back to Claude
    6. Claude provides a natural language response
    7. The response is displayed to you

    ## Best practices

    1. **Error Handling**
       * Always wrap tool calls in try-catch blocks
       * Provide meaningful error messages
       * Gracefully handle connection issues

    2. **Resource Management**
       * Use `AsyncExitStack` for proper cleanup
       * Close connections when done
       * Handle server disconnections

    3. **Security**
       * Store API keys securely in `.env`
       * Validate server responses
       * Be cautious with tool permissions

    ## Troubleshooting

    ### Server Path Issues

    * Double-check the path to your server script is correct
    * Use the absolute path if the relative path isn't working
    * For Windows users, make sure to use forward slashes (/) or escaped backslashes (\\) in the path
    * Verify the server file has the correct extension (.py for Python or .js for Node.js)

    Example of correct path usage:

    ```bash
    # Relative path
    uv run client.py ./server/weather.py

    # Absolute path
    uv run client.py /Users/username/projects/mcp-server/weather.py

    # Windows path (either format works)
    uv run client.py C:/projects/mcp-server/weather.py
    uv run client.py C:\\projects\\mcp-server\\weather.py
    ```

    ### Response Timing

    * The first response might take up to 30 seconds to return
    * This is normal and happens while:
      * The server initializes
      * Claude processes the query
      * Tools are being executed
    * Subsequent responses are typically faster
    * Don't interrupt the process during this initial waiting period

    ### Common Error Messages

    If you see:

    * `FileNotFoundError`: Check your server path
    * `Connection refused`: Ensure the server is running and the path is correct
    * `Tool execution failed`: Verify the tool's required environment variables are set
    * `Timeout error`: Consider increasing the timeout in your client configuration
  </Tab>

  <Tab title="Java">
    <Note>
      This is a quickstart demo based on Spring AI MCP auto-configuration and boot starters.
      To learn how to create sync and async MCP Clients manually, consult the [Java SDK Client](/sdk/java/mcp-client) documentation
    </Note>

    This example demonstrates how to build an interactive chatbot that combines Spring AI's Model Context Protocol (MCP) with the [Brave Search MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/brave-search). The application creates a conversational interface powered by Anthropic's Claude AI model that can perform internet searches through Brave Search, enabling natural language interactions with real-time web data.
    [You can find the complete code for this tutorial here.](https://github.com/spring-projects/spring-ai-examples/tree/main/model-context-protocol/web-search/brave-chatbot)

    ## System Requirements

    Before starting, ensure your system meets these requirements:

    * Java 17 or higher
    * Maven 3.6+
    * npx package manager
    * Anthropic API key (Claude)
    * Brave Search API key

    ## Setting Up Your Environment

    1. Install npx (Node Package eXecute):
       First, make sure to install [npm](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)
       and then run:
       ```bash
       npm install -g npx
       ```

    2. Clone the repository:
       ```bash
       git clone https://github.com/spring-projects/spring-ai-examples.git
       cd model-context-protocol/brave-chatbot
       ```

    3. Set up your API keys:
       ```bash
       export ANTHROPIC_API_KEY='your-anthropic-api-key-here'
       export BRAVE_API_KEY='your-brave-api-key-here'
       ```

    4. Build the application:
       ```bash
       ./mvnw clean install
       ```

    5. Run the application using Maven:
       ```bash
       ./mvnw spring-boot:run
       ```

    <Warning>
      Make sure you keep your `ANTHROPIC_API_KEY` and `BRAVE_API_KEY` keys secure!
    </Warning>

    ## How it Works

    The application integrates Spring AI with the Brave Search MCP server through several components:

    ### MCP Client Configuration

    1. Required dependencies in pom.xml:

    ```xml
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-mcp-client-spring-boot-starter</artifactId>
    </dependency>
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-anthropic-spring-boot-starter</artifactId>
    </dependency>
    ```

    2. Application properties (application.yml):

    ```yml
    spring:
      ai:
        mcp:
          client:
            enabled: true
            name: brave-search-client
            version: 1.0.0
            type: SYNC
            request-timeout: 20s
            stdio:
              root-change-notification: true
              servers-configuration: classpath:/mcp-servers-config.json
        anthropic:
          api-key: ${ANTHROPIC_API_KEY}
    ```

    This activates the `spring-ai-mcp-client-spring-boot-starter` to create one or more `McpClient`s based on the provided server configuration.

    3. MCP Server Configuration (`mcp-servers-config.json`):

    ```json
    {
      "mcpServers": {
        "brave-search": {
          "command": "npx",
          "args": [
            "-y",
            "@modelcontextprotocol/server-brave-search"
          ],
          "env": {
            "BRAVE_API_KEY": "<PUT YOUR BRAVE API KEY>"
          }
        }
      }
    }
    ```

    ### Chat Implementation

    The chatbot is implemented using Spring AI's ChatClient with MCP tool integration:

    ```java
    var chatClient = chatClientBuilder
        .defaultSystem("You are useful assistant, expert in AI and Java.")
        .defaultTools((Object[]) mcpToolAdapter.toolCallbacks())
        .defaultAdvisors(new MessageChatMemoryAdvisor(new InMemoryChatMemory()))
        .build();
    ```

    Key features:

    * Uses Claude AI model for natural language understanding
    * Integrates Brave Search through MCP for real-time web search capabilities
    * Maintains conversation memory using InMemoryChatMemory
    * Runs as an interactive command-line application

    ### Build and run

    ```bash
    ./mvnw clean install
    java -jar ./target/ai-mcp-brave-chatbot-0.0.1-SNAPSHOT.jar
    ```

    or

    ```bash
    ./mvnw spring-boot:run
    ```

    The application will start an interactive chat session where you can ask questions. The chatbot will use Brave Search when it needs to find information from the internet to answer your queries.

    The chatbot can:

    * Answer questions using its built-in knowledge
    * Perform web searches when needed using Brave Search
    * Remember context from previous messages in the conversation
    * Combine information from multiple sources to provide comprehensive answers

    ### Advanced Configuration

    The MCP client supports additional configuration options:

    * Client customization through `McpSyncClientCustomizer` or `McpAsyncClientCustomizer`
    * Multiple clients with multiple transport types: `STDIO` and `SSE` (Server-Sent Events)
    * Integration with Spring AI's tool execution framework
    * Automatic client initialization and lifecycle management

    For WebFlux-based applications, you can use the WebFlux starter instead:

    ```xml
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-mcp-client-webflux-spring-boot-starter</artifactId>
    </dependency>
    ```

    This provides similar functionality but uses a WebFlux-based SSE transport implementation, recommended for production deployments.
  </Tab>
</Tabs>

## Next steps

<CardGroup cols={2}>
  <Card title="Example servers" icon="grid" href="/examples">
    Check out our gallery of official MCP servers and implementations
  </Card>

  <Card title="Clients" icon="cubes" href="/clients">
    View the list of clients that support MCP integrations
  </Card>

  <Card title="Building MCP with LLMs" icon="comments" href="/building-mcp-with-llms">
    Learn how to use LLMs like Claude to speed up your MCP development
  </Card>

  <Card title="Core architecture" icon="sitemap" href="/docs/concepts/architecture">
    Understand how MCP connects clients, servers, and LLMs
  </Card>
</CardGroup>


# For Server Developers
Source: https://modelcontextprotocol.io/quickstart/server

Get started building your own server to use in Claude for Desktop and other clients.

In this tutorial, we'll build a simple MCP weather server and connect it to a host, Claude for Desktop. We'll start with a basic setup, and then progress to more complex use cases.

### What we'll be building

Many LLMs (including Claude) do not currently have the ability to fetch the forecast and severe weather alerts. Let's use MCP to solve that!

We'll build a server that exposes two tools: `get-alerts` and `get-forecast`. Then we'll connect the server to an MCP host (in this case, Claude for Desktop):

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/weather-alerts.png" />
</Frame>

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/current-weather.png" />
</Frame>

<Note>
  Servers can connect to any client. We've chosen Claude for Desktop here for simplicity, but we also have guides on [building your own client](/quickstart/client) as well as a [list of other clients here](/clients).
</Note>

<Accordion title="Why Claude for Desktop and not Claude.ai?">
  Because servers are locally run, MCP currently only supports desktop hosts. Remote hosts are in active development.
</Accordion>

### Core MCP Concepts

MCP servers can provide three main types of capabilities:

1. **Resources**: File-like data that can be read by clients (like API responses or file contents)
2. **Tools**: Functions that can be called by the LLM (with user approval)
3. **Prompts**: Pre-written templates that help users accomplish specific tasks

This tutorial will primarily focus on tools.

<Tabs>
  <Tab title="Python">
    Let's get started with building our weather server! [You can find the complete code for what we'll be building here.](https://github.com/modelcontextprotocol/quickstart-resources/tree/main/weather-server-python)

    ### Prerequisite knowledge

    This quickstart assumes you have familiarity with:

    * Python
    * LLMs like Claude

    ### System requirements

    * Python 3.10 or higher installed.
    * You must use the Python MCP SDK 1.2.0 or higher.

    ### Set up your environment

    First, let's install `uv` and set up our Python project and environment:

    <CodeGroup>
      ```bash MacOS/Linux
      curl -LsSf https://astral.sh/uv/install.sh | sh
      ```

      ```powershell Windows
      powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
      ```
    </CodeGroup>

    Make sure to restart your terminal afterwards to ensure that the `uv` command gets picked up.

    Now, let's create and set up our project:

    <CodeGroup>
      ```bash MacOS/Linux
      # Create a new directory for our project
      uv init weather
      cd weather

      # Create virtual environment and activate it
      uv venv
      source .venv/bin/activate

      # Install dependencies
      uv add "mcp[cli]" httpx

      # Create our server file
      touch weather.py
      ```

      ```powershell Windows
      # Create a new directory for our project
      uv init weather
      cd weather

      # Create virtual environment and activate it
      uv venv
      .venv\Scripts\activate

      # Install dependencies
      uv add mcp[cli] httpx

      # Create our server file
      new-item weather.py
      ```
    </CodeGroup>

    Now let's dive into building your server.

    ## Building your server

    ### Importing packages and setting up the instance

    Add these to the top of your `weather.py`:

    ```python
    from typing import Any
    import httpx
    from mcp.server.fastmcp import FastMCP

    # Initialize FastMCP server
    mcp = FastMCP("weather")

    # Constants
    NWS_API_BASE = "https://api.weather.gov"
    USER_AGENT = "weather-app/1.0"
    ```

    The FastMCP class uses Python type hints and docstrings to automatically generate tool definitions, making it easy to create and maintain MCP tools.

    ### Helper functions

    Next, let's add our helper functions for querying and formatting the data from the National Weather Service API:

    ```python
    async def make_nws_request(url: str) -> dict[str, Any] | None:
        """Make a request to the NWS API with proper error handling."""
        headers = {
            "User-Agent": USER_AGENT,
            "Accept": "application/geo+json"
        }
        async with httpx.AsyncClient() as client:
            try:
                response = await client.get(url, headers=headers, timeout=30.0)
                response.raise_for_status()
                return response.json()
            except Exception:
                return None

    def format_alert(feature: dict) -> str:
        """Format an alert feature into a readable string."""
        props = feature["properties"]
        return f"""
    Event: {props.get('event', 'Unknown')}
    Area: {props.get('areaDesc', 'Unknown')}
    Severity: {props.get('severity', 'Unknown')}
    Description: {props.get('description', 'No description available')}
    Instructions: {props.get('instruction', 'No specific instructions provided')}
    """
    ```

    ### Implementing tool execution

    The tool execution handler is responsible for actually executing the logic of each tool. Let's add it:

    ```python
    @mcp.tool()
    async def get_alerts(state: str) -> str:
        """Get weather alerts for a US state.

        Args:
            state: Two-letter US state code (e.g. CA, NY)
        """
        url = f"{NWS_API_BASE}/alerts/active/area/{state}"
        data = await make_nws_request(url)

        if not data or "features" not in data:
            return "Unable to fetch alerts or no alerts found."

        if not data["features"]:
            return "No active alerts for this state."

        alerts = [format_alert(feature) for feature in data["features"]]
        return "\n---\n".join(alerts)

    @mcp.tool()
    async def get_forecast(latitude: float, longitude: float) -> str:
        """Get weather forecast for a location.

        Args:
            latitude: Latitude of the location
            longitude: Longitude of the location
        """
        # First get the forecast grid endpoint
        points_url = f"{NWS_API_BASE}/points/{latitude},{longitude}"
        points_data = await make_nws_request(points_url)

        if not points_data:
            return "Unable to fetch forecast data for this location."

        # Get the forecast URL from the points response
        forecast_url = points_data["properties"]["forecast"]
        forecast_data = await make_nws_request(forecast_url)

        if not forecast_data:
            return "Unable to fetch detailed forecast."

        # Format the periods into a readable forecast
        periods = forecast_data["properties"]["periods"]
        forecasts = []
        for period in periods[:5]:  # Only show next 5 periods
            forecast = f"""
    {period['name']}:
    Temperature: {period['temperature']}{period['temperatureUnit']}
    Wind: {period['windSpeed']} {period['windDirection']}
    Forecast: {period['detailedForecast']}
    """
            forecasts.append(forecast)

        return "\n---\n".join(forecasts)
    ```

    ### Running the server

    Finally, let's initialize and run the server:

    ```python
    if __name__ == "__main__":
        # Initialize and run the server
        mcp.run(transport='stdio')
    ```

    Your server is complete! Run `uv run weather.py` to confirm that everything's working.

    Let's now test your server from an existing MCP host, Claude for Desktop.

    ## Testing your server with Claude for Desktop

    <Note>
      Claude for Desktop is not yet available on Linux. Linux users can proceed to the [Building a client](/quickstart/client) tutorial to build an MCP client that connects to the server we just built.
    </Note>

    First, make sure you have Claude for Desktop installed. [You can install the latest version
    here.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**

    We'll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn't exist.

    For example, if you have [VS Code](https://code.visualstudio.com/) installed:

    <Tabs>
      <Tab title="MacOS/Linux">
        ```bash
        code ~/Library/Application\ Support/Claude/claude_desktop_config.json
        ```
      </Tab>

      <Tab title="Windows">
        ```powershell
        code $env:AppData\Claude\claude_desktop_config.json
        ```
      </Tab>
    </Tabs>

    You'll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.

    In this case, we'll add our single weather server like so:

    <Tabs>
      <Tab title="MacOS/Linux">
        ```json Python
        {
            "mcpServers": {
                "weather": {
                    "command": "uv",
                    "args": [
                        "--directory",
                        "/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather",
                        "run",
                        "weather.py"
                    ]
                }
            }
        }
        ```
      </Tab>

      <Tab title="Windows">
        ```json Python
        {
            "mcpServers": {
                "weather": {
                    "command": "uv",
                    "args": [
                        "--directory",
                        "C:\\ABSOLUTE\\PATH\\TO\\PARENT\\FOLDER\\weather",
                        "run",
                        "weather.py"
                    ]
                }
            }
        }
        ```
      </Tab>
    </Tabs>

    <Warning>
      You may need to put the full path to the `uv` executable in the `command` field. You can get this by running `which uv` on MacOS/Linux or `where uv` on Windows.
    </Warning>

    <Note>
      Make sure you pass in the absolute path to your server.
    </Note>

    This tells Claude for Desktop:

    1. There's an MCP server named "weather"
    2. To launch it by running `uv --directory /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather run weather`

    Save the file, and restart **Claude for Desktop**.
  </Tab>

  <Tab title="Node">
    Let's get started with building our weather server! [You can find the complete code for what we'll be building here.](https://github.com/modelcontextprotocol/quickstart-resources/tree/main/weather-server-typescript)

    ### Prerequisite knowledge

    This quickstart assumes you have familiarity with:

    * TypeScript
    * LLMs like Claude

    ### System requirements

    For TypeScript, make sure you have the latest version of Node installed.

    ### Set up your environment

    First, let's install Node.js and npm if you haven't already. You can download them from [nodejs.org](https://nodejs.org/).
    Verify your Node.js installation:

    ```bash
    node --version
    npm --version
    ```

    For this tutorial, you'll need Node.js version 16 or higher.

    Now, let's create and set up our project:

    <CodeGroup>
      ```bash MacOS/Linux
      # Create a new directory for our project
      mkdir weather
      cd weather

      # Initialize a new npm project
      npm init -y

      # Install dependencies
      npm install @modelcontextprotocol/sdk zod
      npm install -D @types/node typescript

      # Create our files
      mkdir src
      touch src/index.ts
      ```

      ```powershell Windows
      # Create a new directory for our project
      md weather
      cd weather

      # Initialize a new npm project
      npm init -y

      # Install dependencies
      npm install @modelcontextprotocol/sdk zod
      npm install -D @types/node typescript

      # Create our files
      md src
      new-item src\index.ts
      ```
    </CodeGroup>

    Update your package.json to add type: "module" and a build script:

    ```json package.json
    {
      "type": "module",
      "bin": {
        "weather": "./build/index.js"
      },
      "scripts": {
        "build": "tsc && node -e \"require('fs').chmodSync('build/index.js', '755')\"",
      },
      "files": [
        "build"
      ],
    }
    ```

    Create a `tsconfig.json` in the root of your project:

    ```json tsconfig.json
    {
      "compilerOptions": {
        "target": "ES2022",
        "module": "Node16",
        "moduleResolution": "Node16",
        "outDir": "./build",
        "rootDir": "./src",
        "strict": true,
        "esModuleInterop": true,
        "skipLibCheck": true,
        "forceConsistentCasingInFileNames": true
      },
      "include": ["src/**/*"],
      "exclude": ["node_modules"]
    }
    ```

    Now let's dive into building your server.

    ## Building your server

    ### Importing packages and setting up the instance

    Add these to the top of your `src/index.ts`:

    ```typescript
    import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
    import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
    import { z } from "zod";

    const NWS_API_BASE = "https://api.weather.gov";
    const USER_AGENT = "weather-app/1.0";

    // Create server instance
    const server = new McpServer({
      name: "weather",
      version: "1.0.0",
    });
    ```

    ### Helper functions

    Next, let's add our helper functions for querying and formatting the data from the National Weather Service API:

    ```typescript
    // Helper function for making NWS API requests
    async function makeNWSRequest<T>(url: string): Promise<T | null> {
      const headers = {
        "User-Agent": USER_AGENT,
        Accept: "application/geo+json",
      };

      try {
        const response = await fetch(url, { headers });
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        return (await response.json()) as T;
      } catch (error) {
        console.error("Error making NWS request:", error);
        return null;
      }
    }

    interface AlertFeature {
      properties: {
        event?: string;
        areaDesc?: string;
        severity?: string;
        status?: string;
        headline?: string;
      };
    }

    // Format alert data
    function formatAlert(feature: AlertFeature): string {
      const props = feature.properties;
      return [
        `Event: ${props.event || "Unknown"}`,
        `Area: ${props.areaDesc || "Unknown"}`,
        `Severity: ${props.severity || "Unknown"}`,
        `Status: ${props.status || "Unknown"}`,
        `Headline: ${props.headline || "No headline"}`,
        "---",
      ].join("\n");
    }

    interface ForecastPeriod {
      name?: string;
      temperature?: number;
      temperatureUnit?: string;
      windSpeed?: string;
      windDirection?: string;
      shortForecast?: string;
    }

    interface AlertsResponse {
      features: AlertFeature[];
    }

    interface PointsResponse {
      properties: {
        forecast?: string;
      };
    }

    interface ForecastResponse {
      properties: {
        periods: ForecastPeriod[];
      };
    }
    ```

    ### Implementing tool execution

    The tool execution handler is responsible for actually executing the logic of each tool. Let's add it:

    ```typescript
    // Register weather tools
    server.tool(
      "get-alerts",
      "Get weather alerts for a state",
      {
        state: z.string().length(2).describe("Two-letter state code (e.g. CA, NY)"),
      },
      async ({ state }) => {
        const stateCode = state.toUpperCase();
        const alertsUrl = `${NWS_API_BASE}/alerts?area=${stateCode}`;
        const alertsData = await makeNWSRequest<AlertsResponse>(alertsUrl);

        if (!alertsData) {
          return {
            content: [
              {
                type: "text",
                text: "Failed to retrieve alerts data",
              },
            ],
          };
        }

        const features = alertsData.features || [];
        if (features.length === 0) {
          return {
            content: [
              {
                type: "text",
                text: `No active alerts for ${stateCode}`,
              },
            ],
          };
        }

        const formattedAlerts = features.map(formatAlert);
        const alertsText = `Active alerts for ${stateCode}:\n\n${formattedAlerts.join("\n")}`;

        return {
          content: [
            {
              type: "text",
              text: alertsText,
            },
          ],
        };
      },
    );

    server.tool(
      "get-forecast",
      "Get weather forecast for a location",
      {
        latitude: z.number().min(-90).max(90).describe("Latitude of the location"),
        longitude: z.number().min(-180).max(180).describe("Longitude of the location"),
      },
      async ({ latitude, longitude }) => {
        // Get grid point data
        const pointsUrl = `${NWS_API_BASE}/points/${latitude.toFixed(4)},${longitude.toFixed(4)}`;
        const pointsData = await makeNWSRequest<PointsResponse>(pointsUrl);

        if (!pointsData) {
          return {
            content: [
              {
                type: "text",
                text: `Failed to retrieve grid point data for coordinates: ${latitude}, ${longitude}. This location may not be supported by the NWS API (only US locations are supported).`,
              },
            ],
          };
        }

        const forecastUrl = pointsData.properties?.forecast;
        if (!forecastUrl) {
          return {
            content: [
              {
                type: "text",
                text: "Failed to get forecast URL from grid point data",
              },
            ],
          };
        }

        // Get forecast data
        const forecastData = await makeNWSRequest<ForecastResponse>(forecastUrl);
        if (!forecastData) {
          return {
            content: [
              {
                type: "text",
                text: "Failed to retrieve forecast data",
              },
            ],
          };
        }

        const periods = forecastData.properties?.periods || [];
        if (periods.length === 0) {
          return {
            content: [
              {
                type: "text",
                text: "No forecast periods available",
              },
            ],
          };
        }

        // Format forecast periods
        const formattedForecast = periods.map((period: ForecastPeriod) =>
          [
            `${period.name || "Unknown"}:`,
            `Temperature: ${period.temperature || "Unknown"}${period.temperatureUnit || "F"}`,
            `Wind: ${period.windSpeed || "Unknown"} ${period.windDirection || ""}`,
            `${period.shortForecast || "No forecast available"}`,
            "---",
          ].join("\n"),
        );

        const forecastText = `Forecast for ${latitude}, ${longitude}:\n\n${formattedForecast.join("\n")}`;

        return {
          content: [
            {
              type: "text",
              text: forecastText,
            },
          ],
        };
      },
    );
    ```

    ### Running the server

    Finally, implement the main function to run the server:

    ```typescript
    async function main() {
      const transport = new StdioServerTransport();
      await server.connect(transport);
      console.error("Weather MCP Server running on stdio");
    }

    main().catch((error) => {
      console.error("Fatal error in main():", error);
      process.exit(1);
    });
    ```

    Make sure to run `npm run build` to build your server! This is a very important step in getting your server to connect.

    Let's now test your server from an existing MCP host, Claude for Desktop.

    ## Testing your server with Claude for Desktop

    <Note>
      Claude for Desktop is not yet available on Linux. Linux users can proceed to the [Building a client](/quickstart/client) tutorial to build an MCP client that connects to the server we just built.
    </Note>

    First, make sure you have Claude for Desktop installed. [You can install the latest version
    here.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**

    We'll need to configure Claude for Desktop for whichever MCP servers you want to use. To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor. Make sure to create the file if it doesn't exist.

    For example, if you have [VS Code](https://code.visualstudio.com/) installed:

    <Tabs>
      <Tab title="MacOS/Linux">
        ```bash
        code ~/Library/Application\ Support/Claude/claude_desktop_config.json
        ```
      </Tab>

      <Tab title="Windows">
        ```powershell
        code $env:AppData\Claude\claude_desktop_config.json
        ```
      </Tab>
    </Tabs>

    You'll then add your servers in the `mcpServers` key. The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.

    In this case, we'll add our single weather server like so:

    <Tabs>
      <Tab title="MacOS/Linux">
        <CodeGroup>
          ```json Node
          {
              "mcpServers": {
                  "weather": {
                      "command": "node",
                      "args": [
                          "/ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/index.js"
                      ]
                  }
              }
          }
          ```
        </CodeGroup>
      </Tab>

      <Tab title="Windows">
        <CodeGroup>
          ```json Node
          {
              "mcpServers": {
                  "weather": {
                      "command": "node",
                      "args": [
                          "C:\\PATH\\TO\\PARENT\\FOLDER\\weather\\build\\index.js"
                      ]
                  }
              }
          }
          ```
        </CodeGroup>
      </Tab>
    </Tabs>

    This tells Claude for Desktop:

    1. There's an MCP server named "weather"
    2. Launch it by running `node /ABSOLUTE/PATH/TO/PARENT/FOLDER/weather/build/index.js`

    Save the file, and restart **Claude for Desktop**.
  </Tab>

  <Tab title="Java">
    <Note>
      This is a quickstart demo based on Spring AI MCP auto-configuraiton and boot starters.
      To learn how to create sync and async MCP Servers, manually, consult the [Java SDK Server](/sdk/java/mcp-server) documentation.
    </Note>

    Let's get started with building our weather server!
    [You can find the complete code for what we'll be building here.](https://github.com/spring-projects/spring-ai-examples/tree/main/model-context-protocol/weather/starter-stdio-server)

    For more information, see the [MCP Server Boot Starter](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html) reference documentation.
    For manual MCP Server implementation, refer to the [MCP Server Java SDK documentation](/sdk/java/mcp-server).

    ### System requirements

    * Java 17 or higher installed.
    * [Spring Boot 3.3.x](https://docs.spring.io/spring-boot/installing.html) or higher

    ### Set up your environment

    Use the [Spring Initizer](https://start.spring.io/) to bootstrat the project.

    You will need to add the following dependencies:

    <Tabs>
      <Tab title="Maven">
        ```xml
        <dependencies>
              <dependency>
                  <groupId>org.springframework.ai</groupId>
                  <artifactId>spring-ai-mcp-server-spring-boot-starter</artifactId>
              </dependency>

              <dependency>
                  <groupId>org.springframework</groupId>
                  <artifactId>spring-web</artifactId>
              </dependency>
        </dependencies>
        ```
      </Tab>

      <Tab title="Gradle">
        ```groovy
        dependencies {
          implementation platform("org.springframework.ai:spring-ai-mcp-server-spring-boot-starter")
          implementation platform("org.springframework:spring-web")   
        }
        ```
      </Tab>
    </Tabs>

    Then configure your application by setting the applicaiton properties:

    <CodeGroup>
      ```bash application.properties
      spring.main.bannerMode=off
      logging.pattern.console=
      ```

      ```yaml application.yml
      logging:
        pattern:
          console:
      spring:
        main:
          banner-mode: off
      ```
    </CodeGroup>

    The [Server Configuration Properties](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-starter-docs.html#_configuration_properties) documents all available properties.

    Now let's dive into building your server.

    ## Building your server

    ### Weather Service

    Let's implement a [WeatheService.java](https://github.com/spring-projects/spring-ai-examples/blob/main/model-context-protocol/weather/starter-stdio-server/src/main/java/org/springframework/ai/mcp/sample/server/WeatherService.java) that uses a REST client to query the data from the National Weather Service API:

    ```java
    @Service
    public class WeatherService {

    	private final RestClient restClient;

    	public WeatherService() {
    		this.restClient = RestClient.builder()
    			.baseUrl("https://api.weather.gov")
    			.defaultHeader("Accept", "application/geo+json")
    			.defaultHeader("User-Agent", "WeatherApiClient/1.0 (your@email.com)")
    			.build();
    	}

      @Tool(description = "Get weather forecast for a specific latitude/longitude")
      public String getWeatherForecastByLocation(
          double latitude,   // Latitude coordinate
          double longitude   // Longitude coordinate
      ) {
          // Returns detailed forecast including:
          // - Temperature and unit
          // - Wind speed and direction
          // - Detailed forecast description
      }
    	
      @Tool(description = "Get weather alerts for a US state")
      public String getAlerts(
          @ToolParam(description = "Two-letter US state code (e.g. CA, NY") String state)
      ) {
          // Returns active alerts including:
          // - Event type
          // - Affected area
          // - Severity
          // - Description
          // - Safety instructions
      }

      // ......
    }
    ```

    The `@Service` annotation with auto-register the service in your applicaiton context.
    The Spring AI `@Tool` annotation, making it easy to create and maintain MCP tools.

    The auto-configuration will automatically register these tools with the MCP server.

    ### Create your Boot Applicaiton

    ```java
    @SpringBootApplication
    public class McpServerApplication {

    	public static void main(String[] args) {
    		SpringApplication.run(McpServerApplication.class, args);
    	}

    	@Bean
    	public ToolCallbackProvider weatherTools(WeatherService weatherService) {
    		return  MethodToolCallbackProvider.builder().toolObjects(weatherService).build();
    	}
    }
    ```

    Uses the the `MethodToolCallbackProvider` utils to convert the `@Tools` into actionalble callbackes used by the MCP server.

    ### Running the server

    Finally, let's build the server:

    ```bash
    ./mvnw clean install
    ```

    This will generate a `mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar` file within the `target` folder.

    Let's now test your server from an existing MCP host, Claude for Desktop.

    ## Testing your server with Claude for Desktop

    <Note>
      Claude for Desktop is not yet available on Linux.
    </Note>

    First, make sure you have Claude for Desktop installed.
    [You can install the latest version here.](https://claude.ai/download) If you already have Claude for Desktop, **make sure it's updated to the latest version.**

    We'll need to configure Claude for Desktop for whichever MCP servers you want to use.
    To do this, open your Claude for Desktop App configuration at `~/Library/Application Support/Claude/claude_desktop_config.json` in a text editor.
    Make sure to create the file if it doesn't exist.

    For example, if you have [VS Code](https://code.visualstudio.com/) installed:

    <Tabs>
      <Tab title="MacOS/Linux">
        ```bash
        code ~/Library/Application\ Support/Claude/claude_desktop_config.json
        ```
      </Tab>

      <Tab title="Windows">
        ```powershell
        code $env:AppData\Claude\claude_desktop_config.json
        ```
      </Tab>
    </Tabs>

    You'll then add your servers in the `mcpServers` key.
    The MCP UI elements will only show up in Claude for Desktop if at least one server is properly configured.

    In this case, we'll add our single weather server like so:

    <Tabs>
      <Tab title="MacOS/Linux">
        ```json java
        {
          "mcpServers": {
            "spring-ai-mcp-weather": {
              "command": "java",
              "args": [
                "-Dspring.ai.mcp.server.stdio=true",
                "-jar",
                "/ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar"
              ]
            }
          }
        }
        ```
      </Tab>

      <Tab title="Windows">
        ```json java
        {
          "mcpServers": {
            "spring-ai-mcp-weather": {
              "command": "java",
              "args": [
                "-Dspring.ai.mcp.server.transport=STDIO",
                "-jar",
                "C:\\ABSOLUTE\\PATH\\TO\\PARENT\\FOLDER\\weather\\mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar"
              ]
            }
          }
        }
        ```
      </Tab>
    </Tabs>

    <Note>
      Make sure you pass in the absolute path to your server.
    </Note>

    This tells Claude for Desktop:

    1. There's an MCP server named "my-weather-server"
    2. To launch it by running `java -jar /ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar`

    Save the file, and restart **Claude for Desktop**.

    ## Testing your server with Java client

    ### Create a MCP Client manually

    Use the `McpClient` to connect to the server:

    ```java
    var stdioParams = ServerParameters.builder("java")
      .args("-jar", "/ABSOLUTE/PATH/TO/PARENT/FOLDER/mcp-weather-stdio-server-0.0.1-SNAPSHOT.jar")
      .build();

    var stdioTransport = new StdioClientTransport(stdioParams);

    var mcpClient = McpClient.sync(stdioTransport).build();

    mcpClient.initialize();

    ListToolsResult toolsList = mcpClient.listTools();

    CallToolResult weather = mcpClient.callTool(
      new CallToolRequest("getWeatherForecastByLocation",
          Map.of("latitude", "47.6062", "longitude", "-122.3321")));

    CallToolResult alert = mcpClient.callTool(
      new CallToolRequest("getAlerts", Map.of("state", "NY")));

    mcpClient.closeGracefully();
    ```

    ### Use MCP Client Boot Starter

    Create a new boot starter applicaiton using the `spring-ai-mcp-client-spring-boot-starter` dependency:

    ```xml
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-mcp-client-spring-boot-starter</artifactId>
    </dependency>
    ```

    and set the `spring.ai.mcp.client.stdio.servers-configuration` property to point to your `claude_desktop_config.json`.
    You can re-use the existing Anthropic Destop configuration:

    ```properties
    spring.ai.mcp.client.stdio.servers-configuration=file:PATH/TO/claude_desktop_config.json
    ```

    When you stasrt your client applicaiton, the auto-configuration will create, automatically MCP clients from the claude\_desktop\_config.json.

    For more information, see the [MCP Client Boot Starters](https://docs.spring.io/spring-ai/reference/api/mcp/mcp-server-boot-client-docs.html) reference documentation.

    ## More Java MCP Server examples

    The [starter-webflux-server](https://github.com/spring-projects/spring-ai-examples/tree/main/model-context-protocol/weather/starter-webflux-server) demonstrates how to create a MCP server using SSE transport.
    It showcases how to define and register MCP Tools, Resources, and Prompts, using the Spring Boot's auto-configuration capabilities.
  </Tab>
</Tabs>

### Test with commands

Let's make sure Claude for Desktop is picking up the two tools we've exposed in our `weather` server. You can do this by looking for the hammer <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg" style={{display: 'inline', margin: 0, height: '1.3em'}} /> icon:

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/visual-indicator-mcp-tools.png" />
</Frame>

After clicking on the hammer icon, you should see two tools listed:

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/available-mcp-tools.png" />
</Frame>

If your server isn't being picked up by Claude for Desktop, proceed to the [Troubleshooting](#troubleshooting) section for debugging tips.

If the hammer icon has shown up, you can now test your server by running the following commands in Claude for Desktop:

* What's the weather in Sacramento?
* What are the active weather alerts in Texas?

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/current-weather.png" />
</Frame>

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/weather-alerts.png" />
</Frame>

<Note>
  Since this is the US National Weather service, the queries will only work for US locations.
</Note>

## What's happening under the hood

When you ask a question:

1. The client sends your question to Claude
2. Claude analyzes the available tools and decides which one(s) to use
3. The client executes the chosen tool(s) through the MCP server
4. The results are sent back to Claude
5. Claude formulates a natural language response
6. The response is displayed to you!

## Troubleshooting

<AccordionGroup>
  <Accordion title="Claude for Desktop Integration Issues">
    **Getting logs from Claude for Desktop**

    Claude.app logging related to MCP is written to log files in `~/Library/Logs/Claude`:

    * `mcp.log` will contain general logging about MCP connections and connection failures.
    * Files named `mcp-server-SERVERNAME.log` will contain error (stderr) logging from the named server.

    You can run the following command to list recent logs and follow along with any new ones:

    ```bash
    # Check Claude's logs for errors
    tail -n 20 -f ~/Library/Logs/Claude/mcp*.log
    ```

    **Server not showing up in Claude**

    1. Check your `claude_desktop_config.json` file syntax
    2. Make sure the path to your project is absolute and not relative
    3. Restart Claude for Desktop completely

    **Tool calls failing silently**

    If Claude attempts to use the tools but they fail:

    1. Check Claude's logs for errors
    2. Verify your server builds and runs without errors
    3. Try restarting Claude for Desktop

    **None of this is working. What do I do?**

    Please refer to our [debugging guide](/docs/tools/debugging) for better debugging tools and more detailed guidance.
  </Accordion>

  <Accordion title="Weather API Issues">
    **Error: Failed to retrieve grid point data**

    This usually means either:

    1. The coordinates are outside the US
    2. The NWS API is having issues
    3. You're being rate limited

    Fix:

    * Verify you're using US coordinates
    * Add a small delay between requests
    * Check the NWS API status page

    **Error: No active alerts for \[STATE]**

    This isn't an error - it just means there are no current weather alerts for that state. Try a different state or check during severe weather.
  </Accordion>
</AccordionGroup>

<Note>
  For more advanced troubleshooting, check out our guide on [Debugging MCP](/docs/tools/debugging)
</Note>

## Next steps

<CardGroup cols={2}>
  <Card title="Building a client" icon="outlet" href="/quickstart/client">
    Learn how to build your own MCP client that can connect to your server
  </Card>

  <Card title="Example servers" icon="grid" href="/examples">
    Check out our gallery of official MCP servers and implementations
  </Card>

  <Card title="Debugging Guide" icon="bug" href="/docs/tools/debugging">
    Learn how to effectively debug MCP servers and integrations
  </Card>

  <Card title="Building MCP with LLMs" icon="comments" href="/building-mcp-with-llms">
    Learn how to use LLMs like Claude to speed up your MCP development
  </Card>
</CardGroup>


# For Claude Desktop Users
Source: https://modelcontextprotocol.io/quickstart/user

Get started using pre-built servers in Claude for Desktop.

In this tutorial, you will extend [Claude for Desktop](https://claude.ai/download) so that it can read from your computer's file system, write new files, move files, and even search files.

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/quickstart-filesystem.png" />
</Frame>

Don't worry  it will ask you for your permission before executing these actions!

## 1. Download Claude for Desktop

Start by downloading [Claude for Desktop](https://claude.ai/download), choosing either macOS or Windows. (Linux is not yet supported for Claude for Desktop.)

Follow the installation instructions.

If you already have Claude for Desktop, make sure it's on the latest version by clicking on the Claude menu on your computer and selecting "Check for Updates..."

<Accordion title="Why Claude for Desktop and not Claude.ai?">
  Because servers are locally run, MCP currently only supports desktop hosts. Remote hosts are in active development.
</Accordion>

## 2. Add the Filesystem MCP Server

To add this filesystem functionality, we will be installing a pre-built [Filesystem MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem) to Claude for Desktop. This is one of dozens of [servers](https://github.com/modelcontextprotocol/servers/tree/main) created by Anthropic and the community.

Get started by opening up the Claude menu on your computer and select "Settings..." Please note that these are not the Claude Account Settings found in the app window itself.

This is what it should look like on a Mac:

<Frame style={{ textAlign: 'center' }}>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/quickstart-menu.png" width="400" />
</Frame>

Click on "Developer" in the lefthand bar of the Settings pane, and then click on "Edit Config":

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/quickstart-developer.png" />
</Frame>

This will create a configuration file at:

* macOS: `~/Library/Application Support/Claude/claude_desktop_config.json`
* Windows: `%APPDATA%\Claude\claude_desktop_config.json`

if you don't already have one, and will display the file in your file system.

Open up the configuration file in any text editor. Replace the file contents with this:

<Tabs>
  <Tab title="MacOS/Linux">
    ```json
    {
      "mcpServers": {
        "filesystem": {
          "command": "npx",
          "args": [
            "-y",
            "@modelcontextprotocol/server-filesystem",
            "/Users/username/Desktop",
            "/Users/username/Downloads"
          ]
        }
      }
    }
    ```
  </Tab>

  <Tab title="Windows">
    ```json
    {
      "mcpServers": {
        "filesystem": {
          "command": "npx",
          "args": [
            "-y",
            "@modelcontextprotocol/server-filesystem",
            "C:\\Users\\username\\Desktop",
            "C:\\Users\\username\\Downloads"
          ]
        }
      }
    }
    ```
  </Tab>
</Tabs>

Make sure to replace `username` with your computer's username. The paths should point to valid directories that you want Claude to be able to access and modify. It's set up to work for Desktop and Downloads, but you can add more paths as well.

You will also need [Node.js](https://nodejs.org) on your computer for this to run properly. To verify you have Node installed, open the command line on your computer.

* On macOS, open the Terminal from your Applications folder
* On Windows, press Windows + R, type "cmd", and press Enter

Once in the command line, verify you have Node installed by entering in the following command:

```bash
node --version
```

If you get an error saying "command not found" or "node is not recognized", download Node from [nodejs.org](https://nodejs.org/).

<Tip>
  **How does the configuration file work?**

  This configuration file tells Claude for Desktop which MCP servers to start up every time you start the application. In this case, we have added one server called "filesystem" that will use the Node `npx` command to install and run `@modelcontextprotocol/server-filesystem`. This server, described [here](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem), will let you access your file system in Claude for Desktop.
</Tip>

<Warning>
  **Command Privileges**

  Claude for Desktop will run the commands in the configuration file with the permissions of your user account, and access to your local files. Only add commands if you understand and trust the source.
</Warning>

## 3. Restart Claude

After updating your configuration file, you need to restart Claude for Desktop.

Upon restarting, you should see a hammer <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/claude-desktop-mcp-hammer-icon.svg" style={{display: 'inline', margin: 0, height: '1.3em'}} /> icon in the bottom right corner of the input box:

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/quickstart-hammer.png" />
</Frame>

After clicking on the hammer icon, you should see the tools that come with the Filesystem MCP Server:

<Frame style={{ textAlign: 'center' }}>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/quickstart-tools.png" width="400" />
</Frame>

If your server isn't being picked up by Claude for Desktop, proceed to the [Troubleshooting](#troubleshooting) section for debugging tips.

## 4. Try it out!

You can now talk to Claude and ask it about your filesystem. It should know when to call the relevant tools.

Things you might try asking Claude:

* Can you write a poem and save it to my desktop?
* What are some work-related files in my downloads folder?
* Can you take all the images on my desktop and move them to a new folder called "Images"?

As needed, Claude will call the relevant tools and seek your approval before taking an action:

<Frame style={{ textAlign: 'center' }}>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/quickstart-approve.png" width="500" />
</Frame>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Server not showing up in Claude / hammer icon missing">
    1. Restart Claude for Desktop completely
    2. Check your `claude_desktop_config.json` file syntax
    3. Make sure the file paths included in `claude_desktop_config.json` are valid and that they are absolute and not relative
    4. Look at [logs](#getting-logs-from-claude-for-desktop) to see why the server is not connecting
    5. In your command line, try manually running the server (replacing `username` as you did in `claude_desktop_config.json`) to see if you get any errors:

    <Tabs>
      <Tab title="MacOS/Linux">
        ```bash
        npx -y @modelcontextprotocol/server-filesystem /Users/username/Desktop /Users/username/Downloads
        ```
      </Tab>

      <Tab title="Windows">
        ```bash
        npx -y @modelcontextprotocol/server-filesystem C:\Users\username\Desktop C:\Users\username\Downloads
        ```
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="Getting logs from Claude for Desktop">
    Claude.app logging related to MCP is written to log files in:

    * macOS: `~/Library/Logs/Claude`

    * Windows: `%APPDATA%\Claude\logs`

    * `mcp.log` will contain general logging about MCP connections and connection failures.

    * Files named `mcp-server-SERVERNAME.log` will contain error (stderr) logging from the named server.

    You can run the following command to list recent logs and follow along with any new ones (on Windows, it will only show recent logs):

    <Tabs>
      <Tab title="MacOS/Linux">
        ```bash
        # Check Claude's logs for errors
        tail -n 20 -f ~/Library/Logs/Claude/mcp*.log
        ```
      </Tab>

      <Tab title="Windows">
        ```bash
        type "%APPDATA%\Claude\logs\mcp*.log"
        ```
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="Tool calls failing silently">
    If Claude attempts to use the tools but they fail:

    1. Check Claude's logs for errors
    2. Verify your server builds and runs without errors
    3. Try restarting Claude for Desktop
  </Accordion>

  <Accordion title="None of this is working. What do I do?">
    Please refer to our [debugging guide](/docs/tools/debugging) for better debugging tools and more detailed guidance.
  </Accordion>

  <Accordion title="ENOENT error and `${APPDATA}` in paths on Windows">
    If your configured server fails to load, and you see within its logs an error referring to `${APPDATA}` within a path, you may need to add the expanded value of `%APPDATA%` to your `env` key in `claude_desktop_config.json`:

    ```json
    {
      "brave-search": {
        "command": "npx",
        "args": ["-y", "@modelcontextprotocol/server-brave-search"],
        "env": {
          "APPDATA": "C:\\Users\\user\\AppData\\Roaming\\",
          "BRAVE_API_KEY": "..."
        }
      }
    }
    ```

    With this change in place, launch Claude Desktop once again.

    <Warning>
      **NPM should be installed globally**

      The `npx` command may continue to fail if you have not installed NPM globally. If NPM is already installed globally, you will find `%APPDATA%\npm` exists on your system. If not, you can install NPM globally by running the following command:

      ```bash
      npm install -g npm
      ```
    </Warning>
  </Accordion>
</AccordionGroup>

## Next steps

<CardGroup cols={2}>
  <Card title="Explore other servers" icon="grid" href="/examples">
    Check out our gallery of official MCP servers and implementations
  </Card>

  <Card title="Build your own server" icon="code" href="/quickstart/server">
    Now build your own custom server to use in Claude for Desktop and other clients
  </Card>
</CardGroup>


# MCP Client
Source: https://modelcontextprotocol.io/sdk/java/mcp-client

Learn how to use the Model Context Protocol (MCP) client to interact with MCP servers

# Model Context Protocol Client

The MCP Client is a key component in the Model Context Protocol (MCP) architecture, responsible for establishing and managing connections with MCP servers. It implements the client-side of the protocol, handling:

* Protocol version negotiation to ensure compatibility with servers
* Capability negotiation to determine available features
* Message transport and JSON-RPC communication
* Tool discovery and execution
* Resource access and management
* Prompt system interactions
* Optional features like roots management and sampling support

The client provides both synchronous and asynchronous APIs for flexibility in different application contexts.

<Tabs>
  <Tab title="Sync API">
    ```java
    // Create a sync client with custom configuration
    McpSyncClient client = McpClient.sync(transport)
        .requestTimeout(Duration.ofSeconds(10))
        .capabilities(ClientCapabilities.builder()
            .roots(true)      // Enable roots capability
            .sampling()       // Enable sampling capability
            .build())
        .sampling(request -> new CreateMessageResult(response))
        .build();

    // Initialize connection
    client.initialize();

    // List available tools
    ListToolsResult tools = client.listTools();

    // Call a tool
    CallToolResult result = client.callTool(
        new CallToolRequest("calculator", 
            Map.of("operation", "add", "a", 2, "b", 3))
    );

    // List and read resources
    ListResourcesResult resources = client.listResources();
    ReadResourceResult resource = client.readResource(
        new ReadResourceRequest("resource://uri")
    );

    // List and use prompts
    ListPromptsResult prompts = client.listPrompts();
    GetPromptResult prompt = client.getPrompt(
        new GetPromptRequest("greeting", Map.of("name", "Spring"))
    );

    // Add/remove roots
    client.addRoot(new Root("file:///path", "description"));
    client.removeRoot("file:///path");

    // Close client
    client.closeGracefully();
    ```
  </Tab>

  <Tab title="Async API">
    ```java
    // Create an async client with custom configuration
    McpAsyncClient client = McpClient.async(transport)
        .requestTimeout(Duration.ofSeconds(10))
        .capabilities(ClientCapabilities.builder()
            .roots(true)      // Enable roots capability
            .sampling()       // Enable sampling capability
            .build())
        .sampling(request -> Mono.just(new CreateMessageResult(response)))
        .toolsChangeConsumer(tools -> Mono.fromRunnable(() -> {
            logger.info("Tools updated: {}", tools);
        }))
        .resourcesChangeConsumer(resources -> Mono.fromRunnable(() -> {
            logger.info("Resources updated: {}", resources);
        }))
        .promptsChangeConsumer(prompts -> Mono.fromRunnable(() -> {
            logger.info("Prompts updated: {}", prompts);
        }))
        .build();

    // Initialize connection and use features
    client.initialize()
        .flatMap(initResult -> client.listTools())
        .flatMap(tools -> {
            return client.callTool(new CallToolRequest(
                "calculator", 
                Map.of("operation", "add", "a", 2, "b", 3)
            ));
        })
        .flatMap(result -> {
            return client.listResources()
                .flatMap(resources -> 
                    client.readResource(new ReadResourceRequest("resource://uri"))
                );
        })
        .flatMap(resource -> {
            return client.listPrompts()
                .flatMap(prompts ->
                    client.getPrompt(new GetPromptRequest(
                        "greeting", 
                        Map.of("name", "Spring")
                    ))
                );
        })
        .flatMap(prompt -> {
            return client.addRoot(new Root("file:///path", "description"))
                .then(client.removeRoot("file:///path"));            
        })
        .doFinally(signalType -> {
            client.closeGracefully().subscribe();
        })
        .subscribe();
    ```
  </Tab>
</Tabs>

## Client Transport

The transport layer handles the communication between MCP clients and servers, providing different implementations for various use cases. The client transport manages message serialization, connection establishment, and protocol-specific communication patterns.

<Tabs>
  <Tab title="STDIO">
    Creates transport for in-process based communication

    ```java
    ServerParameters params = ServerParameters.builder("npx")
        .args("-y", "@modelcontextprotocol/server-everything", "dir")
        .build();
    McpTransport transport = new StdioClientTransport(params);
    ```
  </Tab>

  <Tab title="SSE (HttpClient)">
    Creates a framework agnostic (pure Java API) SSE client transport. Included in the core mcp module.

    ```java
    McpTransport transport = new HttpClientSseClientTransport("http://your-mcp-server");
    ```
  </Tab>

  <Tab title="SSE (WebFlux)">
    Creates WebFlux-based SSE client transport. Requires the mcp-webflux-sse-transport dependency.

    ```java
    WebClient.Builder webClientBuilder = WebClient.builder()
        .baseUrl("http://your-mcp-server");
    McpTransport transport = new WebFluxSseClientTransport(webClientBuilder);
    ```
  </Tab>
</Tabs>

## Client Capabilities

The client can be configured with various capabilities:

```java
var capabilities = ClientCapabilities.builder()
    .roots(true)      // Enable filesystem roots support with list changes notifications
    .sampling()       // Enable LLM sampling support
    .build();
```

### Roots Support

Roots define the boundaries of where servers can operate within the filesystem:

```java
// Add a root dynamically
client.addRoot(new Root("file:///path", "description"));

// Remove a root
client.removeRoot("file:///path");

// Notify server of roots changes
client.rootsListChangedNotification();
```

The roots capability allows servers to:

* Request the list of accessible filesystem roots
* Receive notifications when the roots list changes
* Understand which directories and files they have access to

### Sampling Support

Sampling enables servers to request LLM interactions ("completions" or "generations") through the client:

```java
// Configure sampling handler
Function<CreateMessageRequest, CreateMessageResult> samplingHandler = request -> {
    // Sampling implementation that interfaces with LLM
    return new CreateMessageResult(response);
};

// Create client with sampling support
var client = McpClient.sync(transport)
    .capabilities(ClientCapabilities.builder()
        .sampling()
        .build())
    .sampling(samplingHandler)
    .build();
```

This capability allows:

* Servers to leverage AI capabilities without requiring API keys
* Clients to maintain control over model access and permissions
* Support for both text and image-based interactions
* Optional inclusion of MCP server context in prompts

## Using MCP Clients

### Tool Execution

Tools are server-side functions that clients can discover and execute. The MCP client provides methods to list available tools and execute them with specific parameters. Each tool has a unique name and accepts a map of parameters.

<Tabs>
  <Tab title="Sync API">
    ```java
    // List available tools and their names
    var tools = client.listTools();
    tools.forEach(tool -> System.out.println(tool.getName()));

    // Execute a tool with parameters
    var result = client.callTool("calculator", Map.of(
        "operation", "add",
        "a", 1,
        "b", 2
    ));
    ```
  </Tab>

  <Tab title="Async API">
    ```java
    // List available tools asynchronously
    client.listTools()
        .doOnNext(tools -> tools.forEach(tool -> 
            System.out.println(tool.getName())))
        .subscribe();

    // Execute a tool asynchronously
    client.callTool("calculator", Map.of(
            "operation", "add",
            "a", 1,
            "b", 2
        ))
        .subscribe();
    ```
  </Tab>
</Tabs>

### Resource Access

Resources represent server-side data sources that clients can access using URI templates. The MCP client provides methods to discover available resources and retrieve their contents through a standardized interface.

<Tabs>
  <Tab title="Sync API">
    ```java
    // List available resources and their names
    var resources = client.listResources();
    resources.forEach(resource -> System.out.println(resource.getName()));

    // Retrieve resource content using a URI template
    var content = client.getResource("file", Map.of(
        "path", "/path/to/file.txt"
    ));
    ```
  </Tab>

  <Tab title="Async API">
    ```java
    // List available resources asynchronously
    client.listResources()
        .doOnNext(resources -> resources.forEach(resource -> 
            System.out.println(resource.getName())))
        .subscribe();

    // Retrieve resource content asynchronously
    client.getResource("file", Map.of(
            "path", "/path/to/file.txt"
        ))
        .subscribe();
    ```
  </Tab>
</Tabs>

### Prompt System

The prompt system enables interaction with server-side prompt templates. These templates can be discovered and executed with custom parameters, allowing for dynamic text generation based on predefined patterns.

<Tabs>
  <Tab title="Sync API">
    ```java
    // List available prompt templates
    var prompts = client.listPrompts();
    prompts.forEach(prompt -> System.out.println(prompt.getName()));

    // Execute a prompt template with parameters
    var response = client.executePrompt("echo", Map.of(
        "text", "Hello, World!"
    ));
    ```
  </Tab>

  <Tab title="Async API">
    ```java
    // List available prompt templates asynchronously
    client.listPrompts()
        .doOnNext(prompts -> prompts.forEach(prompt -> 
            System.out.println(prompt.getName())))
        .subscribe();

    // Execute a prompt template asynchronously
    client.executePrompt("echo", Map.of(
            "text", "Hello, World!"
        ))
        .subscribe();
    ```
  </Tab>
</Tabs>


# Overview
Source: https://modelcontextprotocol.io/sdk/java/mcp-overview

Introduction to the Model Context Protocol (MCP) Java SDK

Java SDK for the [Model Context Protocol](https://modelcontextprotocol.org/docs/concepts/architecture)
enables standardized integration between AI models and tools.

## Features

* MCP Client and MCP Server implementations supporting:
  * Protocol [version compatibility negotiation](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/lifecycle/#initialization)
  * [Tool](https://spec.modelcontextprotocol.io/specification/2024-11-05/server/tools/) discovery, execution, list change notifications
  * [Resource](https://spec.modelcontextprotocol.io/specification/2024-11-05/server/resources/) management with URI templates
  * [Roots](https://spec.modelcontextprotocol.io/specification/2024-11-05/client/roots/) list management and notifications
  * [Prompt](https://spec.modelcontextprotocol.io/specification/2024-11-05/server/prompts/) handling and management
  * [Sampling](https://spec.modelcontextprotocol.io/specification/2024-11-05/client/sampling/) support for AI model interactions
* Multiple transport implementations:
  * Default transports:
    * Stdio-based transport for process-based communication
    * Java HttpClient-based SSE client transport for HTTP SSE Client-side streaming
    * Servlet-based SSE server transport for HTTP SSE Server streaming
  * Spring-based transports:
    * WebFlux SSE client and server transports for reactive HTTP streaming
    * WebMVC SSE transport for servlet-based HTTP streaming
* Supports Synchronous and Asynchronous programming paradigms

## Architecture

The SDK follows a layered architecture with clear separation of concerns:

![MCP Stack Architecture](https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/java/mcp-stack.svg)

* **Client/Server Layer (McpClient/McpServer)**: Both use McpSession for sync/async operations,
  with McpClient handling client-side protocol operations and McpServer managing server-side protocol operations.
* **Session Layer (McpSession)**: Manages communication patterns and state using DefaultMcpSession implementation.
* **Transport Layer (McpTransport)**: Handles JSON-RPC message serialization/deserialization via:
  * StdioTransport (stdin/stdout) in the core module
  * HTTP SSE transports in dedicated transport modules (Java HttpClient, Spring WebFlux, Spring WebMVC)

The MCP Client is a key component in the Model Context Protocol (MCP) architecture, responsible for establishing and managing connections with MCP servers.
It implements the client-side of the protocol.

![Java MCP Client Architecture](https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/java/java-mcp-client-architecture.jpg)

The MCP Server is a foundational component in the Model Context Protocol (MCP) architecture that provides tools, resources, and capabilities to clients.
It implements the server-side of the protocol.

![Java MCP Server Architecture](https://mintlify.s3.us-west-1.amazonaws.com/mcp/images/java/java-mcp-server-architecture.jpg)

Key Interactions:

* **Client/Server Initialization**: Transport setup, protocol compatibility check, capability negotiation, and implementation details exchange.
* **Message Flow**: JSON-RPC message handling with validation, type-safe response processing, and error handling.
* **Resource Management**: Resource discovery, URI template-based access, subscription system, and content retrieval.

## Dependencies

Add the following Maven dependency to your project:

<Tabs>
  <Tab title="Maven">
    The core MCP functionality:

    ```xml
    <dependency>
        <groupId>io.modelcontextprotocol.sdk</groupId>
        <artifactId>mcp</artifactId>
    </dependency>
    ```

    For HTTP SSE transport implementations, add one of the following dependencies:

    ```xml
    <!-- Spring WebFlux-based SSE client and server transport -->
    <dependency>
        <groupId>io.modelcontextprotocol.sdk</groupId>
        <artifactId>mcp-spring-webflux</artifactId>
    </dependency>

    <!-- Spring WebMVC-based SSE server transport -->
    <dependency>
        <groupId>io.modelcontextprotocol.sdk</groupId>
        <artifactId>mcp-spring-webmvc</artifactId>
    </dependency>
    ```
  </Tab>

  <Tab title="Gradle">
    The core MCP functionality:

    ```groovy
    dependencies {
      implementation platform("io.modelcontextprotocol.sdk:mcp")
      //...
    }
    ```

    For HTTP SSE transport implementations, add one of the following dependencies:

    ```groovy
    // Spring WebFlux-based SSE client and server transport
    dependencies {
      implementation platform("io.modelcontextprotocol.sdk:mcp-spring-webflux")
    }

    // Spring WebMVC-based SSE server transport
    dependencies {
      implementation platform("io.modelcontextprotocol.sdk:mcp-spring-webmvc")
    }
    ```
  </Tab>
</Tabs>

### Bill of Materials (BOM)

The Bill of Materials (BOM) declares the recommended versions of all the dependencies used by a given release.
Using the BOM from your application's build script avoids the need for you to specify and maintain the dependency versions yourself.
Instead, the version of the BOM you're using determines the utilized dependency versions.
It also ensures that you're using supported and tested versions of the dependencies by default, unless you choose to override them.

Add the BOM to your project:

<Tabs>
  <Tab title="Maven">
    ```xml
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>io.modelcontextprotocol.sdk</groupId>
                <artifactId>mcp-bom</artifactId>
                <version>0.7.0</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>
    ```
  </Tab>

  <Tab title="Gradle">
    ```groovy
    dependencies {
      implementation platform("io.modelcontextprotocol.sdk:mcp-bom:0.7.0")
      //...
    }
    ```

    Gradle users can also use the Spring AI MCP BOM by leveraging Gradle (5.0+) native support for declaring dependency constraints using a Maven BOM.
    This is implemented by adding a 'platform' dependency handler method to the dependencies section of your Gradle build script.
    As shown in the snippet above this can then be followed by version-less declarations of the Starter Dependencies for the one or more spring-ai modules you wish to use, e.g. spring-ai-openai.
  </Tab>
</Tabs>

Replace the version number with the version of the BOM you want to use.

### Available Dependencies

The following dependencies are available and managed by the BOM:

* Core Dependencies
  * `io.modelcontextprotocol.sdk:mcp` - Core MCP library providing the base functionality and APIs for Model Context Protocol implementation.
* Transport Dependencies
  * `io.modelcontextprotocol.sdk:mcp-spring-webflux` - WebFlux-based Server-Sent Events (SSE) transport implementation for reactive applications.
  * `io.modelcontextprotocol.sdk:mcp-spring-webmvc` - WebMVC-based Server-Sent Events (SSE) transport implementation for servlet-based applications.
* Testing Dependencies
  * `io.modelcontextprotocol.sdk:mcp-test` - Testing utilities and support for MCP-based applications.


# MCP Server
Source: https://modelcontextprotocol.io/sdk/java/mcp-server

Learn how to implement and configure a Model Context Protocol (MCP) server

## Overview

The MCP Server is a foundational component in the Model Context Protocol (MCP) architecture that provides tools, resources, and capabilities to clients. It implements the server-side of the protocol, responsible for:

* Exposing tools that clients can discover and execute
* Managing resources with URI-based access patterns
* Providing prompt templates and handling prompt requests
* Supporting capability negotiation with clients
* Implementing server-side protocol operations
* Managing concurrent client connections
* Providing structured logging and notifications

The server supports both synchronous and asynchronous APIs, allowing for flexible integration in different application contexts.

<Tabs>
  <Tab title="Sync API">
    ```java
    // Create a server with custom configuration
    McpSyncServer syncServer = McpServer.sync(transport)
        .serverInfo("my-server", "1.0.0")
        .capabilities(ServerCapabilities.builder()
            .resources(true)     // Enable resource support
            .tools(true)         // Enable tool support
            .prompts(true)       // Enable prompt support
            .logging()           // Enable logging support
            .build())
        .build();

    // Register tools, resources, and prompts
    syncServer.addTool(syncToolRegistration);
    syncServer.addResource(syncResourceRegistration);
    syncServer.addPrompt(syncPromptRegistration);

    // Send logging notifications
    syncServer.loggingNotification(LoggingMessageNotification.builder()
        .level(LoggingLevel.INFO)
        .logger("custom-logger")
        .data("Server initialized")
        .build());

    // Close the server when done
    syncServer.close();
    ```
  </Tab>

  <Tab title="Async API">
    ```java
    // Create an async server with custom configuration
    McpAsyncServer asyncServer = McpServer.async(transport)
        .serverInfo("my-server", "1.0.0")
        .capabilities(ServerCapabilities.builder()
            .resources(true)     // Enable resource support
            .tools(true)         // Enable tool support
            .prompts(true)       // Enable prompt support
            .logging()           // Enable logging support
            .build())
        .build();

    // Register tools, resources, and prompts
    asyncServer.addTool(asyncToolRegistration)
        .doOnSuccess(v -> logger.info("Tool registered"))
        .subscribe();

    asyncServer.addResource(asyncResourceRegistration)
        .doOnSuccess(v -> logger.info("Resource registered"))
        .subscribe();

    asyncServer.addPrompt(asyncPromptRegistration)
        .doOnSuccess(v -> logger.info("Prompt registered"))
        .subscribe();

    // Send logging notifications
    asyncServer.loggingNotification(LoggingMessageNotification.builder()
        .level(LoggingLevel.INFO)
        .logger("custom-logger")
        .data("Server initialized")
        .build());

    // Close the server when done
    asyncServer.close()
        .doOnSuccess(v -> logger.info("Server closed"))
        .subscribe();
    ```
  </Tab>
</Tabs>

## Server Transport

The transport layer in the MCP SDK is responsible for handling the communication between clients and servers. It provides different implementations to support various communication protocols and patterns. The SDK includes several built-in transport implementations:

<Tabs>
  <Tab title="STDIO">
    <>
      Create in-process based transport:

      ```java
      StdioServerTransport transport = new StdioServerTransport(new ObjectMapper());
      ```

      Provides bidirectional JSON-RPC message handling over standard input/output streams with non-blocking message processing, serialization/deserialization, and graceful shutdown support.

      Key features:

      <ul>
        <li>Bidirectional communication through stdin/stdout</li>
        <li>Process-based integration support</li>
        <li>Simple setup and configuration</li>
        <li>Lightweight implementation</li>
      </ul>
    </>
  </Tab>

  <Tab title="SSE (WebFlux)">
    <>
      <p>Creates WebFlux-based SSE server transport.<br />Requires the <code>mcp-spring-webflux</code> dependency.</p>

      ```java
      @Configuration
      class McpConfig {
          @Bean
          WebFluxSseServerTransport webFluxSseServerTransport(ObjectMapper mapper) {
              return new WebFluxSseServerTransport(mapper, "/mcp/message");
          }

          @Bean
          RouterFunction<?> mcpRouterFunction(WebFluxSseServerTransport transport) {
              return transport.getRouterFunction();
          }
      }
      ```

      <p>Implements the MCP HTTP with SSE transport specification, providing:</p>

      <ul>
        <li>Reactive HTTP streaming with WebFlux</li>
        <li>Concurrent client connections through SSE endpoints</li>
        <li>Message routing and session management</li>
        <li>Graceful shutdown capabilities</li>
      </ul>
    </>
  </Tab>

  <Tab title="SSE (WebMvc)">
    <>
      <p>Creates WebMvc-based SSE server transport.<br />Requires the <code>mcp-spring-webmvc</code> dependency.</p>

      ```java
      @Configuration
      @EnableWebMvc
      class McpConfig {
          @Bean
          WebMvcSseServerTransport webMvcSseServerTransport(ObjectMapper mapper) {
              return new WebMvcSseServerTransport(mapper, "/mcp/message");
          }

          @Bean
          RouterFunction<ServerResponse> mcpRouterFunction(WebMvcSseServerTransport transport) {
              return transport.getRouterFunction();
          }
      }
      ```

      <p>Implements the MCP HTTP with SSE transport specification, providing:</p>

      <ul>
        <li>Server-side event streaming</li>
        <li>Integration with Spring WebMVC</li>
        <li>Support for traditional web applications</li>
        <li>Synchronous operation handling</li>
      </ul>
    </>
  </Tab>

  <Tab title="SSE (Servlet)">
    <>
      <p>
        Creates a Servlet-based SSE server transport. It is included in the core <code>mcp</code> module.<br />
        The <code>HttpServletSseServerTransport</code> can be used with any Servlet container.<br />
        To use it with a Spring Web application, you can register it as a Servlet bean:
      </p>

      ```java
      @Configuration
      @EnableWebMvc
      public class McpServerConfig implements WebMvcConfigurer {

          @Bean
          public HttpServletSseServerTransport servletSseServerTransport() {
              return new HttpServletSseServerTransport(new ObjectMapper(), "/mcp/message");
          }

          @Bean
          public ServletRegistrationBean customServletBean(HttpServletSseServerTransport servlet) {
              return new ServletRegistrationBean(servlet);
          }
      }
      ```

      <p>
        Implements the MCP HTTP with SSE transport specification using the traditional Servlet API, providing:
      </p>

      <ul>
        <li>Asynchronous message handling using Servlet 6.0 async support</li>
        <li>Session management for multiple client connections</li>

        <li>
          Two types of endpoints:

          <ul>
            <li>SSE endpoint (<code>/sse</code>) for server-to-client events</li>
            <li>Message endpoint (configurable) for client-to-server requests</li>
          </ul>
        </li>

        <li>Error handling and response formatting</li>
        <li>Graceful shutdown support</li>
      </ul>
    </>
  </Tab>
</Tabs>

## Server Capabilities

The server can be configured with various capabilities:

```java
var capabilities = ServerCapabilities.builder()
    .resources(false, true)  // Resource support with list changes notifications
    .tools(true)            // Tool support with list changes notifications
    .prompts(true)          // Prompt support with list changes notifications
    .logging()              // Enable logging support (enabled by default with loging level INFO)
    .build();
```

### Logging Support

The server provides structured logging capabilities that allow sending log messages to clients with different severity levels:

```java
// Send a log message to clients
server.loggingNotification(LoggingMessageNotification.builder()
    .level(LoggingLevel.INFO)
    .logger("custom-logger")
    .data("Custom log message")
    .build());
```

Clients can control the minimum logging level they receive through the `mcpClient.setLoggingLevel(level)` request. Messages below the set level will be filtered out.
Supported logging levels (in order of increasing severity): DEBUG (0), INFO (1), NOTICE (2), WARNING (3), ERROR (4), CRITICAL (5), ALERT (6), EMERGENCY (7)

### Tool Registration

<Tabs>
  <Tab title="Sync">
    ```java
    // Sync tool registration
    var syncToolRegistration = new McpServerFeatures.SyncToolRegistration(
        new Tool("calculator", "Basic calculator", Map.of(
            "operation", "string",
            "a", "number",
            "b", "number"
        )),
        arguments -> {
            // Tool implementation
            return new CallToolResult(result, false);
        }
    );
    ```
  </Tab>

  <Tab title="Async">
    ```java
    // Async tool registration
    var asyncToolRegistration = new McpServerFeatures.AsyncToolRegistration(
        new Tool("calculator", "Basic calculator", Map.of(
            "operation", "string",
            "a", "number",
            "b", "number"
        )),
        arguments -> {
            // Tool implementation
            return Mono.just(new CallToolResult(result, false));
        }
    );
    ```
  </Tab>
</Tabs>

### Resource Registration

<Tabs>
  <Tab title="Sync">
    ```java
    // Sync resource registration
    var syncResourceRegistration = new McpServerFeatures.SyncResourceRegistration(
        new Resource("custom://resource", "name", "description", "mime-type", null),
        request -> {
            // Resource read implementation
            return new ReadResourceResult(contents);
        }
    );
    ```
  </Tab>

  <Tab title="Async">
    ```java
    // Async resource registration
    var asyncResourceRegistration = new McpServerFeatures.AsyncResourceRegistration(
        new Resource("custom://resource", "name", "description", "mime-type", null),
        request -> {
            // Resource read implementation
            return Mono.just(new ReadResourceResult(contents));
        }
    );
    ```
  </Tab>
</Tabs>

### Prompt Registration

<Tabs>
  <Tab title="Sync">
    ```java
    // Sync prompt registration
    var syncPromptRegistration = new McpServerFeatures.SyncPromptRegistration(
        new Prompt("greeting", "description", List.of(
            new PromptArgument("name", "description", true)
        )),
        request -> {
            // Prompt implementation
            return new GetPromptResult(description, messages);
        }
    );
    ```
  </Tab>

  <Tab title="Async">
    ```java
    // Async prompt registration
    var asyncPromptRegistration = new McpServerFeatures.AsyncPromptRegistration(
        new Prompt("greeting", "description", List.of(
            new PromptArgument("name", "description", true)
        )),
        request -> {
            // Prompt implementation
            return Mono.just(new GetPromptResult(description, messages));
        }
    );
    ```
  </Tab>
</Tabs>

## Error Handling

The SDK provides comprehensive error handling through the McpError class, covering protocol compatibility, transport communication, JSON-RPC messaging, tool execution, resource management, prompt handling, timeouts, and connection issues. This unified error handling approach ensures consistent and reliable error management across both synchronous and asynchronous operations.


# Building MCP with LLMs
Source: https://modelcontextprotocol.io/tutorials/building-mcp-with-llms

Speed up your MCP development using LLMs such as Claude!

This guide will help you use LLMs to help you build custom Model Context Protocol (MCP) servers and clients. We'll be focusing on Claude for this tutorial, but you can do this with any frontier LLM.

## Preparing the documentation

Before starting, gather the necessary documentation to help Claude understand MCP:

1.  Visit [https://modelcontextprotocol.io/llms-full.txt](https://modelcontextprotocol.io/llms-full.txt) and copy the full documentation text
2.  Navigate to either the [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk) or [Python SDK repository](https://github.com/modelcontextprotocol/python-sdk)
3.  Copy the README files and other relevant documentation
4.  Paste these documents into your conversation with Claude

## Describing your server

Once you've provided the documentation, clearly describe to Claude what kind of server you want to build. Be specific about:

*   What resources your server will expose
*   What tools it will provide
*   Any prompts it should offer
*   What external systems it needs to interact with

For example:

```
Build an MCP server that:
- Connects to my company's PostgreSQL database
- Exposes table schemas as resources
- Provides tools for running read-only SQL queries
- Includes prompts for common data analysis tasks
```

## Working with Claude

When working with Claude on MCP servers:

1.  Start with the core functionality first, then iterate to add more features
2.  Ask Claude to explain any parts of the code you don't understand
3.  Request modifications or improvements as needed
4.  Have Claude help you test the server and handle edge cases

Claude can help implement all the key MCP features:

*   Resource management and exposure
*   Tool definitions and implementations
*   Prompt templates and handlers
*   Error handling and logging
*   Connection and transport setup

## Best practices

When building MCP servers with Claude:

*   Break down complex servers into smaller pieces
*   Test each component thoroughly before moving on
*   Keep security in mind - validate inputs and limit access appropriately
*   Document your code well for future maintenance
*   Follow MCP protocol specifications carefully

## Next steps

After Claude helps you build your server:

1.  Review the generated code carefully
2.  Test the server with the MCP Inspector tool
3.  Connect it to Claude.app or other MCP clients
4.  Iterate based on real usage and feedback

Remember that Claude can help you modify and improve your server as requirements change over time.

Need more guidance? Just ask Claude specific questions about implementing MCP features or troubleshooting issues that arise.

================
File: evai/docs/openai_quickstart.md
================
Developer quickstart
====================

Learn how to make your first API request.

The OpenAI API provides a simple interface to state-of-the-art AI [models](/docs/models) for natural language processing, image generation, semantic search, and speech recognition. Follow this guide to learn how to generate human-like responses to [natural language prompts](/docs/guides/text-generation), [create vector embeddings](/docs/guides/embeddings) for semantic search, and [generate images](/docs/guides/images) from textual descriptions.

Create and export an API key
----------------------------

[Create an API key in the dashboard here](/api-keys), which youll use to securely [access the API](/docs/api-reference/authentication). Store the key in a safe location, like a [`.zshrc` file](https://www.freecodecamp.org/news/how-do-zsh-configuration-files-work/) or another text file on your computer. Once youve generated an API key, export it as an [environment variable](https://en.wikipedia.org/wiki/Environment_variable) in your terminal.

macOS / Linux

Export an environment variable on macOS or Linux systems

```bash
export OPENAI_API_KEY="your_api_key_here"
```

Windows

Export an environment variable in PowerShell

```bash
setx OPENAI_API_KEY "your_api_key_here"
```

Make your first API request
---------------------------

With your OpenAI API key exported as an environment variable, you're ready to make your first API request. You can either use the [REST API](/docs/api-reference) directly with the HTTP client of your choice, or use one of our [official SDKs](/docs/libraries) as shown below.

JavaScript

To use the OpenAI API in server-side JavaScript environments like Node.js, Deno, or Bun, you can use the official [OpenAI SDK for TypeScript and JavaScript](https://github.com/openai/openai-node). Get started by installing the SDK using [npm](https://www.npmjs.com/) or your preferred package manager:

Install the OpenAI SDK with npm

```bash
npm install openai
```

With the OpenAI SDK installed, create a file called `example.mjs` and copy one of the following examples into it:

Generate text

Create a human-like response to a prompt

```javascript
import OpenAI from "openai";
const openai = new OpenAI();

const completion = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
        { role: "system", content: "You are a helpful assistant." },
        {
            role: "user",
            content: "Write a haiku about recursion in programming.",
        },
    ],
    store: true,
});

console.log(completion.choices[0].message);
```

Generate an image

Generate an image based on a textual prompt

```javascript
import OpenAI from "openai";
const openai = new OpenAI();

const image = await openai.images.generate({ prompt: "A cute baby sea otter" });

console.log(image.data[0].url);
```

Create vector embeddings

Create vector embeddings for a string of text

```javascript
import OpenAI from "openai";
const openai = new OpenAI();

const embedding = await openai.embeddings.create({
    model: "text-embedding-3-large",
    input: "The quick brown fox jumped over the lazy dog",
});

console.log(embedding);
```

Execute the code with `node example.mjs` (or the equivalent command for Deno or Bun). In a few moments, you should see the output of your API request!

Python

To use the OpenAI API in Python, you can use the official [OpenAI SDK for Python](https://github.com/openai/openai-python). Get started by installing the SDK using [pip](https://pypi.org/project/pip/):

Install the OpenAI SDK with pip

```bash
pip install openai
```

With the OpenAI SDK installed, create a file called `example.py` and copy one of the following examples into it:

Generate text

Create a human-like response to a prompt

```python
from openai import OpenAI
client = OpenAI()

completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Write a haiku about recursion in programming."
        }
    ]
)

print(completion.choices[0].message)
```

Generate an image

Generate an image based on a textual prompt

```python
from openai import OpenAI
client = OpenAI()

response = client.images.generate(
    prompt="A cute baby sea otter",
    n=2,
    size="1024x1024"
)

print(response.data[0].url)
```

Create vector embeddings

Create vector embeddings for a string of text

```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    model="text-embedding-3-large",
    input="The food was delicious and the waiter..."
)

print(response)
```

Execute the code with `python example.py`. In a few moments, you should see the output of your API request!

curl

On Unix-based systems, you can test out the [OpenAI REST API](/docs/api-reference) using [curl](https://curl.se/). The following commands assume that you have exported the `OPENAI_API_KEY` system environment variable as shown above.

Generate text

Create a human-like response to a prompt

```bash
curl "https://api.openai.com/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "model": "gpt-4o-mini",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "Write a haiku that explains the concept of recursion."
            }
        ]
    }'
```

Generate an image

Generate an image based on a textual prompt

```bash
curl "https://api.openai.com/v1/images/generations" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "prompt": "A cute baby sea otter",
        "n": 2,
        "size": "1024x1024"
    }'
```

Create vector embeddings

Create vector embeddings for a string of text

```bash
curl "https://api.openai.com/v1/embeddings" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY" \
    -d '{
        "input": "The food was delicious and the waiter...",
        "model": "text-embedding-3-large"
    }'
```

Execute the curl commands above in your terminal. In a few moments, you should see the output of your API request!

Next steps
----------

Now that you've made your first OpenAI API request, you can explore the following resources:

[

Chat Completions

Learn more about generating text responses to natural language prompts

](/docs/guides/text-generation)[

Image Generation

Generate images using our DALLE model

](/docs/guides/images)[

Embeddings

Create vector representations of text, used for similarity search

](/docs/guides/embeddings)[

Text-to-speech

Generate human-like voice recordings with our text-to-speech model

](/docs/guides/text-to-speech)[

Speech-to-text

Create transcriptions of voice recordings with our Whisper model

](/docs/guides/speech-to-text)[

Moderation

Analyze and filter user-created content with our moderation model

](/docs/guides/moderation)[

Fine-tuning

Fine-tune our models with your own data

](/docs/guides/fine-tuning)[

Batch

Batch requests for async jobs

](/docs/guides/batch)[

Full API Reference

View the full REST API reference for OpenAI

](/docs/api-reference)

================
File: evai/docs/prompt_plan.md
================
Below is a comprehensive plan that evolves in multiple iterations, starting from a high-level blueprint, then breaking the tasks into more detailed, smaller steps until each part can be implemented safely and cleanly. Finally, you will find a collection of prompts that you can feed into a code-generation LLM (e.g., ChatGPT, Claude, etc.) to implement each step in a test-driven manner, ensuring no large leaps in complexity.

1. High-Level Blueprint
	1.	Initialize Core Project Structure
		Create a Python package or directory (evai/) for the CLI.
		Set up a minimal setup.py or a pyproject.toml if needed.
		Create a basic CLI entry point (evai) using argparse or click.
	2.	Implement Command File Management
		Define a directory structure under ~/.evai/commands/<command-name>/.
		Specify the required YAML file (command.yaml) and Python file (command.py).
		Implement a utility for reading, writing, and validating command metadata from YAML.
	3.	Add Command Creation Workflow
		Implement evai command add <command-name> to:
	1.	Generate default YAML metadata.
	2.	Prompt user to review and edit the metadata in $EDITOR.
	3.	Prompt user to implement the Python code in $EDITOR.
	4.	Perform post-editing validation (YAML schema check, flake8 lint check).
	5.	Handle re-editing if any validation fails.
	4.	MCP Integration
		Implement a minimal MCP server that picks up all commands from ~/.evai/commands/*/command.yaml.
		Expose each command at an endpoint, e.g., /commands/<command-name>.
	5.	LLM Interaction
		(Optional step, but included for completeness.)
		Include a submodule for LLM calls (e.g., Anthropic or OpenAI).
		Hook LLM calls when the commands llm_interaction metadata is enabled.
	6.	Testing, Linting, and Validation
		Add unit tests for YAML metadata reading/writing.
		Add integration tests ensuring commands are recognized by evai and the MCP server.
		Validate the user editing/linting cycle with flake8 and a basic YAML schema check.
	7.	Deployment / Packaging
		Confirm everything runs with a local install.
		(Optionally) Release to PyPI or a local environment.

2. Breaking the Blueprint into Iterative Chunks

Below is a more detailed breakdown of the same blueprint, with each chunk building on the previous chunk.
	1.	Chunk 1: Project Scaffolding
		Set up basic Python project.
		Create minimal CLI entry point using argparse (or your chosen CLI library).
		Verify evai can run and display a help message.
	2.	Chunk 2: Command Directory & YAML Management
		Implement a helper function to locate ~/.evai/commands/<command-name>/.
		Implement reading/writing YAML metadata (using PyYAML or equivalent).
		Create unit tests to verify metadata read/write.
	3.	Chunk 3: command add Workflow (Initial Version)
		Implement evai command add <command-name> that:
		Creates the directory structure.
		Generates a default command.yaml.
		Generates a stub command.py.
		Writes minimal content.
		Add basic tests.
	4.	Chunk 4: Interactive Editing
		Add $EDITOR invocation to let the user edit the YAML.
		Validate YAML structure after edit.
		If validation fails, keep re-invoking $EDITOR.
	5.	Chunk 5: Implementation Editing & Lint Checking
		Prompt user to open command.py in $EDITOR.
		Validate Python code via flake8.
		If linting fails, re-prompt user.
	6.	Chunk 6: Command Execution & MCP Exposure
		Make the evai CLI able to list commands.
		Add a minimal embedded MCP server.
		Expose each command at a route for remote invocation.
		Write tests verifying command execution from CLI and from MCP.
	7.	Chunk 7: Optional LLM Interaction
		Integrate an LLM client to propose default metadata.
		If llm_interaction.enabled is true, call the LLM from within the command logic.
		Add tests for the LLM interaction path.
	8.	Chunk 8: Polishing & Final Testing
		Write final integration tests and ensure everything is consistent.
		Confirm TDD with all tests passing.

3. Breaking Chunks into Smaller Steps

Below is another iteration, splitting each chunk into smaller, incremental steps that are feasible to implement safely, with strong testing at each stage.
	1.	Chunk 1: Project Scaffolding
	1.	Create evai/ folder and basic Python package structure.
	2.	Add __init__.py.
	3.	Add cli.py using argparse or click.
	4.	Implement a placeholder main() that prints a version or help.
	5.	Add a simple test (test_cli.py) that checks for the printed help message.
	6.	Confirm the test passes.
	2.	Chunk 2: Command Directory & YAML Management
	1.	Implement a function: get_command_dir(command_name) to build ~/.evai/commands/<command-name>.
	2.	Implement load_command_metadata(path) and save_command_metadata(path, data).
	3.	Use PyYAML to parse and write metadata.
	4.	Write tests (test_metadata.py) for each function to ensure correct directory resolution and file read/write.
	3.	Chunk 3: command add Workflow (Initial Version)
	1.	Add subcommand: evai command add <command-name>.
	2.	Within that, create the command directory.
	3.	Generate default YAML metadata (in memory).
	4.	Generate a stub Python implementation file.
	5.	Save the files to disk.
	6.	Test this by verifying the directory and files exist.
	4.	Chunk 4: Interactive Editing
	1.	After stub files are created, call $EDITOR to open command.yaml.
	2.	On save, parse the updated YAML.
	3.	If invalid, print an error and reopen $EDITOR.
	4.	Write tests to simulate successful and failing edits (possibly mocking $EDITOR calls).
	5.	Chunk 5: Implementation Editing & Lint Checking
	1.	Prompt user to open command.py in $EDITOR.
	2.	Run flake8 programmatically on command.py.
	3.	If lint errors exist, prompt the user to re-edit.
	4.	Test with a good Python file and a file with a known lint error.
	6.	Chunk 6: Command Execution & MCP Exposure
	1.	In the CLI, add evai command list to show available commands.
	2.	Add evai command run <command-name> --param1=... --param2=... to execute the command directly.
	3.	Implement a minimal MCP server that runs in a background thread or in the main thread, listing available commands.
	4.	Test it by calling the CLI subcommands, verifying output.
	7.	Chunk 7: Optional LLM Interaction
	1.	Implement a function generate_default_metadata_with_llm(command_name) that queries an LLM.
	2.	Use the response to populate default command metadata.
	3.	Add tests that mock the LLM client, ensuring fallback if the LLM is unreachable.
	8.	Chunk 8: Polishing & Final Testing
	1.	Add final integration tests across all subcommands and flows.
	2.	Ensure TDD coverage is near 100%.
	3.	Validate final usage from a typical user perspective (UAT).

4. Prompts for a Code-Generation LLM (Test-Driven Implementation)

Below is a set of sequential prompts you can feed into a code-generation LLM (such as ChatGPT, Claude, etc.). Each prompt is self-contained but references the outputs from prior steps. They are designed to walk through implementing the entire project in small, test-driven increments. You would copy each prompt (as a single code block) into your LLM, let it generate the code or confirm it, then proceed to the next prompt. The final prompt wires everything together, ensuring no hanging or orphaned code.

Prompt 1: Project Scaffolding

You are implementing an EVAI CLI in Python. In this step, create a minimal project scaffold:

- Create a folder structure: `evai/` with an `__init__.py`.
- Add a `cli.py` that uses `argparse` and has a `main()` function. 
- When `python -m evai.cli` is run, it should print a version or help text. 
- Create a test file `tests/test_cli.py` that checks the CLI prints some expected text (like "EVAI CLI version 0.1").

Please:
1. Write the code for `evai/__init__.py`.
2. Write the code for `evai/cli.py`.
3. Write the `tests/test_cli.py`.
4. Include instructions (a shell command or two) on how to run these tests with `pytest`.
Use best practices, including a `if __name__ == "__main__": main()` guard in `cli.py`.

Prompt 2: Command Directory & YAML Management

Building on the previous code, add functionality to manage a command repository under the users home directory:

1. Implement a function `get_command_dir(command_name)` in a new file `evai/command_storage.py` that returns the path: `~/.evai/commands/<command_name>`. It should create the directory if it doesnt exist.
2. Implement `load_command_metadata(path) -> dict` and `save_command_metadata(path, data: dict) -> None` in the same file. Use PyYAML to parse/write YAML to a file named `command.yaml` in that directory.
3. Update your test suite in a new file `tests/test_metadata.py` to test these three functions.
4. Provide a final updated tree structure of the project, including new files, and the commands to run tests.

Make sure tests pass and we follow best practices for Python code.

Prompt 3: command add <command-name> Workflow (Initial Version)

Extend the CLI with a subcommand: `evai command add <command-name>`.

1. In `cli.py`, add a subcommand group `command` with a subcommand `add`.
2. When called, it should:
   - Use `get_command_dir` to find/create the directory.
   - Create a default Python file `command.py` with a simple `def run(**kwargs): print("Hello World")`.
   - Create a default YAML file `command.yaml` with fields:
     ```
     name: <command-name>
     description: "Default description"
     params: []
     hidden: false
     disabled: false
     mcp_integration:
       enabled: true
       metadata:
         endpoint: ""
         method: "POST"
         authentication_required: false
     llm_interaction:
       enabled: false
       auto_apply: true
       max_llm_turns: 15
     ```
3. Write tests in `tests/test_add_command.py` verifying that after running `evai command add <command-name>`, the directory is created with the above files containing the correct content.

Provide the updated code changes and the test code.

Prompt 4: Interactive Editing of command.yaml

Add interactive editing support for `command.yaml`:

1. After creating the default metadata, invoke an editor for the user to review/edit. Respect the environment variable `$EDITOR`, or default to `vi` if `$EDITOR` is not set.
2. When the user saves and exits, parse the YAML again. If invalid, re-open the editor until the user fixes it or chooses to abort.
3. In `tests/test_add_command.py`, add a test that mocks the editor call to simulate user changes. For the real editor invocation, you can use a subprocess call.

Provide updated code. Include any new helper functions and test code. Ensure the user can exit gracefully if they want to abort.

Prompt 5: Implementation Editing & Lint Checking

Now add a similar interactive editing step for `command.py`. Then perform a lint check:

1. Once `command.yaml` is finalized, open `command.py` in the editor.
2. After the user saves, run `flake8` (or a Python wrapper around it). 
3. If errors are found, display them and re-open the editor. 
4. Allow user to abort if they cant fix the issues.
5. Create `tests/test_edit_implementation.py` that checks if a file with a known lint error triggers a re-edit.
6. Provide updated code for everything needed, including how you handle the lint check programmatically in Python.

Prompt 6: Command Execution & MCP Exposure

Next, enable listing and executing commands, and expose them via an MCP server:

1. Add `evai command list` to scan `~/.evai/commands` and print available command names.
2. Add `evai command run <command-name>` with optional `--param key=value` pairs. This should:
   - Load `command.py` dynamically (e.g., via importlib).
   - Call the `run(**kwargs)`.
3. Implement a minimal MCP server that reads all `command.yaml` files, and for each command with `mcp_integration.enabled=True`, expose a route like `/commands/<command-name>`. 
   - The route handles a POST request with JSON data. 
   - Translate that data into keyword args for `run()`.
4. Add tests: `test_list_and_run.py` for the CLI behaviors, `test_mcp_exposure.py` for verifying the MCP routes respond correctly. 
5. Provide updated code and instructions on how to run the MCP server for local testing.

Prompt 7: Optional LLM Interaction

Implement optional LLM interaction for default metadata generation and in-command usage:

1. Add a function `generate_default_metadata_with_llm(command_name)` that makes a mock or real LLM call, returning YAML metadata. If the call fails, use standard defaults.
2. Modify `evai command add <command-name>` so that it offers to call `generate_default_metadata_with_llm` for initial metadata if `llm_interaction.enabled` is set.
3. In command execution, if `llm_interaction.enabled=True`, optionally call an LLM inside `command.py` if the user so desires. This is not strictly necessary but can be tested with a mock LLM.
4. Add or update tests in `test_llm.py`, ensuring that if the LLM is unreachable, we gracefully revert to basic defaults.

Return the updated code, focusing on minimal or mock LLM calls, plus the new/updated tests.

Prompt 8: Final Polishing & Integration Tests

Time to finalize and polish:

1. Add final integration tests that create a command, edit it, lint it, execute it, run it from MCP, etc.
2. Ensure best practices: handle exceptions gracefully, provide clear error messages.
3. Confirm TDD with all tests passing.
4. Provide instructions for running the entire test suite, manually verifying the workflow, and any final details needed to consider this project complete.

Return the final integrated code. 

These prompts are designed to build the EVAI CLI Custom Commands Integration in small, test-driven increments, ensuring each step is validated before moving on.

================
File: evai/docs/todo.md
================
# TODO: Implementation Steps for EVAI CLI Custom Commands Integration

A comprehensive, step-by-step checklist to guide the development of the EVAI CLI, command storage, and MCP integration. Mark each step as completed once done.

---

## 1. Project Scaffolding
- [X] **Create Project Structure**  
  - [X] Create `__init__.py` inside `evai/`.
  - [X] Create `cli.py` with a basic `main()` function.
- [X] **Command-Line Entry Point**  
  - [X] Decide on `argparse`, `click`, or similar.
  - [X] Implement minimal CLI that prints help/version info.
- [X] **Basic Testing**  
  - [X] Create `tests/` folder with a simple test (`test_cli.py`).
  - [X] Verify that invoking CLI with `python -m evai.cli` works.

---

## 2. Command Directory & YAML Management
- [X] **Utility Functions**  
  - [X] `get_command_dir(command_name)`: Return (and create if needed) `~/.evai/commands/<command-name>/`.
  - [X] `load_command_metadata(path) -> dict`: Load YAML from `command.yaml`.
  - [X] `save_command_metadata(path, data: dict) -> None`: Write YAML data to `command.yaml`.
- [X] **Testing**  
  - [X] Write `test_metadata.py` to cover file I/O, YAML parse/write, and directory creation.
  - [X] Confirm that all tests pass before proceeding.

---

## 3. `command add` Workflow (Initial Version)
- [X] **CLI Subcommand**  
  - [X] Implement `evai command add <command-name>` in `cli.py`.
- [X] **File Creation**  
  - [X] Within this subcommand, create default `command.yaml` with standard fields and placeholders.
  - [X] Create default `command.py` with a stub `def run(**kwargs): print("Hello World")`.
- [X] **Testing**  
  - [X] In `test_add_command.py`, confirm the directory and files are created with correct content.

---

## 4. Editing of `command.yaml`
- [X] **Non-interactive editing**
- [X] **Environment Variable `$EDITOR`**  
  - [X] Check `$EDITOR`; default to `vi` if not set.
- [X] **Editing Loop**
  - [X] Open the newly created `command.yaml` in the editor.
  - [X] Parse YAML on save.  
  - [X] If invalid, prompt user to fix it. Offer to abort if needed.
- [X] **Testing**  
  - [X] Mock the editor call in `test_add_command.py` or a new test file to simulate user edits.
  - [X] Validate re-editing behavior on invalid YAML.

---

## 5. Implementation Editing & Lint Checking
- [X] **Editor Invocation**  
  - [X] Open `command.py` in `$EDITOR`.
- [X] **Lint Check**  
  - [X] Run `flake8` on `command.py` programmatically.
  - [X] If lint errors, show them to the user and re-open editor.
  - [X] Offer user the option to abort if unresolved.
- [X] **Testing**  
  - [X] Create `test_edit_implementation.py` to ensure a known lint error triggers re-edit.
  - [X] Confirm passing code requires no further edits.

---

## 6. Command Execution & MCP Exposure
- [X] **List & Run**  
  - [X] `evai command list`: Scan `~/.evai/commands` and list command names.
  - [X] `evai command run <command-name>`: Dynamically import `command.py` and call `run(**kwargs)`.
- [X] **MCP Server**  
  - [X] Create a minimal server that:
    - [X] Scans `command.yaml` files.
    - [X] For each command with `mcp_integration.enabled = true`, exposes `/commands/<command-name>` (POST).
    - [X] Executes corresponding `run()` with JSON data as kwargs.
- [X] **Testing**  
  - [X] `test_list_and_run.py`: CLI tests for listing and running commands.
  - [X] `test_mcp_exposure.py`: Confirm commands are reachable via MCP server, verifying correct endpoints and data flow.

---

## 7. LLM Interaction - Use OpenAI API
- [X] Create new CLI command `evai command llmadd <command-name>`
- [X] Query user for command description
- [X] Call LLM with description and name to see if additional information is needed, and if so, display the response to the user and allow them to provide additional details.
- [X] Build templates for metadata and implementation
- [X] **LLM for Default Metadata**  
  - [X] `generate_default_metadata_with_llm(command_name)`: Populate YAML metadata from an LLM.
  - [X] Fallback to basic defaults if LLM is unreachable or disabled.
- [X] **LLM for Command Implementation**  
  - [X] If `llm_interaction.enabled` is `true`, integrate a call to the LLM inside `command.py` logic as needed.
- [X] **Testing**  
  - [X] `test_llm.py`: Mock the LLM.  
  - [X] Ensure graceful fallback on LLM failures.

---

## 8. Final Polishing & Integration Tests
- [ ] **Integration Testing**  
  - [ ] Create tests that simulate full workflow: adding a command, editing files, lint checking, executing via CLI, executing via MCP.
  - [ ] Ensure robust error handling for each step.
- [ ] **User Acceptance**  
  - [ ] Confirm typical user can navigate the CLI steps easily.
  - [ ] Ensure error messaging is clear and user-friendly.
- [ ] **Final Validation**  
  - [ ] Run entire test suite.  
  - [ ] Verify all features are consistent with requirements.  
  - [ ] Document usage for future maintainers.

---

## 9. Release or Local Deployment
- [ ] **Installation**  
  - [ ] Confirm local install with `pip install .` or equivalent works.
- [ ] **Packaging**  
  - [ ] (Optional) Publish to PyPI if desired, or maintain locally.
- [ ] **Documentation**  
  - [ ] Provide a README with usage, environment variables, and known limitations.

---

**End of Checklist**

================
File: evai/docs/tool_positional_args_todo.md
================
# Tool Positional Arguments Implementation

## Task Description
Modify the `run_tool` function to handle command-line style parameters for tool functions. The current implementation expects a dictionary of keyword arguments, but we need to adapt it to work with positional arguments from the command line.

For example, running `evai tools run subtract 8 5` should run the `tool_subtract` method in the subtract tool with `8` as the first parameter and `5` as the second parameter.

## Implementation Plan
[X] Modify the `run_tool` function in `tool_storage.py` to accept positional arguments
[X] Update the `run` command in `tools.py` to handle positional arguments
[X] Create tests to verify the changes
[X] Run the tests to ensure everything works correctly

## Changes Made

### 1. Modified `run_tool` function in `tool_storage.py`
- Added support for positional arguments using `*args`
- Added type conversion based on function signature
- Maintained backward compatibility with keyword arguments

### 2. Updated `run` command in `tools.py`
- Added support for positional arguments using Click's `nargs=-1` parameter
- Maintained backward compatibility with `--param` option
- Updated the help text to explain both usage patterns

### 3. Created tests
- Created unit tests for the `run_tool` function
- Created integration tests for the CLI

## Testing
All tests are passing, which verifies that:
- Positional arguments work correctly
- Keyword arguments still work (backward compatibility)
- Mixed arguments raise an appropriate error

## Example Usage
```bash
# Using positional arguments
evai tools run subtract 8 5
# Result: 3.0

# Using keyword arguments (backward compatibility)
evai tools run subtract --param minuend=8 --param subtrahend=5
# Result: 3.0
```

================
File: evai/mcp/mcp_prompts.py
================
"""MCP prompts for EVAI CLI."""

import logging
import os
from typing import Dict, Any, List

try:
    from mcp.server.fastmcp import FastMCP
    import mcp.types as types
    from mcp.types import PromptMessage
except ImportError:
    # Provide a helpful error message if MCP is not installed
    raise ImportError(
        "The MCP Python SDK is required for MCP server integration. "
        "Please install it with: pip install mcp"
    )

# Set up logging
logger = logging.getLogger(__name__)

# Define available prompts
PROMPTS = {
    "git-commit": types.Prompt(
        name="git-commit",
        description="Generate a Git commit message",
        arguments=[
            types.PromptArgument(
                name="changes",
                description="Git diff or description of changes",
                required=True
            )
        ],
    ),
    "explain-code": types.Prompt(
        name="explain-code",
        description="Explain how code works",
        arguments=[
            types.PromptArgument(
                name="code",
                description="Code to explain",
                required=True
            ),
            types.PromptArgument(
                name="language",
                description="Programming language",
                required=False
            )
        ],
    )
}


def register_prompts(mcp: FastMCP, server) -> None:
    """
    Register all available prompts.
    
    Args:
        mcp: The MCP server instance
        server: The EVAIServer instance for file reading
    """
    logger.debug("Registering prompts")
    
    # Register the analyze-file prompt
    @mcp.prompt(name="analyze-file", description="Analyze a file")
    async def analyze_file(path: str) -> list[PromptMessage]:
        """
        Analyze a file and provide insights.
        
        Args:
            path: Path to the file to analyze
            
        Returns:
            A list of prompt messages
        """
        logger.debug(f"Analyzing file: {path}")
        try:
            # Read the file
            content = server.read_file(path)
            
            # Return the file content as a prompt message
            return [PromptMessage(role="user", content=f"Please analyze this file:\n\n```\n{content}\n```")]
        except Exception as e:
            logger.error(f"Error analyzing file: {e}")
            return [PromptMessage(role="user", content=f"Error analyzing file: {e}")]
    
    logger.debug("Prompts registered successfully")

================
File: evai/mcp/mcp_server.py
================
"""MCP server integration for EVAI CLI."""

import os
import sys
import logging
import json
import subprocess
import importlib.util
from typing import Dict, Any, List, Optional, Tuple

try:
    from mcp.server.fastmcp import FastMCP, Context
    import mcp.types as types
    from mcp.types import PromptMessage
except ImportError:
    # Provide a helpful error message if MCP is not installed
    raise ImportError(
        "The MCP Python SDK is required for MCP server integration. "
        "Please install it with: pip install mcp"
    )

# Add the parent directory to sys.path
# sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from evai.tool_storage import (
    list_tools, 
    run_tool, 
    load_tool_metadata, 
    get_tool_dir,
    save_tool_metadata,
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    import_tool_module
)

# Import the new modules
from evai.mcp.mcp_prompts import register_prompts
from evai.mcp.mcp_tools import register_built_in_tools, register_tools, register_tool

# Set up logging
logger = logging.getLogger(__name__)
mcp = FastMCP("evai")


class EVAIServer:
    """MCP server for EVAI CLI custom tools."""
    
    def __init__(self, mcp: FastMCP):
        """
        Initialize the MCP server.
        
        Args:
            name: The name of the server
        """
        print(f"[DEBUG] Entering EVAIServer.__init__ with name=evai", file=sys.stderr)
        self.name = "evai"
        self.mcp = mcp
        self.tools = {}
        
        # Use the new modules for registration
        register_built_in_tools(self.mcp)
        register_prompts(self.mcp, self)
        register_tools(self.mcp)
        
        print(f"[DEBUG] Exiting EVAIServer.__init__", file=sys.stderr)
    
    def read_file(self, path: str) -> str:
        """Read a file and return its contents."""
        with open(path, "r") as f:
            return f.read()
    
    def run(self) -> None:
        """Run the MCP server."""
        print(f"[DEBUG] Entering EVAIServer.run", file=sys.stderr)
        try:
            # Start the server
            self.mcp.run()
        except KeyboardInterrupt:
            print("Server stopped by user.")
        except Exception as e:
            logger.error(f"Error running MCP server: {e}")
            print(f"Error running MCP server: {e}", file=sys.stderr)
        print(f"[DEBUG] Exiting EVAIServer.run", file=sys.stderr)


def create_server(name: str = "EVAI Tools") -> EVAIServer:
    """
    Create an MCP server for EVAI CLI custom tools.
    
    Args:
        name: The name of the server
        
    Returns:
        The MCP server
    """
    print(f"[DEBUG] Entering create_server with name={name}", file=sys.stderr)
    server = EVAIServer(mcp)
    print(f"[DEBUG] Exiting create_server", file=sys.stderr)
    return server


def run_server(name: str = "EVAI Tools") -> None:
    """
    Run an MCP server for EVAI CLI custom tools.
    
    Args:
        name: The name of the server
    """
    print(f"[DEBUG] Entering run_server with name={name}", file=sys.stderr)
    server = create_server(name)
    server.run()
    print(f"[DEBUG] Exiting run_server", file=sys.stderr)

server = EVAIServer(mcp)

================
File: evai/mcp/mcp_tools.py
================
"""MCP tools for EVAI CLI."""

import os
import sys
import logging
import inspect
from typing import Dict, Any, List, Optional

try:
    from mcp.server.fastmcp import FastMCP
except ImportError:
    # Provide a helpful error message if MCP is not installed
    raise ImportError(
        "The MCP Python SDK is required for MCP server integration. "
        "Please install it with: pip install mcp"
    )

from evai.tool_storage import (
    list_tools, 
    run_tool, 
    load_tool_metadata, 
    get_tool_dir,
    save_tool_metadata,
    edit_tool_metadata,
    edit_tool_implementation,
    run_lint_check,
    import_tool_module
)

# Set up logging
logger = logging.getLogger(__name__)


def register_built_in_tools(mcp: FastMCP) -> None:
    """
    Register built-in tools like tool creation.
    
    Args:
        mcp: The MCP server instance
    """
    logger.debug("Registering built-in tools")
    
    @mcp.tool(name="list_tools")
    def list_available_tools() -> Dict[str, Any]:
        """
        List all available tools.
        
        Returns:
            A dictionary with the list of available tools
        """
        logger.debug("Listing available tools")
        try:
            tools_list = list_tools()
            logger.debug(f"Found {len(tools_list)} tools")
            return {
                "status": "success",
                "tools": tools_list
            }
        except Exception as e:
            logger.error(f"Error listing tools: {e}")
            return {"status": "error", "message": str(e)}
    
    @mcp.tool(name="edit_tool_implementation")
    def edit_tool_implementation_tool(tool_name: str, implementation: str) -> Dict[str, Any]:
        """
        Edit the implementation of an existing tool.
        
        Args:
            tool_name: The name of the tool to edit
            implementation: The new implementation code
            
        Returns:
            A dictionary with the status of the edit
        """
        logger.debug(f"Editing implementation for tool: {tool_name}")
        try:
            # Get the tool directory
            tool_dir = get_tool_dir(tool_name)
            
            # Check if tool exists
            tool_py_path = os.path.join(tool_dir, "tool.py")
            if not os.path.exists(tool_py_path):
                logger.error(f"Tool '{tool_name}' does not exist")
                return {"status": "error", "message": f"Tool '{tool_name}' does not exist"}
            
            # Write the new implementation
            with open(tool_py_path, "w") as f:
                f.write(implementation)
            
            # Check if the tool is already registered
            try:
                # Re-import the module to update the implementation
                from importlib import reload
                import sys
                
                # Get the module name
                module_name = f"evai.tools.{tool_name}"
                
                # If the module is already loaded, reload it
                if module_name in sys.modules:
                    reload(sys.modules[module_name])
                    
                logger.info(f"Reloaded implementation for tool '{tool_name}'")
            except Exception as e:
                logger.warning(f"Failed to reload implementation for tool '{tool_name}': {e}")
            
            result = {
                "status": "success",
                "message": f"Implementation for tool '{tool_name}' updated successfully",
                "implementation_path": tool_py_path
            }
            logger.debug(f"Successfully edited implementation for tool: {tool_name}")
            return result
            
        except Exception as e:
            logger.error(f"Error editing tool implementation: {e}")
            return {"status": "error", "message": str(e)}
            
    @mcp.tool(name="edit_tool_metadata")
    def edit_tool_metadata_tool(tool_name: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """
        Edit the metadata of an existing tool.
        
        Args:
            tool_name: The name of the tool to edit
            metadata: The new metadata
            
        Returns:
            A dictionary with the status of the edit
        """
        logger.debug(f"Editing metadata for tool: {tool_name}")
        try:
            # Get the tool directory
            tool_dir = get_tool_dir(tool_name)
            
            # Check if tool exists
            tool_yaml_path = os.path.join(tool_dir, "tool.yaml")
            if not os.path.exists(tool_yaml_path):
                logger.error(f"Tool '{tool_name}' does not exist")
                return {"status": "error", "message": f"Tool '{tool_name}' does not exist"}
            
            # Ensure the name field matches the tool_name
            metadata["name"] = tool_name
            
            # Save the metadata
            save_tool_metadata(tool_dir, metadata)
            
            result = {
                "status": "success",
                "message": f"Metadata for tool '{tool_name}' updated successfully",
                "metadata_path": tool_yaml_path
            }
            logger.debug(f"Successfully edited metadata for tool: {tool_name}")
            return result
            
        except Exception as e:
            logger.error(f"Error editing tool metadata: {e}")
            return {"status": "error", "message": str(e)}
    
    logger.debug("Built-in tools registered successfully")


def register_tools(mcp: FastMCP) -> None:
    """
    Register all available tools.
    
    Args:
        mcp: The MCP server instance
    """
    logger.debug("Registering custom tools")
    
    try:
        # Get all available tools
        tools = list_tools()
        
        # Register each tool
        for tool in tools:
            tool_name = tool["name"]
            tool_dir = tool["path"]
            
            try:
                # Load the tool metadata
                metadata = load_tool_metadata(tool_dir)
                
                # Register the tool
                register_tool(mcp, tool_name, metadata)
                
            except Exception as e:
                logger.error(f"Error registering tool '{tool_name}': {e}")
        
        logger.debug(f"Registered {len(tools)} custom tools")
        
    except Exception as e:
        logger.error(f"Error registering tools: {e}")
    

def register_tool(mcp: FastMCP, tool_name: str, metadata: Dict[str, Any]) -> None:
    """
    Register a tool as an MCP tool.
    
    Args:
        mcp: The MCP server instance
        tool_name: The name of the tool
        metadata: The tool metadata
    """
    logger.debug(f"Registering tool: {tool_name}")
    
    try:
        # Import the tool module using the existing function
        module = import_tool_module(tool_name)
        
        # Find any function that starts with 'tool_'
        tool_functions = [
            name for name, obj in inspect.getmembers(module)
            if inspect.isfunction(obj) and name.startswith('tool_')
        ]
        
        if not tool_functions:
            raise AttributeError(f"Tool module doesn't have any tool_* functions")
        
        # Use the first tool function found
        tool_function_name = tool_functions[0]
        logger.debug(f"Found tool function: {tool_function_name}")
        
        # Get the tool function and register it with MCP
        tool_function = getattr(module, tool_function_name)
        mcp.tool(name=tool_name)(tool_function)
        
        logger.debug(f"Successfully registered tool: {tool_name}")
    except Exception as e:
        logger.error(f"Error registering tool '{tool_name}': {e}")
        raise

================
File: evai/templates/sample_command.py
================
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}

================
File: evai/templates/sample_command.yaml
================
name: "{command_name}"
description: "Default description"
arguments: []
options: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: "POST"
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15

================
File: evai/templates/sample_tool.py
================
"""Custom tool implementation."""

def tool_echo(echo_string: str) -> str:
    """Run the echo tool with the given arguments."""
    
    return echo_string

================
File: evai/templates/sample_tool.yaml
================
name: {tool_name}
description: Default description
params: []
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15

================
File: evai/__init__.py
================
"""EVAI CLI - Command-line interface for EVAI."""

__version__ = "0.1.0"

================
File: evai/command_storage.py
================
"""Command storage utilities for EVAI CLI."""

import os
import yaml
from pathlib import Path
import importlib.util
import inspect
import logging

logger = logging.getLogger(__name__)

COMMANDS_DIR = Path.home() / ".evai" / "commands"

def get_command_dir(command_name: str) -> Path:
    """Get the directory path for a command and create it if it doesn't exist."""
    command_dir = COMMANDS_DIR / command_name
    command_dir.mkdir(parents=True, exist_ok=True)
    return command_dir

def load_command_metadata(path: Path) -> dict:
    """Load command metadata from command.yaml."""
    yaml_path = path / "command.yaml"
    if not yaml_path.exists():
        raise FileNotFoundError(f"Command metadata file not found: {yaml_path}")
    with yaml_path.open("r") as f:
        return yaml.safe_load(f) or {}

def save_command_metadata(path: Path, data: dict) -> None:
    """Save command metadata to command.yaml."""
    yaml_path = path / "command.yaml"
    with yaml_path.open("w") as f:
        yaml.dump(data, f, default_flow_style=False)

def list_commands() -> list[dict]:
    """List all available commands."""
    if not COMMANDS_DIR.exists():
        return []
    commands = []
    for cmd_dir in COMMANDS_DIR.iterdir():
        if cmd_dir.is_dir():
            try:
                metadata = load_command_metadata(cmd_dir)
                if not metadata.get("disabled", False):
                    commands.append({
                        "name": metadata.get("name", cmd_dir.name),
                        "description": metadata.get("description", "No description"),
                        "path": cmd_dir
                    })
            except Exception as e:
                logger.warning(f"Error loading command {cmd_dir.name}: {e}")
    return commands

def import_command_module(command_name: str):
    """Dynamically import a command module."""
    cmd_dir = get_command_dir(command_name)
    py_path = cmd_dir / "command.py"
    if not py_path.exists():
        raise FileNotFoundError(f"Command implementation file not found: {py_path}")
    spec = importlib.util.spec_from_file_location(f"evai.commands.{command_name}", py_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

def run_command(command_name: str, *args, **kwargs):
    """Run a command with the given arguments."""
    try:
        # Import the command module
        module = import_command_module(command_name)
        
        # Check if the module has a run function
        if not hasattr(module, "run"):
            raise AttributeError(f"Command '{command_name}' does not have a run function")
        
        run_func = getattr(module, "run")
        
        # Get the function signature
        sig = inspect.signature(run_func)
        
        # Check if we're using args or kwargs
        if args and len(args) > 0:
            # Convert positional args to kwargs based on metadata
            cmd_dir = get_command_dir(command_name)
            metadata = load_command_metadata(cmd_dir)
            arg_names = [arg["name"] for arg in metadata.get("arguments", [])]
            
            # Map positional args to named args
            if len(args) > len(arg_names):
                raise ValueError(f"Too many arguments provided. Expected: {len(arg_names)}, Got: {len(args)}")
                
            # Create kwargs from positional args
            for i, arg in enumerate(args):
                if i < len(arg_names):
                    kwargs[arg_names[i]] = arg
            
            # Run with kwargs
            return run_func(**kwargs)
        else:
            # Run with kwargs
            return run_func(**kwargs)
    except Exception as e:
        logger.error(f"Error running command: {e}")
        raise

def remove_command(command_name: str) -> None:
    """Remove a command directory and its files."""
    import shutil
    cmd_dir = get_command_dir(command_name)
    if not cmd_dir.exists():
        raise FileNotFoundError(f"Command '{command_name}' not found")
    shutil.rmtree(cmd_dir)

================
File: evai/llm_client.py
================
"""LLM client for EVAI CLI."""

import os
import logging
import json
import yaml
from typing import Dict, Any, Optional, Tuple, List
import time

# Set up logging
logger = logging.getLogger(__name__)

# Default metadata template
DEFAULT_METADATA = {
    "name": "",
    "description": "",
    "params": [],
    "hidden": False,
    "disabled": False,
    "mcp_integration": {
        "enabled": True,
        "metadata": {
            "endpoint": "",
            "method": "POST",
            "authentication_required": False
        }
    },
    "llm_interaction": {
        "enabled": False,
        "auto_apply": True,
        "max_llm_turns": 15
    }
}

# Default implementation template
DEFAULT_IMPLEMENTATION = '''"""Custom command implementation."""


def run(**kwargs):
    """Run the command with the given arguments."""
    print("Hello World")
    return {"status": "success"}
'''

class LLMClientError(Exception):
    """Exception raised for errors in the LLM client."""
    pass


def get_openai_client():
    """
    Get an OpenAI client instance.
    
    Returns:
        OpenAI client instance
        
    Raises:
        LLMClientError: If the OpenAI package is not installed or API key is not set
    """
    try:
        from openai import OpenAI
    except ImportError:
        logger.error("OpenAI package not installed. Install with: pip install openai")
        raise LLMClientError("OpenAI package not installed. Install with: pip install openai")
    
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        logger.error("OPENAI_API_KEY environment variable not set")
        raise LLMClientError("OPENAI_API_KEY environment variable not set. Please set it to use LLM features.")
    
    return OpenAI(api_key=api_key)


def generate_metadata_with_llm(command_name: str, description: str) -> Dict[str, Any]:
    """
    Generate command metadata using an LLM.
    
    Args:
        command_name: The name of the command
        description: User-provided description of the command
        
    Returns:
        Dictionary containing the generated command metadata
        
    Raises:
        LLMClientError: If there's an error communicating with the LLM
    """
    try:
        client = get_openai_client()
        
        # Create a prompt for the LLM
        prompt = f"""
        Generate YAML metadata for a command named '{command_name}' with the following description:
        
        {description}
        
        The metadata should follow this structure:
        ```yaml
        name: string (required)
        description: string (required)
        params:
          - name: string (required)
            type: string (default: "string")
            description: string (optional, default: "")
            required: boolean (default: true)
            default: any (optional, default: null)
        hidden: boolean (default: false)
        disabled: boolean (default: false)
        mcp_integration:
          enabled: boolean (default: true)
          metadata:
            endpoint: string (default auto-generated)
            method: string (default: "POST")
            authentication_required: boolean (default: false)
        llm_interaction:
          enabled: boolean (default: false)
          auto_apply: boolean (default: true)
          max_llm_turns: integer (default: 15)
        ```
        
        Based on the description, infer appropriate parameters that the command might need.
        Return only the YAML content, nothing else.
        """
        
        # Call the OpenAI API
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Using a smaller model for cost efficiency
            messages=[
                {"role": "system", "content": "You are a helpful assistant that generates YAML metadata for commands."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # Lower temperature for more deterministic output
            max_tokens=1000
        )
        
        # Extract the YAML content from the response
        yaml_content = response.choices[0].message.content.strip()
        
        # If the response contains markdown code blocks, extract just the YAML
        if "```yaml" in yaml_content:
            yaml_content = yaml_content.split("```yaml")[1].split("```")[0].strip()
        elif "```" in yaml_content:
            yaml_content = yaml_content.split("```")[1].split("```")[0].strip()
        
        # Parse the YAML content
        metadata = yaml.safe_load(yaml_content)
        
        # Ensure the command name is set correctly
        metadata["name"] = command_name
        
        # Validate the metadata structure
        if "description" not in metadata:
            metadata["description"] = description
        
        # Ensure all required fields are present
        for key, value in DEFAULT_METADATA.items():
            if key not in metadata:
                metadata[key] = value
        
        return metadata
    
    except Exception as e:
        logger.error(f"Error generating metadata with LLM: {e}")
        raise LLMClientError(f"Error generating metadata with LLM: {e}")

implementation_guidelines = """
            The implementation should be a Python file with a `tool_<command_name>()` function that:
            1. Accepts the parameters defined in the metadata
            2. Implements the functionality described in the metadata description
            3. Returns a result as defined in the metadata
            
            Follow these guidelines:
            - The entry point of the tool is the tool_<command_name>() function.  Other functions are allowed.
            - The input args should be simple python types
            - The output should be a simple python type
            - Include proper docstrings
            - This is not a Flask app, do not include any Flask-specific code
            - Handle parameter validation
            - Include error handling
            - Follow PEP 8 style guidelines
"""

def generate_implementation_with_llm(command_name: str, metadata: Dict[str, Any]) -> str:
    """
    Generate command implementation using an LLM.
    
    Args:
        command_name: The name of the command
        metadata: The command metadata
        
    Returns:
        String containing the generated command implementation
        
    Raises:
        LLMClientError: If there's an error communicating with the LLM
    """
    try:
        client = get_openai_client()
        
        # Create a prompt for the LLM
        prompt = f"""
        Generate a Python implementation for a command named '{command_name}' with the following metadata:
        
        ```yaml
        {yaml.dump(metadata, default_flow_style=False)}
        ```
        
        {implementation_guidelines}
        Return only the Python code, nothing else.
        """
        
        # Call the OpenAI API
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Using a smaller model for cost efficiency
            messages=[
                {"role": "system", "content": "You are a helpful assistant that generates Python code for commands."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # Lower temperature for more deterministic output
            max_tokens=2000
        )
        
        # Extract the Python code from the response
        code_content = response.choices[0].message.content.strip()
        
        # If the response contains markdown code blocks, extract just the Python code
        if "```python" in code_content:
            code_content = code_content.split("```python")[1].split("```")[0].strip()
        elif "```" in code_content:
            code_content = code_content.split("```")[1].split("```")[0].strip()
        
        return code_content
    
    except Exception as e:
        logger.error(f"Error generating implementation with LLM: {e}")
        raise LLMClientError(f"Error generating implementation with LLM: {e}")


def check_additional_info_needed(command_name: str, description: str) -> Optional[str]:
    """
    Check if additional information is needed from the user to generate a good command.
    
    Args:
        command_name: The name of the command
        description: User-provided description of the command
        
    Returns:
        String with follow-up questions if more information is needed, None otherwise
        
    Raises:
        LLMClientError: If there's an error communicating with the LLM
    """
    try:
        client = get_openai_client()
        
        # Create a prompt for the LLM
        prompt = f"""
        I'm creating a command named '{command_name}' with the following description:
        ```
        {description}
        {implementation_guidelines}
        ```
        Based on this information, do you need any additional details to create a good command implementation?
        If yes, provide specific questions that would help clarify the command's purpose and functionality.
        If no, just respond with "No additional information needed."
        """
        
        # Call the OpenAI API
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # Using a smaller model for cost efficiency
            messages=[
                {"role": "system", "content": "You are a helpful assistant that helps users create commands."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,  # Lower temperature for more deterministic output
            max_tokens=500
        )
        
        # Extract the response
        response_text = response.choices[0].message.content.strip()
        
        # Check if additional information is needed
        if "no additional information needed" in response_text.lower():
            return None
        
        return response_text
    
    except Exception as e:
        logger.error(f"Error checking for additional information: {e}")
        # Don't raise an exception here, just return None to continue with available information
        return None


def generate_default_metadata_with_llm(command_name: str, description: str = "") -> Dict[str, Any]:
    """
    Generate default metadata for a command using an LLM, with fallback to basic defaults.
    
    Args:
        command_name: The name of the command
        description: Optional description of the command
        
    Returns:
        Dictionary containing the command metadata
    """
    try:
        # If no description is provided, use a generic one
        if not description:
            description = f"Command named {command_name}"
        
        # Generate metadata with LLM
        metadata = generate_metadata_with_llm(command_name, description)
        return metadata
    
    except LLMClientError as e:
        logger.warning(f"Falling back to default metadata: {e}")
        
        # Create basic default metadata
        metadata = DEFAULT_METADATA.copy()
        metadata["name"] = command_name
        metadata["description"] = description or f"Command named {command_name}"
        
        return metadata
    
    except Exception as e:
        logger.error(f"Unexpected error generating metadata: {e}")
        
        # Create basic default metadata
        metadata = DEFAULT_METADATA.copy()
        metadata["name"] = command_name
        metadata["description"] = description or f"Command named {command_name}"
        
        return metadata

================
File: evai/tool_storage.py
================
"""Tool storage utilities for EVAI CLI."""

import os
import logging
import subprocess
import tempfile
import importlib.util
import sys
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import inspect

import yaml
import json

# Set up logging
logger = logging.getLogger(__name__)

# Get the path to the templates directory
TEMPLATES_DIR = os.path.join(os.path.dirname(__file__), "templates")


def get_tool_dir(tool_name: str) -> str:
    """
    Get the directory path for a tool and create it if it doesn't exist.
    
    Args:
        tool_name: The name of the tool
        
    Returns:
        The absolute path to the tool directory
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_name={tool_name}", file=sys.stderr)
    
    if not tool_name:
        raise ValueError("Tool name cannot be empty")
    
    # Validate tool name (alphanumeric, hyphens, and underscores only)
    if not all(c.isalnum() or c in "-_" for c in tool_name):
        raise ValueError(
            "Tool name must contain only alphanumeric characters, hyphens, and underscores"
        )
    
    # Get the tool directory path
    tool_dir = os.path.expanduser(f"~/.evai/tools/{tool_name}")
    
    # Create the directory if it doesn't exist
    try:
        os.makedirs(tool_dir, exist_ok=True)
        logger.debug(f"Tool directory created or already exists: {tool_dir}")
    except OSError as e:
        logger.error(f"Failed to create tool directory: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    
    # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={tool_dir}", file=sys.stderr)
    return tool_dir


def load_tool_metadata(path: str) -> Dict[str, Any]:
    """
    Load tool metadata from a YAML file.
    
    Args:
        path: Path to the directory containing the tool.yaml file
        
    Returns:
        Dictionary containing the tool metadata
        
    Raises:
        FileNotFoundError: If the tool.yaml file doesn't exist
        yaml.YAMLError: If the YAML file is invalid
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - path={path}", file=sys.stderr)
    
    yaml_path = os.path.join(path, "tool.yaml")
    
    try:
        with open(yaml_path, "r") as f:
            metadata = yaml.safe_load(f)
            logger.debug(f"Loaded tool metadata from {yaml_path}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={metadata}", file=sys.stderr)
            return metadata if metadata else {}
    except FileNotFoundError:
        logger.error(f"Tool metadata file not found: {yaml_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise
    except yaml.YAMLError as e:
        logger.error(f"Invalid YAML in tool metadata file: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except Exception as e:
        logger.error(f"Error loading tool metadata: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def save_tool_metadata(path: str, data: Dict[str, Any]) -> None:
    """
    Save tool metadata to a YAML file.
    
    Args:
        path: Path to the directory where tool.yaml will be saved
        data: Dictionary containing the tool metadata
        
    Raises:
        OSError: If the file cannot be written
        yaml.YAMLError: If the data cannot be serialized to YAML
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - path={path}, data={data}", file=sys.stderr)
    
    if not data:
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: ValueError", file=sys.stderr)
        raise ValueError("Tool metadata cannot be empty")
    
    yaml_path = os.path.join(path, "tool.yaml")
    
    # Create the directory if it doesn't exist
    os.makedirs(os.path.dirname(yaml_path), exist_ok=True)
    
    try:
        with open(yaml_path, "w") as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)
            logger.debug(f"Saved tool metadata to {yaml_path}")
    except yaml.YAMLError as e:
        logger.error(f"Failed to serialize tool metadata to YAML: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except OSError as e:
        logger.error(f"Failed to write tool metadata file: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except Exception as e:
        logger.error(f"Error saving tool metadata: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    
    # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=None", file=sys.stderr)


def get_editor() -> str:
    """
    Get the user's preferred editor.
    
    Returns:
        The path to the editor executable
    """
    # Try to get the editor from the EDITOR environment variable
    editor = os.environ.get("EDITOR")
    
    # If not set, use a default editor
    if not editor:
        if sys.platform == "win32":
            editor = "notepad.exe"
        else:
            # Try to find a common editor
            for e in ["nano", "vim", "vi", "emacs"]:
                try:
                    subprocess.run(["which", e], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    editor = e
                    break
                except subprocess.SubprocessError:
                    continue
            
            # If no editor found, use nano as a last resort
            if not editor:
                editor = "nano"
    
    return editor


def load_sample_tool_py() -> str:
    """
    Load the sample tool.py template.
    
    Returns:
        The contents of the sample tool.py file
        
    Raises:
        FileNotFoundError: If the sample file doesn't exist
    """
    sample_path = os.path.join(TEMPLATES_DIR, "sample_tool.py")
    
    try:
        with open(sample_path, "r") as f:
            return f.read()
    except FileNotFoundError:
        logger.error(f"Sample tool.py file not found: {sample_path}")
        raise
    except Exception as e:
        logger.error(f"Error loading sample tool.py: {e}")
        raise


def load_sample_tool_yaml(tool_name: str) -> Dict[str, Any]:
    """
    Load the sample tool.yaml template and substitute the tool name.
    
    Args:
        tool_name: The name of the tool
        
    Returns:
        Dictionary containing the tool metadata
        
    Raises:
        FileNotFoundError: If the sample file doesn't exist
        yaml.YAMLError: If the YAML file is invalid
    """
    sample_path = os.path.join(TEMPLATES_DIR, "sample_tool.yaml")
    
    try:
        with open(sample_path, "r") as f:
            template = f.read()
            # Replace the placeholder with the actual tool name
            template = template.replace("{tool_name}", tool_name)
            # Parse the YAML
            metadata = yaml.safe_load(template)
            return metadata if metadata else {}
    except FileNotFoundError:
        logger.error(f"Sample tool.yaml file not found: {sample_path}")
        raise
    except yaml.YAMLError as e:
        logger.error(f"Invalid YAML in sample tool.yaml file: {e}")
        raise
    except Exception as e:
        logger.error(f"Error loading sample tool.yaml: {e}")
        raise


def edit_tool_metadata(tool_dir: str) -> Tuple[bool, Optional[Dict[str, Any]]]:
    """
    Open the tool.yaml file in the user's preferred editor and validate it after editing.
    
    Args:
        tool_dir: Path to the tool directory
        
    Returns:
        A tuple containing:
        - A boolean indicating whether the edit was successful
        - The updated metadata dictionary if successful, None otherwise
        
    Raises:
        FileNotFoundError: If the tool.yaml file doesn't exist
        subprocess.SubprocessError: If the editor process fails
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_dir={tool_dir}", file=sys.stderr)
    
    yaml_path = os.path.join(tool_dir, "tool.yaml")
    
    if not os.path.exists(yaml_path):
        logger.error(f"Tool metadata file not found: {yaml_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool metadata file not found: {yaml_path}")
    
    editor = get_editor()
    logger.debug(f"Using editor: {editor}")
    
    try:
        # Open the editor for the user to edit the file
        subprocess.run([editor, yaml_path], check=True)
        logger.debug(f"Editor closed for {yaml_path}")
        
        # Try to load the edited file
        try:
            metadata = load_tool_metadata(tool_dir)
            result = (True, metadata)
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={result}", file=sys.stderr)
            return result
        except yaml.YAMLError as e:
            logger.error(f"Invalid YAML after editing: {e}")
            result = (False, None)
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={result}", file=sys.stderr)
            return result
            
    except subprocess.SubprocessError as e:
        logger.error(f"Error running editor: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def edit_tool_implementation(tool_dir: str) -> bool:
    """
    Open the tool.py file in the user's preferred editor.
    
    Args:
        tool_dir: Path to the tool directory
        
    Returns:
        A boolean indicating whether the edit was successful
        
    Raises:
        FileNotFoundError: If the tool.py file doesn't exist
        subprocess.SubprocessError: If the editor process fails
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_dir={tool_dir}", file=sys.stderr)
    
    py_path = os.path.join(tool_dir, "tool.py")
    
    if not os.path.exists(py_path):
        logger.error(f"Tool implementation file not found: {py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool implementation file not found: {py_path}")
    
    editor = get_editor()
    logger.debug(f"Using editor: {editor}")
    
    try:
        # Open the editor for the user to edit the file
        subprocess.run([editor, py_path], check=True)
        logger.debug(f"Editor closed for {py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=True", file=sys.stderr)
        return True
            
    except subprocess.SubprocessError as e:
        logger.error(f"Error running editor: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def run_lint_check(tool_dir: str) -> Tuple[bool, Optional[str]]:
    """
    Run flake8 on the tool.py file to check for linting errors.
    
    Args:
        tool_dir: Path to the tool directory
        
    Returns:
        A tuple containing:
        - A boolean indicating whether the lint check passed
        - The lint error output if the check failed, None otherwise
        
    Raises:
        FileNotFoundError: If the tool.py file doesn't exist
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_dir={tool_dir}", file=sys.stderr)
    
    py_path = os.path.join(tool_dir, "tool.py")
    
    if not os.path.exists(py_path):
        logger.error(f"Tool implementation file not found: {py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool implementation file not found: {py_path}")
    
    try:
        # Run flake8 on the file
        result = subprocess.run(
            ["flake8", py_path],
            check=False,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # Check if flake8 found any errors
        if result.returncode == 0:
            logger.debug(f"Lint check passed for {py_path}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(True, None)", file=sys.stderr)
            return (True, None)
        else:
            logger.warning(f"Lint check failed for {py_path}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(False, {result.stdout})", file=sys.stderr)
            return (False, result.stdout)
    except FileNotFoundError:
        # flake8 is not installed
        logger.warning("flake8 is not installed, skipping lint check")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(True, None)", file=sys.stderr)
        return (True, None)
    except Exception as e:
        logger.error(f"Error running lint check: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return=(False, {str(e)})", file=sys.stderr)
        return (False, str(e))


def list_tools() -> List[Dict[str, Any]]:
    """
    List all available tools.
    
    Returns:
        A list of dictionaries containing tool metadata
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name}", file=sys.stderr)
    
    tools_dir = os.path.expanduser("~/.evai/tools")
    
    # Create the directory if it doesn't exist
    os.makedirs(tools_dir, exist_ok=True)
    
    tools = []
    
    # Scan the tools directory
    for tool_name in os.listdir(tools_dir):
        tool_dir = os.path.join(tools_dir, tool_name)
        
        # Skip if not a directory
        if not os.path.isdir(tool_dir):
            continue
        
        try:
            # Load the tool metadata
            metadata = load_tool_metadata(tool_dir)
            
            # Skip disabled tools
            if metadata.get("disabled", False):
                continue
                
            # Add the tool to the list
            tools.append({
                "name": metadata.get("name", tool_name),
                "description": metadata.get("description", "No description"),
                "path": tool_dir
            })
        except Exception as e:
            logger.warning(f"Error loading tool '{tool_name}': {e}")
    
    # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={tools}", file=sys.stderr)
    return tools


def import_tool_module(tool_name: str) -> Any:
    """
    Dynamically import a tool module.
    
    Args:
        tool_name: The name of the tool
        
    Returns:
        The imported module
        
    Raises:
        ImportError: If the module cannot be imported
        FileNotFoundError: If the tool.py file doesn't exist
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_name={tool_name}", file=sys.stderr)
    
    tool_dir = get_tool_dir(tool_name)
    tool_py_path = os.path.join(tool_dir, "tool.py")
    
    if not os.path.exists(tool_py_path):
        logger.error(f"Tool implementation file not found: {tool_py_path}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: FileNotFoundError", file=sys.stderr)
        raise FileNotFoundError(f"Tool implementation file not found: {tool_py_path}")
    
    try:
        # Create a module spec
        spec = importlib.util.spec_from_file_location(
            f"evai.tools.{tool_name}", tool_py_path
        )
        
        if spec is None or spec.loader is None:
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: ImportError", file=sys.stderr)
            raise ImportError(f"Failed to create module spec for {tool_py_path}")
        
        # Create the module
        module = importlib.util.module_from_spec(spec)
        
        # Add the module to sys.modules
        sys.modules[spec.name] = module
        
        # Execute the module
        spec.loader.exec_module(module)
        
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={module}", file=sys.stderr)
        return module
    except Exception as e:
        logger.error(f"Error importing tool module: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise ImportError(f"Error importing tool module: {e}")


def run_tool(tool_name: str, *args, **kwargs) -> Any:
    """
    Run a tool with the given arguments.
    
    Args:
        tool_name: The name of the tool
        *args: Positional arguments to pass to the tool function
        **kwargs: Keyword arguments to pass to the tool
        
    Returns:
        The result of the tool
        
    Raises:
        ImportError: If the tool module cannot be imported
        AttributeError: If the tool module doesn't have any tool_* functions
        Exception: If the tool execution fails
    """
    # print(f"DEBUG: ENTER {inspect.currentframe().f_code.co_name} - tool_name={tool_name}, args={args}, kwargs={kwargs}", file=sys.stderr)
    
    try:
        # Import the tool module
        module = import_tool_module(tool_name)
        
        # Find callable functions in the module that start with "tool_"
        tool_functions = [
            name for name, obj in inspect.getmembers(module)
            if inspect.isfunction(obj) and name.startswith('tool_')
        ]
        
        if not tool_functions:
            logger.error(f"Tool module doesn't have any tool_* functions: {tool_name}")
            # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: AttributeError", file=sys.stderr)
            raise AttributeError(f"Tool module doesn't have any tool_* functions: {tool_name}")
        
        # For backward compatibility, try 'run' first if it exists
        if "run" in tool_functions:
            result = module.run(**kwargs)
        else:
            # Use the first tool function found
            function_name = tool_functions[0]
            function = getattr(module, function_name)
            
            # Get the function signature
            sig = inspect.signature(function)
            
            # If we have positional arguments, use them
            if args:
                # Convert args to appropriate types based on function signature
                converted_args = []
                for i, (param_name, param) in enumerate(sig.parameters.items()):
                    if i < len(args):
                        # Get the parameter type annotation
                        param_type = param.annotation
                        if param_type is inspect.Parameter.empty:
                            # No type annotation, use the arg as is
                            converted_args.append(args[i])
                        else:
                            # Try to convert the arg to the annotated type
                            try:
                                # Handle special cases for common types
                                if param_type is float or param_type is int:
                                    converted_args.append(param_type(args[i]))
                                elif param_type is bool:
                                    # Convert string to bool
                                    value = str(args[i]).lower()
                                    converted_args.append(value in ('true', 't', 'yes', 'y', '1'))
                                else:
                                    # For other types, try direct conversion
                                    converted_args.append(param_type(args[i]))
                            except (ValueError, TypeError):
                                # If conversion fails, use the original value
                                logger.warning(f"Could not convert argument {args[i]} to {param_type.__name__}")
                                converted_args.append(args[i])
                
                # Call the function with the converted positional arguments
                result = function(*converted_args)
            else:
                # Filter kwargs to only include parameters that the function accepts
                filtered_kwargs = {
                    k: v for k, v in kwargs.items() 
                    if k in sig.parameters
                }
                
                # Call the function with the filtered kwargs
                result = function(**filtered_kwargs)
        
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - return={result}", file=sys.stderr)
        return result
    except (ImportError, AttributeError) as e:
        logger.error(f"Error running tool: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise
    except Exception as e:
        logger.error(f"Tool execution failed: {e}")
        # print(f"DEBUG: EXIT {inspect.currentframe().f_code.co_name} - EXCEPTION: {e}", file=sys.stderr)
        raise


def remove_tool(tool_name: str) -> bool:
    """
    Remove a tool by deleting its directory.
    
    Args:
        tool_name: The name of the tool to remove
        
    Returns:
        True if the tool was successfully removed, False otherwise
        
    Raises:
        ValueError: If the tool name is invalid
        FileNotFoundError: If the tool directory doesn't exist
    """
    if not tool_name:
        raise ValueError("Tool name cannot be empty")
    
    # Validate tool name (alphanumeric, hyphens, and underscores only)
    if not all(c.isalnum() or c in "-_" for c in tool_name):
        raise ValueError(
            "Tool name must contain only alphanumeric characters, hyphens, and underscores"
        )
    
    # Get the tool directory path
    tool_dir = os.path.expanduser(f"~/.evai/tools/{tool_name}")
    
    # Check if the directory exists
    if not os.path.exists(tool_dir):
        raise FileNotFoundError(f"Tool '{tool_name}' not found")
    
    # Remove the directory and all its contents
    try:
        import shutil
        shutil.rmtree(tool_dir)
        logger.debug(f"Tool directory removed: {tool_dir}")
        return True
    except OSError as e:
        logger.error(f"Failed to remove tool directory: {e}")
        raise

================
File: tests/test_tools/test_cli_integration.py
================
import unittest
import os
import sys
import tempfile
import shutil
import subprocess
from unittest.mock import patch

# Add the parent directory to the path so we can import the evai package
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from evai.tool_storage import save_tool_metadata


class TestCLIIntegration(unittest.TestCase):
    def setUp(self):
        # Create a temporary directory for the test tool
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test tool
        self.tool_name = "test_subtract"
        self.tool_dir = os.path.join(self.temp_dir, ".evai", "tools", self.tool_name)
        os.makedirs(self.tool_dir, exist_ok=True)
        
        # Create the tool.py file
        with open(os.path.join(self.tool_dir, "tool.py"), "w") as f:
            f.write("""
def tool_subtract(minuend: float, subtrahend: float) -> float:
    \"\"\"
    Subtract one number from another.

    Parameters:
    minuend (float): The number from which another number will be subtracted.
    subtrahend (float): The number that will be subtracted from the minuend.

    Returns:
    float: The result of the subtraction (minuend - subtrahend).

    Raises:
    ValueError: If either minuend or subtrahend is not a number.
    \"\"\"
    # Validate input types
    if not isinstance(minuend, (int, float)):
        raise ValueError("Minuend must be a number.")
    if not isinstance(subtrahend, (int, float)):
        raise ValueError("Subtrahend must be a number.")

    # Perform the subtraction
    result = minuend - subtrahend
    return result
""")
        
        # Create the tool.yaml file
        metadata = {
            "name": self.tool_name,
            "description": "Test subtract tool",
            "params": [
                {
                    "name": "minuend",
                    "description": "The number from which another number will be subtracted",
                    "type": "float",
                    "required": True
                },
                {
                    "name": "subtrahend",
                    "description": "The number that will be subtracted from the minuend",
                    "type": "float",
                    "required": True
                }
            ]
        }
        save_tool_metadata(self.tool_dir, metadata)
    
    def tearDown(self):
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)
    
    @patch('evai.tool_storage.get_tool_dir')
    def test_cli_positional_args(self, mock_get_tool_dir):
        # Set up the mock
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Run the CLI command with positional arguments
        # Note: This is a simplified test that doesn't actually run the CLI command
        # In a real test, you would use subprocess.run to run the CLI command
        
        # Instead, we'll just verify that our changes to run_tool work correctly
        from evai.tool_storage import run_tool
        
        # Test with positional arguments
        result = run_tool(self.tool_name, "8", "5")
        self.assertEqual(result, 3.0)
        
        # Test with different values
        result = run_tool(self.tool_name, "10", "2")
        self.assertEqual(result, 8.0)
        
        # Test with negative numbers
        result = run_tool(self.tool_name, "-5", "3")
        self.assertEqual(result, -8.0)


if __name__ == "__main__":
    unittest.main()

================
File: tests/test_tools/test_positional_args.py
================
import unittest
import os
import sys
import tempfile
import shutil
from unittest.mock import patch, MagicMock

# Add the parent directory to the path so we can import the evai package
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from evai.tool_storage import save_tool_metadata, run_tool


class TestPositionalArgs(unittest.TestCase):
    def setUp(self):
        # Create a temporary directory for the test tool
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test tool
        self.tool_name = "test_subtract"
        self.tool_dir = os.path.join(self.temp_dir, self.tool_name)
        os.makedirs(self.tool_dir, exist_ok=True)
        
        # Create the tool.py file
        with open(os.path.join(self.tool_dir, "tool.py"), "w") as f:
            f.write("""
def tool_subtract(minuend: float, subtrahend: float) -> float:
    \"\"\"
    Subtract one number from another.

    Parameters:
    minuend (float): The number from which another number will be subtracted.
    subtrahend (float): The number that will be subtracted from the minuend.

    Returns:
    float: The result of the subtraction (minuend - subtrahend).

    Raises:
    ValueError: If either minuend or subtrahend is not a number.
    \"\"\"
    # Validate input types
    if not isinstance(minuend, (int, float)):
        raise ValueError("Minuend must be a number.")
    if not isinstance(subtrahend, (int, float)):
        raise ValueError("Subtrahend must be a number.")

    # Perform the subtraction
    result = minuend - subtrahend
    return result
""")
        
        # Create the tool.yaml file
        metadata = {
            "name": self.tool_name,
            "description": "Test subtract tool",
            "params": [
                {
                    "name": "minuend",
                    "description": "The number from which another number will be subtracted",
                    "type": "float",
                    "required": True
                },
                {
                    "name": "subtrahend",
                    "description": "The number that will be subtracted from the minuend",
                    "type": "float",
                    "required": True
                }
            ]
        }
        save_tool_metadata(self.tool_dir, metadata)
    
    def tearDown(self):
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)
    
    @patch('evai.tool_storage.get_tool_dir')
    @patch('evai.tool_storage.import_tool_module')
    def test_positional_args(self, mock_import_tool_module, mock_get_tool_dir):
        # Set up the mocks
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Create a mock module with the tool_subtract function
        mock_module = MagicMock()
        
        # Define the tool_subtract function
        def tool_subtract(minuend, subtrahend):
            return float(minuend) - float(subtrahend)
        
        # Set the tool_subtract function on the mock module
        mock_module.tool_subtract = tool_subtract
        mock_import_tool_module.return_value = mock_module
        
        # Test with positional arguments
        result = run_tool(self.tool_name, "8", "5")
        self.assertEqual(result, 3.0)
        
        # Test with different values
        result = run_tool(self.tool_name, "10", "2")
        self.assertEqual(result, 8.0)
        
        # Test with negative numbers
        result = run_tool(self.tool_name, "-5", "3")
        self.assertEqual(result, -8.0)
    
    @patch('evai.tool_storage.get_tool_dir')
    @patch('evai.tool_storage.import_tool_module')
    def test_keyword_args(self, mock_import_tool_module, mock_get_tool_dir):
        # Set up the mocks
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Create a mock module with the tool_subtract function
        mock_module = MagicMock()
        
        # Define the tool_subtract function
        def tool_subtract(minuend, subtrahend):
            return float(minuend) - float(subtrahend)
        
        # Set the tool_subtract function on the mock module
        mock_module.tool_subtract = tool_subtract
        mock_import_tool_module.return_value = mock_module
        
        # Test with keyword arguments (backward compatibility)
        result = run_tool(self.tool_name, minuend=8, subtrahend=5)
        self.assertEqual(result, 3.0)
        
        # Test with different values
        result = run_tool(self.tool_name, minuend=10, subtrahend=2)
        self.assertEqual(result, 8.0)
        
        # Test with negative numbers
        result = run_tool(self.tool_name, minuend=-5, subtrahend=3)
        self.assertEqual(result, -8.0)
    
    @patch('evai.tool_storage.get_tool_dir')
    @patch('evai.tool_storage.import_tool_module')
    def test_mixed_args(self, mock_import_tool_module, mock_get_tool_dir):
        # Set up the mocks
        mock_get_tool_dir.return_value = self.tool_dir
        
        # Create a mock module with the tool_subtract function
        mock_module = MagicMock()
        
        # Define the tool_subtract function
        def tool_subtract(minuend, subtrahend):
            return float(minuend) - float(subtrahend)
        
        # Set the tool_subtract function on the mock module
        mock_module.tool_subtract = tool_subtract
        mock_import_tool_module.return_value = mock_module
        
        # This should raise an error because we don't support mixed args
        with self.assertRaises(TypeError):
            run_tool(self.tool_name, "8", subtrahend=5)


if __name__ == "__main__":
    unittest.main()

================
File: tests/__init__.py
================
"""Tests for the EVAI CLI."""

================
File: tests/test_add_command.py
================
"""Tests for the command add functionality."""

import os
import shutil
import tempfile
import yaml
from click.testing import CliRunner
from unittest import mock

from evai.cli.cli import cli


class TestAddCommand:
    """Tests for the command add functionality."""

    def setup_method(self):
        """Set up the test environment."""
        # Create a temporary directory for the tests
        self.temp_dir = tempfile.mkdtemp()
        
        # Create the .evai directory structure
        self.evai_dir = os.path.join(self.temp_dir, '.evai')
        os.makedirs(self.evai_dir, exist_ok=True)
        
        # Mock the expanduser function to use our temporary directory
        self.patcher = mock.patch('os.path.expanduser')
        self.mock_expanduser = self.patcher.start()
        # When expanduser is called with ~/.evai, return our temp .evai dir
        self.mock_expanduser.side_effect = lambda path: path.replace('~', self.temp_dir)

    def teardown_method(self):
        """Clean up after the tests."""
        # Stop the patcher
        self.patcher.stop()
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)

    def test_add_command(self):
        """Test adding a new command."""
        runner = CliRunner()
        result = runner.invoke(cli, ['tools', 'add', 'test-command'])
        
        # Check that the command was successful
        assert result.exit_code == 0
        assert "Tool 'test-command' created successfully." in result.output
        
        # Check that the command directory was created
        command_dir = os.path.join(self.temp_dir, '.evai', 'tools', 'test-command')
        assert os.path.exists(command_dir)
        
        # Check that the command.yaml file was created with the correct content
        yaml_path = os.path.join(command_dir, 'tool.yaml')
        assert os.path.exists(yaml_path)
        
        with open(yaml_path, 'r') as f:
            metadata = yaml.safe_load(f)
        
        assert metadata['name'] == 'test-command'
        assert metadata['description'] == 'Default description'
        assert metadata['params'] == []
        assert metadata['hidden'] is False
        assert metadata['disabled'] is False
        assert metadata['mcp_integration']['enabled'] is True
        assert metadata['mcp_integration']['metadata']['method'] == 'POST'
        assert metadata['mcp_integration']['metadata']['authentication_required'] is False
        assert metadata['llm_interaction']['enabled'] is False
        assert metadata['llm_interaction']['auto_apply'] is True
        assert metadata['llm_interaction']['max_llm_turns'] == 15
        
        # Check that the command.py file was created with the correct content
        py_path = os.path.join(command_dir, 'tool.py')
        assert os.path.exists(py_path)
        
        with open(py_path, 'r') as f:
            content = f.read()
        
        assert '"""Custom tool implementation."""' in content
        assert 'def tool_echo(' in content
        assert 'return echo_string' in content

    def test_add_command_invalid_name(self):
        """Test adding a command with an invalid name."""
        runner = CliRunner()
        result = runner.invoke(cli, ['tools', 'add', 'test command'])  # Space in name
        
        # Check that the command failed
        assert result.exit_code == 1
        assert "Error creating tool" in result.output
        
        # Check that the command directory was not created
        command_dir = os.path.join(self.temp_dir, '.evai', 'tools', 'test command')
        assert not os.path.exists(command_dir)

================
File: tests/test_cli.py
================
"""Tests for the EVAI CLI."""

import subprocess
import sys
from unittest import mock

import click.testing
import pytest

from evai import __version__
from evai.cli import cli, main


def test_version():
    """Test that the version is correct."""
    assert __version__ == "0.1.0"


def test_cli_help():
    """Test that the CLI prints help when invoked with --help."""
    runner = click.testing.CliRunner()
    result = runner.invoke(cli, ["--help"])
    assert result.exit_code == 0
    assert "EVAI CLI - Command-line interface for EVAI" in result.output


def test_cli_version():
    """Test that the CLI prints version when invoked with --version."""
    runner = click.testing.CliRunner()
    result = runner.invoke(cli, ["--version"])
    assert result.exit_code == 0
    assert f"evai, version {__version__}" in result.output


def test_main_no_args():
    """Test that main() shows help when no arguments are provided."""
    with mock.patch("sys.argv", ["evai"]):
        with mock.patch("evai.cli.cli") as mock_cli:
            main()
            mock_cli.assert_called_once()
            # Check that --help was added to sys.argv
            assert sys.argv == ["evai", "--help"]


def test_cli_as_module():
    """Test that the CLI can be invoked as a module."""
    result = subprocess.run(
        [sys.executable, "-m", "evai.cli", "--version"],
        capture_output=True,
        text=True,
        check=False,
    )
    assert result.returncode == 0
    assert f"version {__version__}" in result.stdout

================
File: tests/test_command_add.py
================
"""Tests for command adding functionality."""

import os
import sys
import json
import tempfile
import shutil
from unittest import mock
import pytest
from click.testing import CliRunner

from evai.cli.cli import cli
from evai.command_storage import get_command_dir, save_command_metadata


@pytest.fixture
def mock_commands_dir():
    """Create a temporary directory for commands."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Mock the expanduser function to return our temp directory
        with mock.patch('os.path.expanduser') as mock_expanduser:
            # When expanduser is called with ~/.evai, return our temp .evai dir
            mock_expanduser.side_effect = lambda path: path.replace('~', temp_dir)
            
            # Create the commands directory
            commands_dir = os.path.join(temp_dir, '.evai', 'commands')
            os.makedirs(commands_dir, exist_ok=True)
            
            yield temp_dir


def test_add_command(mock_commands_dir):
    """Test adding a command."""
    runner = CliRunner()
    result = runner.invoke(cli, ['commands', 'add', 'test-command'])
    
    assert result.exit_code == 0
    assert "Command 'test-command' created successfully." in result.output
    
    # Verify files were created
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'test-command')
    assert os.path.exists(command_dir)
    assert os.path.exists(os.path.join(command_dir, 'command.yaml'))
    assert os.path.exists(os.path.join(command_dir, 'command.py'))
    
    # Verify content of command.yaml
    with open(os.path.join(command_dir, 'command.yaml'), 'r') as f:
        metadata = json.load(f)
        assert metadata["name"] == "test-command"

================
File: tests/test_edit_implementation.py
================
"""Tests for the command implementation editing and lint checking functionality."""

import os
import shutil
import tempfile
from unittest import mock
import subprocess

from evai.tool_storage import edit_command_implementation, run_lint_check


class TestEditImplementation:
    """Tests for the command implementation editing and lint checking functionality."""

    def setup_method(self):
        """Set up the test environment."""
        # Create a temporary directory for the tests
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test command directory
        self.command_dir = os.path.join(self.temp_dir, 'test-command')
        os.makedirs(self.command_dir, exist_ok=True)
        
        # Create a test command.py file with valid Python code
        self.py_path = os.path.join(self.command_dir, 'command.py')
        with open(self.py_path, 'w') as f:
            f.write('"""Custom command implementation."""\n\n\ndef tool_echo(echo_string: str) -> str:\n    """Echo the input string."""\n    return echo_string\n')

    def teardown_method(self):
        """Clean up after the tests."""
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)

    @mock.patch('subprocess.run')
    def test_edit_command_implementation_success(self, mock_run):
        """Test editing command implementation successfully."""
        # Mock the subprocess.run call to simulate the editor
        mock_run.return_value = subprocess.CompletedProcess(args=['vi', self.py_path], returncode=0)
        
        # Call the function
        success = edit_command_implementation(self.command_dir)
        
        # Check that the function returned success
        assert success is True
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args
        assert args[0][0] in ['vi', os.environ.get('EDITOR', 'vi')]
        assert args[0][1] == self.py_path
        assert kwargs['check'] is True

    def test_edit_command_implementation_file_not_found(self):
        """Test editing command implementation when the file doesn't exist."""
        # Remove the Python file
        os.remove(self.py_path)
        
        # Call the function and check that it raises FileNotFoundError
        try:
            edit_command_implementation(self.command_dir)
            assert False, "Expected FileNotFoundError but no exception was raised"
        except FileNotFoundError:
            pass

    @mock.patch('subprocess.run')
    def test_edit_command_implementation_subprocess_error(self, mock_run):
        """Test editing command implementation when the subprocess fails."""
        # Mock the subprocess.run call to simulate an error
        mock_run.side_effect = subprocess.SubprocessError("Editor process failed")
        
        # Call the function and check that it raises SubprocessError
        try:
            edit_command_implementation(self.command_dir)
            assert False, "Expected SubprocessError but no exception was raised"
        except subprocess.SubprocessError:
            pass

    @mock.patch('subprocess.run')
    def test_run_lint_check_success(self, mock_run):
        """Test running lint check on a valid Python file."""
        # Mock the subprocess.run call to simulate flake8 passing
        mock_run.return_value = subprocess.CompletedProcess(
            args=['flake8', self.py_path],
            returncode=0,
            stdout='',
            stderr=''
        )
        
        # Call the function
        success, output = run_lint_check(self.command_dir)
        
        # Check that the function returned success
        assert success is True
        assert output is None
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args
        assert args[0][0] == 'flake8'
        assert args[0][1] == self.py_path
        assert kwargs['capture_output'] is True
        assert kwargs['text'] is True
        assert kwargs['check'] is False

    @mock.patch('subprocess.run')
    def test_run_lint_check_failure(self, mock_run):
        """Test running lint check on a Python file with lint errors."""
        # Create a Python file with lint errors
        with open(self.py_path, 'w') as f:
            f.write('"""Custom command implementation."""\n\nimport os\n\ndef tool_echo(echo_string: str) -> str:\n    x = 1\n    y = 2  # unused variable\n    return echo_string\n')
        
        # Mock the subprocess.run call to simulate flake8 failing
        mock_run.return_value = subprocess.CompletedProcess(
            args=['flake8', self.py_path],
            returncode=1,
            stdout=f'{self.py_path}:7:5: F841 local variable \'y\' is assigned to but never used',
            stderr=''
        )
        
        # Call the function
        success, output = run_lint_check(self.command_dir)
        
        # Check that the function returned failure
        assert success is False
        assert 'F841 local variable \'y\' is assigned to but never used' in output
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()

    @mock.patch('subprocess.run')
    def test_run_lint_check_flake8_not_found(self, mock_run):
        """Test running lint check when flake8 is not installed."""
        # Mock the subprocess.run call to simulate flake8 not being found
        mock_run.side_effect = FileNotFoundError("No such file or directory: 'flake8'")
        
        # Call the function
        success, output = run_lint_check(self.command_dir)
        
        # Check that the function returned failure
        assert success is False
        assert 'flake8 command not found' in output
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()

    def test_run_lint_check_file_not_found(self):
        """Test running lint check when the file doesn't exist."""
        # Remove the Python file
        os.remove(self.py_path)
        
        # Call the function and check that it raises FileNotFoundError
        try:
            run_lint_check(self.command_dir)
            assert False, "Expected FileNotFoundError but no exception was raised"
        except FileNotFoundError:
            pass

================
File: tests/test_edit_metadata.py
================
"""Tests for the command metadata editing functionality."""

import os
import shutil
import tempfile
import yaml
from unittest import mock
import subprocess

from evai.tool_storage import edit_command_metadata, get_editor


class TestEditMetadata:
    """Tests for the command metadata editing functionality."""

    def setup_method(self):
        """Set up the test environment."""
        # Create a temporary directory for the tests
        self.temp_dir = tempfile.mkdtemp()
        
        # Create a test command directory
        self.command_dir = os.path.join(self.temp_dir, 'test-command')
        os.makedirs(self.command_dir, exist_ok=True)
        
        # Create a test command.yaml file
        self.yaml_path = os.path.join(self.command_dir, 'command.yaml')
        self.test_metadata = {
            "name": "test-command",
            "description": "Test description",
            "params": [],
            "hidden": False,
            "disabled": False,
            "mcp_integration": {
                "enabled": True,
                "metadata": {
                    "endpoint": "",
                    "method": "POST",
                    "authentication_required": False
                }
            },
            "llm_interaction": {
                "enabled": False,
                "auto_apply": True,
                "max_llm_turns": 15
            }
        }
        with open(self.yaml_path, 'w') as f:
            yaml.dump(self.test_metadata, f, default_flow_style=False, sort_keys=False)

    def teardown_method(self):
        """Clean up after the tests."""
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)

    def test_get_editor(self):
        """Test getting the editor command."""
        # Test with EDITOR environment variable set
        with mock.patch.dict('os.environ', {'EDITOR': 'nano'}):
            assert get_editor() == 'nano'
        
        # Test with EDITOR environment variable not set
        with mock.patch.dict('os.environ', clear=True):
            assert get_editor() == 'vi'

    @mock.patch('subprocess.run')
    def test_edit_command_metadata_success(self, mock_run):
        """Test editing command metadata successfully."""
        # Mock the subprocess.run call to simulate the editor
        mock_run.return_value = subprocess.CompletedProcess(args=['vi', self.yaml_path], returncode=0)
        
        # Call the function
        success, metadata = edit_command_metadata(self.command_dir)
        
        # Check that the function returned success
        assert success is True
        assert metadata == self.test_metadata
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()
        args, kwargs = mock_run.call_args
        assert args[0][0] in ['vi', os.environ.get('EDITOR', 'vi')]
        assert args[0][1] == self.yaml_path
        assert kwargs['check'] is True

    @mock.patch('subprocess.run')
    def test_edit_command_metadata_invalid_yaml(self, mock_run):
        """Test editing command metadata with invalid YAML."""
        # Mock the subprocess.run call to simulate the editor
        mock_run.return_value = subprocess.CompletedProcess(args=['vi', self.yaml_path], returncode=0)
        
        # Write invalid YAML to the file after the mock is set up
        with open(self.yaml_path, 'w') as f:
            f.write('invalid: yaml:\n  - missing: colon\n  indentation error\n')
        
        # Call the function
        success, metadata = edit_command_metadata(self.command_dir)
        
        # Check that the function returned failure
        assert success is False
        assert metadata is None
        
        # Verify that subprocess.run was called with the correct arguments
        mock_run.assert_called_once()

    def test_edit_command_metadata_file_not_found(self):
        """Test editing command metadata when the file doesn't exist."""
        # Remove the YAML file
        os.remove(self.yaml_path)
        
        # Call the function and check that it raises FileNotFoundError
        try:
            edit_command_metadata(self.command_dir)
            assert False, "Expected FileNotFoundError but no exception was raised"
        except FileNotFoundError:
            pass

    @mock.patch('subprocess.run')
    def test_edit_command_metadata_subprocess_error(self, mock_run):
        """Test editing command metadata when the subprocess fails."""
        # Mock the subprocess.run call to simulate an error
        mock_run.side_effect = subprocess.SubprocessError("Editor process failed")
        
        # Call the function and check that it raises SubprocessError
        try:
            edit_command_metadata(self.command_dir)
            assert False, "Expected SubprocessError but no exception was raised"
        except subprocess.SubprocessError:
            pass

================
File: tests/test_list_and_run.py
================
"""Tests for tool listing and execution."""

import os
import sys
import json
import tempfile
import shutil
from unittest import mock
import pytest
from click.testing import CliRunner

from evai.cli.cli import cli
from evai.tool_storage import get_tool_dir, save_tool_metadata


@pytest.fixture
def mock_tools_dir():
    """Create a temporary directory for tools."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Mock the expanduser function to return our temp directory
        with mock.patch('os.path.expanduser') as mock_expanduser:
            # When expanduser is called with ~/.evai, return our temp .evai dir
            mock_expanduser.side_effect = lambda path: path.replace('~', temp_dir)
            
            # Create the tools directory
            tools_dir = os.path.join(temp_dir, '.evai', 'tools')
            os.makedirs(tools_dir, exist_ok=True)
            
            # Create a test tool
            test_tool_dir = os.path.join(tools_dir, 'test-tool')
            os.makedirs(test_tool_dir, exist_ok=True)
            
            # Create tool.yaml
            metadata = {
                "name": "test-tool",
                "description": "A test tool",
                "params": [
                    {
                        "name": "message",
                        "description": "A message to echo",
                        "required": True
                    },
                    {
                        "name": "count",
                        "description": "Number of times to echo",
                        "required": False,
                        "default": 1
                    }
                ],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
            
            with open(os.path.join(test_tool_dir, 'tool.yaml'), 'w') as f:
                json.dump(metadata, f)
            
            # Create tool.py
            with open(os.path.join(test_tool_dir, 'tool.py'), 'w') as f:
                f.write('''"""Test tool implementation."""

def tool_echo(message="Hello", count=1):
    """Echo the message count times."""
    result = []
    for _ in range(count):
        result.append(message)
    return {"messages": result}
''')
            
            # Create a disabled tool
            disabled_tool_dir = os.path.join(tools_dir, 'disabled-tool')
            os.makedirs(disabled_tool_dir, exist_ok=True)
            
            # Create tool.yaml for disabled tool
            disabled_metadata = {
                "name": "disabled-tool",
                "description": "A disabled tool",
                "params": [],
                "hidden": False,
                "disabled": True,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
            
            with open(os.path.join(disabled_tool_dir, 'tool.yaml'), 'w') as f:
                json.dump(disabled_metadata, f)
            
            # Create tool.py for disabled tool
            with open(os.path.join(disabled_tool_dir, 'tool.py'), 'w') as f:
                f.write('''"""Disabled tool implementation."""

def run():
    """Run the disabled tool."""
    return {"status": "disabled"}
''')
            
            yield temp_dir


def test_list_tools(mock_tools_dir):
    """Test listing tools."""
    runner = CliRunner()
    result = runner.invoke(cli, ['tools', 'list'])
    
    assert result.exit_code == 0
    assert "Available tools:" in result.output
    assert "test-tool: A test tool" in result.output
    assert "disabled-tool" not in result.output


def test_run_tool(mock_tools_dir):
    """Test running a tool."""
    runner = CliRunner()
    result = runner.invoke(cli, ['tools', 'run', 'test-tool', '-p', 'message=Hello World', '-p', 'count=3'])
    
    assert result.exit_code == 0
    
    # Parse the JSON output
    output = json.loads(result.output)
    assert output == {"messages": ["Hello World", "Hello World", "Hello World"]}


def test_run_tool_with_default_params(mock_tools_dir):
    """Test running a tool with default parameters."""
    runner = CliRunner()
    result = runner.invoke(cli, ['tools', 'run', 'test-tool', '-p', 'message=Hello World'])
    
    assert result.exit_code == 0
    
    # Parse the JSON output
    output = json.loads(result.output)
    assert output == {"messages": ["Hello World"]}


def test_run_tool_missing_required_param(mock_tools_dir):
    """Test running a tool with a missing required parameter."""
    runner = CliRunner()
    result = runner.invoke(cli, ['tools', 'run', 'test-tool'])
    
    assert result.exit_code == 1
    assert "Missing required parameter: message" in result.output


def test_run_nonexistent_tool(mock_tools_dir):
    """Test running a nonexistent tool."""
    runner = CliRunner()
    result = runner.invoke(cli, ['tools', 'run', 'nonexistent-tool'])
    
    assert result.exit_code == 1
    assert "Error running tool" in result.output

================
File: tests/test_llm.py
================
"""Tests for the LLM interaction functionality."""

import os
import shutil
import tempfile
import yaml
from click.testing import CliRunner
from unittest import mock

from evai.cli import cli
from evai.llm_client import (
    generate_default_metadata_with_llm,
    generate_implementation_with_llm,
    check_additional_info_needed,
    LLMClientError
)


class TestLLMInteraction:
    """Tests for the LLM interaction functionality."""

    def setup_method(self):
        """Set up the test environment."""
        # Create a temporary directory for the tests
        self.temp_dir = tempfile.mkdtemp()
        
        # Create the .evai directory structure
        self.evai_dir = os.path.join(self.temp_dir, '.evai')
        os.makedirs(self.evai_dir, exist_ok=True)
        
        # Mock the expanduser function to use our temporary directory
        self.patcher = mock.patch('os.path.expanduser')
        self.mock_expanduser = self.patcher.start()
        # When expanduser is called with ~/.evai, return our temp .evai dir
        self.mock_expanduser.side_effect = lambda path: path.replace('~', self.temp_dir)

    def teardown_method(self):
        """Clean up after the tests."""
        # Stop the patcher
        self.patcher.stop()
        # Remove the temporary directory
        shutil.rmtree(self.temp_dir)

    @mock.patch('evai.llm_client.get_openai_client')
    def test_generate_metadata_with_llm(self, mock_get_client):
        """Test generating metadata with LLM."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = """```yaml
name: test-command
description: A test command
params:
  - name: param1
    type: string
    description: A test parameter
    required: true
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
```"""
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Call the function
        metadata = generate_default_metadata_with_llm("test-command", "A test command")
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        args, kwargs = mock_client.chat.completions.create.call_args
        assert kwargs["model"] == "gpt-4o-mini"
        assert len(kwargs["messages"]) == 2
        assert kwargs["messages"][0]["role"] == "system"
        assert kwargs["messages"][1]["role"] == "user"
        assert "test-command" in kwargs["messages"][1]["content"]
        assert "A test command" in kwargs["messages"][1]["content"]
        
        # Check the returned metadata
        assert metadata["name"] == "test-command"
        assert metadata["description"] == "A test command"
        assert len(metadata["params"]) == 1
        assert metadata["params"][0]["name"] == "param1"
        assert metadata["params"][0]["type"] == "string"
        assert metadata["params"][0]["description"] == "A test parameter"
        assert metadata["params"][0]["required"] is True
        assert metadata["hidden"] is False
        assert metadata["disabled"] is False
        assert metadata["mcp_integration"]["enabled"] is True
        assert metadata["mcp_integration"]["metadata"]["method"] == "POST"
        assert metadata["mcp_integration"]["metadata"]["authentication_required"] is False
        assert metadata["llm_interaction"]["enabled"] is False
        assert metadata["llm_interaction"]["auto_apply"] is True
        assert metadata["llm_interaction"]["max_llm_turns"] == 15

    @mock.patch('evai.llm_client.get_openai_client')
    def test_generate_implementation_with_llm(self, mock_get_client):
        """Test generating implementation with LLM."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = """```python
\"\"\"Custom command implementation.\"\"\"


def tool_echo(echo_string: str) -> str:
    \"\"\"Echo the input string.\"\"\"
    # Validate parameters
    if not echo_string:
        raise ValueError("Missing required parameter: echo_string")
    
    # Process the input
    result = echo_string.upper()
    
    # Return the result
    return result
```"""
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Create test metadata
        metadata = {
            "name": "test-command",
            "description": "A test command",
            "params": [
                {
                    "name": "param1",
                    "type": "string",
                    "description": "A test parameter",
                    "required": True
                }
            ],
            "hidden": False,
            "disabled": False,
            "mcp_integration": {
                "enabled": True,
                "metadata": {
                    "endpoint": "",
                    "method": "POST",
                    "authentication_required": False
                }
            },
            "llm_interaction": {
                "enabled": False,
                "auto_apply": True,
                "max_llm_turns": 15
            }
        }
        
        # Call the function
        implementation = generate_implementation_with_llm("test-command", metadata)
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        args, kwargs = mock_client.chat.completions.create.call_args
        assert kwargs["model"] == "gpt-4o-mini"
        assert len(kwargs["messages"]) == 2
        assert kwargs["messages"][0]["role"] == "system"
        assert kwargs["messages"][1]["role"] == "user"
        assert "test-command" in kwargs["messages"][1]["content"]
        
        # Check the returned implementation
        assert '"""Custom command implementation."""' in implementation
        assert 'def tool_echo(echo_string: str) -> str:' in implementation
        assert 'if not echo_string:' in implementation
        assert 'return result' in implementation

    @mock.patch('evai.llm_client.get_openai_client')
    def test_check_additional_info_needed(self, mock_get_client):
        """Test checking if additional information is needed."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = "Yes, I need more information. What specific task should this command perform?"
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Call the function
        result = check_additional_info_needed("test-command", "A test command")
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        args, kwargs = mock_client.chat.completions.create.call_args
        assert kwargs["model"] == "gpt-4o-mini"
        assert len(kwargs["messages"]) == 2
        assert kwargs["messages"][0]["role"] == "system"
        assert kwargs["messages"][1]["role"] == "user"
        assert "test-command" in kwargs["messages"][1]["content"]
        assert "A test command" in kwargs["messages"][1]["content"]
        
        # Check the returned result
        assert result == "Yes, I need more information. What specific task should this command perform?"

    @mock.patch('evai.llm_client.get_openai_client')
    def test_check_additional_info_not_needed(self, mock_get_client):
        """Test checking if additional information is not needed."""
        # Mock the OpenAI client
        mock_client = mock.MagicMock()
        mock_get_client.return_value = mock_client
        
        # Mock the response from the OpenAI API
        mock_response = mock.MagicMock()
        mock_message = mock.MagicMock()
        mock_message.content = "No additional information needed."
        mock_choice = mock.MagicMock()
        mock_choice.message = mock_message
        mock_response.choices = [mock_choice]
        mock_client.chat.completions.create.return_value = mock_response
        
        # Call the function
        result = check_additional_info_needed("test-command", "A detailed test command that does something specific")
        
        # Check that the client was called with the correct arguments
        mock_client.chat.completions.create.assert_called_once()
        
        # Check the returned result
        assert result is None

    @mock.patch('evai.llm_client.get_openai_client', side_effect=LLMClientError("Test error"))
    def test_generate_default_metadata_with_llm_fallback(self, mock_get_client):
        """Test fallback to default metadata when LLM fails."""
        # Call the function
        metadata = generate_default_metadata_with_llm("test-command", "A test command")
        
        # Check the returned metadata
        assert metadata["name"] == "test-command"
        assert metadata["description"] == "A test command"
        assert metadata["params"] == []
        assert metadata["hidden"] is False
        assert metadata["disabled"] is False
        assert metadata["mcp_integration"]["enabled"] is True
        assert metadata["llm_interaction"]["enabled"] is False

    @mock.patch('evai.cli.check_additional_info_needed')
    @mock.patch('evai.cli.generate_default_metadata_with_llm')
    @mock.patch('evai.cli.generate_implementation_with_llm')
    @mock.patch('evai.command_storage.edit_command_metadata')
    @mock.patch('evai.command_storage.edit_command_implementation')
    @mock.patch('evai.command_storage.run_lint_check')
    @mock.patch('evai.command_storage.get_editor')
    @mock.patch('subprocess.run')
    def test_llmadd_command(self, mock_subprocess_run, mock_get_editor, mock_lint_check, 
                           mock_edit_impl, mock_edit_meta, mock_gen_impl, 
                           mock_gen_meta, mock_check_info):
        """Test the llmadd command."""
        # Mock the editor
        mock_get_editor.return_value = "vi"
        mock_subprocess_run.return_value = mock.MagicMock(returncode=0)
        
        # Mock the LLM functions
        mock_check_info.return_value = "Additional information needed"
        mock_gen_meta.return_value = {
            "name": "test-command",
            "description": "A test command",
            "params": [
                {
                    "name": "param1",
                    "type": "string",
                    "description": "A test parameter",
                    "required": True
                }
            ],
            "hidden": False,
            "disabled": False,
            "mcp_integration": {
                "enabled": True,
                "metadata": {
                    "endpoint": "",
                    "method": "POST",
                    "authentication_required": False
                }
            },
            "llm_interaction": {
                "enabled": False,
                "auto_apply": True,
                "max_llm_turns": 15
            }
        }
        mock_gen_impl.return_value = '"""Custom command implementation."""\n\ndef tool_echo(echo_string: str) -> str:\n    """Echo the input string."""\n    return echo_string\n'
        
        # Mock the edit functions
        mock_edit_meta.return_value = (True, mock_gen_meta.return_value)
        mock_edit_impl.return_value = True
        mock_lint_check.return_value = (True, None)
        
        # Run the command
        runner = CliRunner()
        result = runner.invoke(
            cli, ['command', 'llmadd', 'test-command'], 
            input='A test command\n\nn\nn\n',
            catch_exceptions=False
        )
        
        # Print the output for debugging
        print(f"Command output: {result.output}")
        
        # Check that the command was successful
        assert result.exit_code == 0
        assert "Command 'test-command' created successfully." in result.output
        
        # Check that the command directory was created
        command_dir = os.path.join(self.temp_dir, '.evai', 'commands', 'test-command')
        assert os.path.exists(command_dir)
        
        # Check that the LLM functions were called
        mock_check_info.assert_called_once_with("test-command", "A test command")
        mock_gen_meta.assert_called_once_with("test-command", "A test command")
        mock_gen_impl.assert_called_once_with("test-command", mock_gen_meta.return_value)

================
File: tests/test_mcp_exposure.py
================
"""Tests for MCP server integration."""

import os
import sys
import json
import tempfile
from unittest import mock
import pytest

# Create a proper mock for FastMCP
class MockFastMCP:
    def __init__(self, name):
        self.name = name
        self.tool = MockToolRegistry()
    
    def run(self):
        """Mock run method."""
        pass

class MockToolRegistry:
    def __init__(self):
        self.registry = {}
    
    def __call__(self, name=None):
        def decorator(func):
            self.registry[name] = func
            return func
        return decorator

class MockContext:
    def __init__(self):
        pass

# Mock the MCP SDK
mcp_mock = mock.MagicMock()
mcp_mock.server.fastmcp.FastMCP = MockFastMCP
mcp_mock.server.fastmcp.Context = MockContext
sys.modules['mcp'] = mcp_mock
sys.modules['mcp.server'] = mcp_mock.server
sys.modules['mcp.server.fastmcp'] = mcp_mock.server.fastmcp

# Import after mocking
from evai.mcp.mcp_server import EVAIServer, create_server
from evai.tool_storage import get_command_dir, save_command_metadata, list_commands


@pytest.fixture
def mock_commands_dir():
    """Create a temporary directory for commands."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Mock the expanduser function to return our temp directory
        with mock.patch('os.path.expanduser', return_value=temp_dir):
            # Create the commands directory
            commands_dir = os.path.join(temp_dir, '.evai', 'commands')
            os.makedirs(commands_dir, exist_ok=True)
            
            # Create a test command
            test_cmd_dir = os.path.join(commands_dir, 'test-command')
            os.makedirs(test_cmd_dir, exist_ok=True)
            
            # Create command.yaml
            metadata = {
                "name": "test-command",
                "description": "A test command",
                "params": [
                    {
                        "name": "message",
                        "description": "A message to echo",
                        "required": True
                    },
                    {
                        "name": "count",
                        "description": "Number of times to echo",
                        "required": False,
                        "default": 1
                    }
                ],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": True,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
            
            with open(os.path.join(test_cmd_dir, 'command.yaml'), 'w') as f:
                json.dump(metadata, f)
            
            # Create command.py
            with open(os.path.join(test_cmd_dir, 'command.py'), 'w') as f:
                f.write('''"""Test command implementation."""

def run(message="Hello", count=1):
    """Run the test command."""
    result = []
    for _ in range(count):
        result.append(message)
    return {"messages": result}
''')
            
            # Create a command with MCP integration disabled
            disabled_mcp_dir = os.path.join(commands_dir, 'disabled-mcp')
            os.makedirs(disabled_mcp_dir, exist_ok=True)
            
            # Create command.yaml for disabled MCP command
            disabled_metadata = {
                "name": "disabled-mcp",
                "description": "A command with MCP integration disabled",
                "params": [],
                "hidden": False,
                "disabled": False,
                "mcp_integration": {
                    "enabled": False,
                    "metadata": {
                        "endpoint": "",
                        "method": "POST",
                        "authentication_required": False
                    }
                },
                "llm_interaction": {
                    "enabled": False,
                    "auto_apply": True,
                    "max_llm_turns": 15
                }
            }
            
            with open(os.path.join(disabled_mcp_dir, 'command.yaml'), 'w') as f:
                json.dump(disabled_metadata, f)
            
            # Create command.py for disabled MCP command
            with open(os.path.join(disabled_mcp_dir, 'command.py'), 'w') as f:
                f.write('''"""Disabled MCP command implementation."""

def run():
    """Run the disabled MCP command."""
    return {"status": "disabled"}
''')
            
            # Mock list_commands to return our test commands
            with mock.patch('evai.mcp_server.list_commands', return_value=[
                {
                    "name": "test-command",
                    "description": "A test command",
                    "path": test_cmd_dir
                },
                {
                    "name": "disabled-mcp",
                    "description": "A command with MCP integration disabled",
                    "path": disabled_mcp_dir
                }
            ]):
                # Mock load_command_metadata to return our test metadata
                with mock.patch('evai.mcp_server.load_command_metadata', side_effect=lambda path: 
                    metadata if "test-command" in path else disabled_metadata):
                    yield temp_dir


@mock.patch('evai.mcp_server.run_command')
def test_server_initialization(mock_run_command, mock_commands_dir):
    """Test MCP server initialization."""
    # Create a server
    server = create_server("Test Server")
    
    # Check that the server was initialized correctly
    assert server.name == "Test Server"
    assert isinstance(server.mcp, MockFastMCP)
    assert "test-command" in server.commands
    assert "disabled-mcp" not in server.commands
    
    # Check that built-in tools are registered
    assert "add_command" in server.mcp.tool.registry
    assert "list_commands" in server.mcp.tool.registry
    assert "edit_command_implementation" in server.mcp.tool.registry
    assert "edit_command_metadata" in server.mcp.tool.registry
    assert "test-command" in server.mcp.tool.registry


@mock.patch('evai.mcp_server.run_command')
def test_command_execution(mock_run_command, mock_commands_dir):
    """Test command execution through MCP."""
    # Mock the run_command function
    mock_run_command.return_value = {"messages": ["Hello World", "Hello World", "Hello World"]}
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "test-command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    result = tool_func(message="Hello World", count=3)
    
    # Check that run_command was called with the correct arguments
    mock_run_command.assert_called_once_with("test-command", message="Hello World", count=3)
    
    # Check the result
    assert result == {"messages": ["Hello World", "Hello World", "Hello World"]}


@mock.patch('evai.mcp_server.run_command')
def test_command_execution_error(mock_run_command, mock_commands_dir):
    """Test command execution error handling."""
    # Mock the run_command function to raise an exception
    mock_run_command.side_effect = Exception("Test error")
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "test-command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    result = tool_func(message="Hello World", count=3)
    
    # Check that run_command was called with the correct arguments
    mock_run_command.assert_called_once_with("test-command", message="Hello World", count=3)
    
    # Check the result
    assert result == {"error": "Test error"}


@mock.patch('evai.mcp_server.list_commands')
def test_list_commands_tool(mock_list_commands, mock_commands_dir):
    """Test the list_commands built-in tool."""
    # Mock the list_commands function
    mock_list_commands.return_value = [
        {
            "name": "test-command",
            "description": "A test command",
            "path": "/path/to/test-command"
        }
    ]
    
    # Create a server
    server = create_server("Test Server")
    
    # Reset the mock to clear previous calls
    mock_list_commands.reset_mock()
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "list_commands":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    result = tool_func()
    
    # Check that list_commands was called
    mock_list_commands.assert_called_once()
    
    # Check the result
    assert result == {
        "status": "success",
        "commands": [
            {
                "name": "test-command",
                "description": "A test command",
                "path": "/path/to/test-command"
            }
        ]
    }


@mock.patch('evai.mcp_server.get_command_dir')
@mock.patch('evai.mcp_server.save_command_metadata')
def test_add_command_tool(mock_save_metadata, mock_get_command_dir, mock_commands_dir):
    """Test the add_command built-in tool."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'new-command')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "add_command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=False):  # Mock that the command doesn't exist yet
        with mock.patch('builtins.open', mock.mock_open()) as mock_file:
            result = tool_func(
                command_name="new-command",
                description="A new command",
                params=[
                    {
                        "name": "message",
                        "description": "A message to echo",
                        "required": True
                    }
                ]
            )
    
    # Check that get_command_dir was called with the correct arguments
    mock_get_command_dir.assert_called_once_with("new-command")
    
    # Check that save_command_metadata was called with the correct arguments
    mock_save_metadata.assert_called_once()
    args, kwargs = mock_save_metadata.call_args
    assert args[0] == command_dir
    assert args[1]["name"] == "new-command"
    assert args[1]["description"] == "A new command"
    assert args[1]["params"] == [{"name": "message", "description": "A message to echo", "required": True}]
    
    # Check the result
    assert result["status"] == "success"
    assert "new-command" in result["message"]
    assert command_dir == result["command_dir"]


def test_add_command_tool_validation(mock_commands_dir):
    """Test validation in the add_command built-in tool."""
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "add_command":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Test empty command name
    result = tool_func(command_name="")
    assert result["status"] == "error"
    assert "Command name cannot be empty" in result["message"]
    
    # Test invalid command name
    result = tool_func(command_name="invalid command name")
    assert result["status"] == "error"
    assert "Command name must contain only alphanumeric characters" in result["message"]
    
    # Test command already exists
    with mock.patch('os.path.exists', return_value=True):  # Mock that the command already exists
        result = tool_func(command_name="existing-command")
        assert result["status"] == "error"
        assert "already exists" in result["message"]


@mock.patch('evai.mcp_server.get_command_dir')
@mock.patch('builtins.open', new_callable=mock.mock_open)
def test_edit_command_implementation_tool(mock_open, mock_get_command_dir, mock_commands_dir):
    """Test the edit_command_implementation built-in tool."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'test-command')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_implementation":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Reset the mock to clear previous calls
    mock_open.reset_mock()
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=True):  # Mock that the command exists
        with mock.patch('importlib.reload') as mock_reload:
            result = tool_func(
                command_name="test-command",
                implementation='''"""Updated test command implementation."""

def run(message="Hello", count=1):
    """Run the updated test command."""
    result = []
    for _ in range(count):
        result.append(f"Updated: {message}")
    return {"messages": result}
'''
            )
    
    # Check that get_command_dir was called with the correct arguments
    mock_get_command_dir.assert_called_once_with("test-command")
    
    # Check that the file was written with the correct content
    mock_open.assert_any_call(os.path.join(command_dir, "command.py"), "w")
    
    # Check the result
    assert result["status"] == "success"
    assert "test-command" in result["message"]
    assert "updated successfully" in result["message"]


@mock.patch('evai.mcp_server.get_command_dir')
def test_edit_command_implementation_nonexistent(mock_get_command_dir, mock_commands_dir):
    """Test editing implementation of a nonexistent command."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'nonexistent')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_implementation":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=False):  # Mock that the command doesn't exist
        result = tool_func(
            command_name="nonexistent",
            implementation="def run(): pass"
        )
    
    # Check the result
    assert result["status"] == "error"
    assert "does not exist" in result["message"]


@mock.patch('evai.mcp_server.get_command_dir')
@mock.patch('evai.mcp_server.save_command_metadata')
def test_edit_command_metadata_tool(mock_save_metadata, mock_get_command_dir, mock_commands_dir):
    """Test the edit_command_metadata built-in tool."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'test-command')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Add a command to the server's commands dict
    server.commands["test-command"] = {
        "name": "test-command",
        "description": "Original description",
        "params": []
    }
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_metadata":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=True):  # Mock that the command exists
        with mock.patch.dict(server.mcp.tool.registry, {"test-command": lambda: None}):
            result = tool_func(
                command_name="test-command",
                metadata={
                    "name": "test-command",
                    "description": "Updated description",
                    "params": [
                        {
                            "name": "new_param",
                            "description": "A new parameter",
                            "required": True
                        }
                    ],
                    "hidden": False,
                    "disabled": False,
                    "mcp_integration": {
                        "enabled": True
                    },
                    "llm_interaction": {
                        "enabled": False
                    }
                }
            )
    
    # Check that get_command_dir was called with the correct arguments
    mock_get_command_dir.assert_called_once_with("test-command")
    
    # Check that save_command_metadata was called with the correct arguments
    mock_save_metadata.assert_called_once()
    args, kwargs = mock_save_metadata.call_args
    assert args[0] == command_dir
    assert args[1]["name"] == "test-command"
    assert args[1]["description"] == "Updated description"
    assert len(args[1]["params"]) == 1
    assert args[1]["params"][0]["name"] == "new_param"
    
    # Check the result
    assert result["status"] == "success"
    assert "test-command" in result["message"]
    assert "updated successfully" in result["message"]
    
    # Check that the server's commands dict was updated
    assert server.commands["test-command"]["description"] == "Updated description"
    assert len(server.commands["test-command"]["params"]) == 1
    assert server.commands["test-command"]["params"][0]["name"] == "new_param"


@mock.patch('evai.mcp_server.get_command_dir')
def test_edit_command_metadata_nonexistent(mock_get_command_dir, mock_commands_dir):
    """Test editing metadata of a nonexistent command."""
    # Mock the get_command_dir function
    command_dir = os.path.join(mock_commands_dir, '.evai', 'commands', 'nonexistent')
    mock_get_command_dir.return_value = command_dir
    
    # Create a server
    server = create_server("Test Server")
    
    # Get the tool function
    tool_func = None
    for name, func in server.mcp.tool.registry.items():
        if name == "edit_command_metadata":
            tool_func = func
            break
    
    assert tool_func is not None
    
    # Call the tool function
    with mock.patch('os.path.exists', return_value=False):  # Mock that the command doesn't exist
        result = tool_func(
            command_name="nonexistent",
            metadata={"name": "nonexistent", "description": "Nonexistent command"}
        )
    
    # Check the result
    assert result["status"] == "error"
    assert "does not exist" in result["message"]

================
File: tests/test_metadata.py
================
"""Tests for the command storage module."""

import os
import shutil
import tempfile
from pathlib import Path
from unittest import mock

import pytest
import yaml

from evai.tool_storage import (
    get_command_dir,
    load_command_metadata,
    save_command_metadata,
)


@pytest.fixture
def temp_home_dir():
    """Create a temporary directory to use as the home directory."""
    with tempfile.TemporaryDirectory() as temp_dir:
        with mock.patch.dict(os.environ, {"HOME": temp_dir}):
            yield temp_dir


class TestGetCommandDir:
    """Tests for the get_command_dir function."""

    def test_get_command_dir_creates_directory(self, temp_home_dir):
        """Test that get_command_dir creates the directory if it doesn't exist."""
        command_name = "test-command"
        expected_path = os.path.join(temp_home_dir, ".evai", "commands", command_name)
        
        # Ensure the directory doesn't exist yet
        assert not os.path.exists(expected_path)
        
        # Call the function
        result = get_command_dir(command_name)
        
        # Check that the directory was created
        assert os.path.exists(expected_path)
        assert os.path.isdir(expected_path)
        assert result == expected_path

    def test_get_command_dir_with_existing_directory(self, temp_home_dir):
        """Test that get_command_dir returns the correct path for an existing directory."""
        command_name = "existing-command"
        expected_path = os.path.join(temp_home_dir, ".evai", "commands", command_name)
        
        # Create the directory
        os.makedirs(expected_path, exist_ok=True)
        
        # Call the function
        result = get_command_dir(command_name)
        
        # Check that the correct path was returned
        assert result == expected_path

    def test_get_command_dir_with_empty_name(self):
        """Test that get_command_dir raises ValueError for an empty command name."""
        with pytest.raises(ValueError, match="Command name cannot be empty"):
            get_command_dir("")

    def test_get_command_dir_with_invalid_name(self):
        """Test that get_command_dir raises ValueError for an invalid command name."""
        with pytest.raises(ValueError, match="Command name must contain only alphanumeric"):
            get_command_dir("invalid/command")


class TestLoadCommandMetadata:
    """Tests for the load_command_metadata function."""

    def test_load_command_metadata(self, temp_home_dir):
        """Test that load_command_metadata correctly loads YAML data."""
        # Create a test directory and YAML file
        test_dir = os.path.join(temp_home_dir, "test-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        test_data = {
            "name": "test-command",
            "description": "Test command",
            "params": [{"name": "param1", "type": "string"}],
        }
        
        yaml_path = os.path.join(test_dir, "command.yaml")
        with open(yaml_path, "w") as f:
            yaml.dump(test_data, f)
        
        # Call the function
        result = load_command_metadata(test_dir)
        
        # Check that the correct data was loaded
        assert result == test_data

    def test_load_command_metadata_file_not_found(self, temp_home_dir):
        """Test that load_command_metadata raises FileNotFoundError if the file doesn't exist."""
        test_dir = os.path.join(temp_home_dir, "nonexistent-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        with pytest.raises(FileNotFoundError):
            load_command_metadata(test_dir)

    def test_load_command_metadata_invalid_yaml(self, temp_home_dir):
        """Test that load_command_metadata raises YAMLError for invalid YAML."""
        # Create a test directory and invalid YAML file
        test_dir = os.path.join(temp_home_dir, "invalid-yaml-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        yaml_path = os.path.join(test_dir, "command.yaml")
        with open(yaml_path, "w") as f:
            f.write("invalid: yaml: :")
        
        with pytest.raises(yaml.YAMLError):
            load_command_metadata(test_dir)


class TestSaveCommandMetadata:
    """Tests for the save_command_metadata function."""

    def test_save_command_metadata(self, temp_home_dir):
        """Test that save_command_metadata correctly saves YAML data."""
        # Create a test directory
        test_dir = os.path.join(temp_home_dir, "save-test-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        test_data = {
            "name": "test-command",
            "description": "Test command",
            "params": [{"name": "param1", "type": "string"}],
        }
        
        # Call the function
        save_command_metadata(test_dir, test_data)
        
        # Check that the file was created with the correct content
        yaml_path = os.path.join(test_dir, "command.yaml")
        assert os.path.exists(yaml_path)
        
        with open(yaml_path, "r") as f:
            loaded_data = yaml.safe_load(f)
        
        assert loaded_data == test_data

    def test_save_command_metadata_creates_directory(self, temp_home_dir):
        """Test that save_command_metadata creates the directory if it doesn't exist."""
        test_dir = os.path.join(temp_home_dir, "nonexistent-dir")
        
        test_data = {"name": "test-command"}
        
        # Call the function
        save_command_metadata(test_dir, test_data)
        
        # Check that the directory and file were created
        yaml_path = os.path.join(test_dir, "command.yaml")
        assert os.path.exists(yaml_path)

    def test_save_command_metadata_empty_data(self, temp_home_dir):
        """Test that save_command_metadata raises ValueError for empty data."""
        test_dir = os.path.join(temp_home_dir, "empty-data-dir")
        os.makedirs(test_dir, exist_ok=True)
        
        with pytest.raises(ValueError, match="Command metadata cannot be empty"):
            save_command_metadata(test_dir, {})

================
File: tests/test_user_commands.py
================
"""Tests for user command loading and running."""

import os
import sys
import json
import tempfile
import shutil
from unittest import mock
import pytest
from click.testing import CliRunner
from pathlib import Path

from evai.cli.cli import cli
from evai.command_storage import get_command_dir, save_command_metadata


@pytest.fixture
def mock_commands_dir():
    """Create a temporary directory for commands with test command."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Mock the expanduser function to return our temp directory
        with mock.patch('os.path.expanduser') as mock_expanduser:
            # When expanduser is called with ~/.evai, return our temp .evai dir
            mock_expanduser.side_effect = lambda path: path.replace('~', temp_dir)
            
            # Create the commands directory
            commands_dir = os.path.join(temp_dir, '.evai', 'commands')
            os.makedirs(commands_dir, exist_ok=True)
            
            # Create a test command
            test_command_dir = os.path.join(commands_dir, 'hello')
            os.makedirs(test_command_dir, exist_ok=True)
            
            # Create command.yaml
            metadata = {
                "name": "hello",
                "description": "A test hello command",
                "arguments": [
                    {
                        "name": "name",
                        "description": "Name to greet",
                        "type": "string"
                    }
                ],
                "options": [
                    {
                        "name": "greeting",
                        "description": "Greeting to use",
                        "type": "string",
                        "required": False,
                        "default": "Hello"
                    }
                ],
                "hidden": False,
                "disabled": False
            }
            
            with open(os.path.join(test_command_dir, 'command.yaml'), 'w') as f:
                json.dump(metadata, f)
            
            # Create command.py
            with open(os.path.join(test_command_dir, 'command.py'), 'w') as f:
                f.write('''"""Test command implementation."""

def run(name, greeting="Hello"):
    """Greet the user."""
    message = f"{greeting}, {name}!"
    return {"message": message}
''')
            
            yield temp_dir


def test_user_command_in_help(mock_commands_dir):
    """Test that user commands appear in help."""
    runner = CliRunner()
    result = runner.invoke(cli, ['--help'])
    
    assert result.exit_code == 0
    assert "user" in result.output
    
    result = runner.invoke(cli, ['user', '--help'])
    assert result.exit_code == 0
    assert "hello" in result.output


def test_run_user_command(mock_commands_dir):
    """Test running a user command."""
    runner = CliRunner()
    result = runner.invoke(cli, ['user', 'hello', 'World', '--greeting', 'Hi'])
    
    assert result.exit_code == 0
    
    # Parse the JSON output
    output = json.loads(result.output)
    assert output == {"message": "Hi, World!"}

================
File: .cursorrules
================
# Instructions
- When using an entity, use:  db.entity_name.method_name to CRUD the entity, DO NOT IMPORT THE ENTITY DB CLASS, JUST USE THE METHODS ON THE DATABASE INSTANCE

## Lessons
During your interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

## Scratchpad
Always use the ./evai/docs/XXXXX_todo.md file (replace XXXXX as appropriate) in docs as a scratchpad to organize your thoughts. Especially when you receive a new task, you should first review the content of the scratchpad if it alreadye exists, clear old tasks if necessary, first explain the task, and plan the steps you need to take to complete the task. Use todo markers to indicate the progress, e.g.
[X] Task 1
[ ] Task 2

Also update the progress of the task in the Scratchpad when you finish a subtask.
Especially when you finished a milestone, it will help to improve your depth of task accomplishment to use the scratchpad to reflect and plan.
The goal is to help you maintain a big picture as well as the progress of the task. Always refer to the Scratchpad when you plan the next step.

## Project Rules:
Never use SQLAlchemy.  Always use straight SQL.
Never use SQLite.  Always use PostgreSQL.
We are not using JWT in the python api.
DB Migrations are in the db/migrations directory.  Use <datetime>_<name>.sql to name the migration files.
To run a db migration, use the following shell command:
```
dbmate up
```
To roll back a migration, use the following shell command:
```
dbmate down
```   
To see current migrations, use the following shell command:
```
dbmate status
```   


## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
llm --prompt "What is the capital of France?"
```

# History
### generated_feed renamed to user_feed
We renamed the generated_feed to user_feed to better reflect the purpose of the feed.

# Lessons

## User Specified Lessons
- You have a python venv in ./.venv. Use it.
- Do not use SQLAlchemy
- Use Pydantic 2 syntax for all data structures
- Include logging useful for debugging in the program output.
- Read the file before you try to edit it.
- Tests are non-interactive.  Using vi or vim to edit files needs to be mocked to avoid test failures.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

### Testing
- Tests should always be executed from the ~/projects/evai directory, not ~/projects/evai/evai
- Always run one test module at a time

## Cursor learned

- Use UUID4 for auto-generating unique identifiers in database models, do not use TEXT fields for IDs.
- When passing UUID fields between models, convert to string if the receiving model expects a string type field
- Always use timezone-aware datetime objects with UTC (from datetime import datetime, UTC) to avoid deprecation warnings and ensure consistent timezone handling
- When working with UUID fields in database operations:
  - Always convert UUID objects to strings before passing to database queries
  - Update type hints to accept both str and uuid.UUID where appropriate
  - Use explicit string conversion (str(uuid_value)) before using in queries
  - Handle cases where foreign key references might not exist by validating data before insertion
- When working with foreign key constraints:
  - Never use special values (like "all") as IDs that need to reference other tables
  - Consider using a separate system/metadata table for tracking operations that do not map directly to entities
  - Handle cases where foreign key references might not exist by validating data before insertion

## Database
- All entities are in their own db classes and are composed into the Database class in database.py
- When using an entity, use:  db.entity_name.method_name to CRUD the entity, DO NOT IMPORT THE ENTITY DB CLASS, JUST USE THE METHODS ON THE DATABASE INSTANCE

- Use asyncpg for PostgreSQL async operations
- PostgreSQL uses $1, $2 etc for parameterized queries instead of ?
- PostgreSQL has native JSON/JSONB support
- Use TIMESTAMP WITH TIME ZONE for proper timezone handling
- Use transactions for multi-statement operations
- PostgreSQL pool management is different from SQLite connections
- Always use CASCADE when dropping tables in PostgreSQL to handle foreign key constraints
- Initialize JSONB fields with default values in test fixtures
- When using UUID fields:
   - Store UUIDs as TEXT in PostgreSQL
   - Always convert UUID objects to strings before passing to database functions
   - Convert back to UUID objects in model layer if needed
   - Be consistent with string conversion in tests and assertions
- Each DB class should:
   - Take a Database instance in constructor
   - Use consistent error handling
   - Include proper type hints
   - Follow existing patterns from `user_db.py`
- Common utilities to keep in `database.py`:
   - Connection management
   - Transaction handling
   - Base query methods (fetch_one, fetch_all, etc.)
   - Row to model conversion utilities
- Error handling:
   - Use custom exceptions from `errors.py`
   - Consistent error messages
   - Proper logging
- Testing:
   - Each DB class should have its own test file
   - Follow existing test patterns
   - Include both success and error cases 

## Directory Structure Conventions

```
evai/
 evai/
    main.py                 # FastAPI initialization
    api/
       dependencies.py     # Shared dependencies (DB sessions, auth, etc.)
       v1/
           user_api.py         # Routes for user operations
           user_feed_api.py    # Routes for user feed operations
           source_feed_api.py  # Routes for source feed operations
    models/                 # Pydantic models
       user.py             # User entity model
       user_feed.py        # User feed entity model
       source_feed.py      # Source feed entity model
    db/                     # Database operations
       user_db.py             # CRUD for user
       user_feed_db.py        # CRUD for user feed
       source_feed_db.py      # CRUD for source feed
    util/                   # Utility functions
    config.py               # Application-wide configuration
 tests/                      # Unit and integration tests
    api/                    # Tests for API routes
       test_user_api.py
       test_user_feed_api.py
       test_source_feed_api.py
    models/                 # Tests for models
       test_user_model.py
       test_user_feed_model.py
       test_source_feed_model.py
    db/                     # Tests for database operations
       test_user_db.py
       test_user_feed_db.py
       test_source_feed_db.py
    util/
 scripts/                     # Utility scripts
 .gitignore
 pyproject.toml                # Build system configuration
 requirements.txt               # Dependencies
 README.md                      # Project documentation
```

## Naming Conventions

- **Python Modules & Directories**: `snake_case`
- **Classes**: `PascalCase`
- **Functions & Variables**: `snake_case`
- **Constants**: `UPPER_CASE`
- **API Routes**: `kebab-case`
- **Database Models**:
  - Class: `PascalCase`
  - Table: `snake_case`
- **Pydantic Schemas**:
  - `UserBase`: Base schema
  - `UserCreate`: Creation
  - `UserUpdate`: Update
  - `UserRead`: API responses

================
File: .gitignore
================
*.xml
*.mp3


# Byte-compiled / optimized / DLL files
Untitled.md
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so
.obsidian/
.DS_Store
MyPods.opml

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot


# Django stuff:
*.log
*.db
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# PyPI configuration file
.pypirc
.smtcmp_chat_histories
.aider*

================
File: .python-version
================
3.12

================
File: CLAUDE.md
================
# EVAI CLI Development Guide

## Build & Test Commands
- Setup: `python -m venv .venv && source .venv/bin/activate && pip install -e ".[dev]"`
- Run all tests: `pytest`
- Run single test file: `pytest tests/test_file.py`
- Run specific test: `pytest tests/test_file.py::test_function_name`
- Lint: `flake8`

## Code Style Guidelines
- Python 3.12+ required
- Use snake_case for variables, functions, modules
- Use PascalCase for classes
- Write unit tests using pytest
- Mock system components for testing
- Use Click for CLI commands, following existing patterns
- Use YAML for configuration files
- CLI tests use click.testing.CliRunner
- Never modify SQLAlchemy or SQLite code (PostgreSQL only)
- Prefer direct database access via db.entity_name.method_name pattern
- Use Pydantic 2 syntax for all data structures
- Include useful debug logging

## Project Organization
- Commands stored in ~/.evai/tools/ directory
- Tools have both metadata (YAML) and implementation (Python)
- Test modules are prefixed with "test_"
- Use the documentation in docs/ for understanding components

================
File: pyproject.toml
================
[project]
name = "evai-cli"
version = "0.1.0"
description = "Command-line interface for EVAI"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "click>=8.1.7",
    "mcp[cli]>=1.3.0",
    "pyyaml>=6.0.1",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "flake8>=6.1.0",
]

[project.scripts]
evai = "evai.cli:main"

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"

================
File: README.md
================
# EVAI CLI

A powerful command-line interface for creating, managing, and executing custom commands with LLM assistance.

## Overview

EVAI CLI is a tool that allows you to create, manage, and run custom commands with the help of Large Language Models (LLMs). It provides a seamless way to:

- Create custom commands with LLM assistance
- Edit command metadata and implementation
- Run commands with parameters
- Integrate with MCP (Machine Control Protocol) for advanced AI interactions
- Expose your commands as a local API server

## Installation

### Prerequisites

- Python 3.12 or higher
- pip (Python package installer)

### Install from source

```bash
# Clone the repository
git clone https://github.com/yourusername/evai-cli.git
cd evai-cli

# Create and activate a virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install the package in development mode
pip install -e .
```

## Usage

### Basic Commands

```bash
# Show help
evai --help

# Show version
evai --version

# List all available commands
evai command list

# Add a new command
evai command add <command_name>

# Add a new command with LLM assistance
evai command llmadd <command_name>

# Edit an existing command
evai command edit <command_name>

# Run a command
evai command run <command_name> --param key=value
```

### MCP Server

Start the MCP server to expose your commands as a local API:

```bash
evai server --name "My EVAI Commands"
```

## Command Structure

Each command consists of:

1. **Metadata** - A YAML file describing the command, its parameters, and integration options
2. **Implementation** - A Python file containing the actual command logic

Commands are stored in `~/.evai/commands/<command_name>/` with the following structure:

```
~/.evai/commands/<command_name>/
 metadata.yaml    # Command metadata
 implementation.py # Command implementation
```

### Command Metadata

The metadata file (`metadata.yaml`) contains information about the command:

```yaml
name: command_name
description: Description of what the command does
params:
  - name: param1
    description: Description of parameter 1
    required: true
  - name: param2
    description: Description of parameter 2
    required: false
hidden: false
disabled: false
mcp_integration:
  enabled: true
  metadata:
    endpoint: ""
    method: POST
    authentication_required: false
llm_interaction:
  enabled: false
  auto_apply: true
  max_llm_turns: 15
```

### Command Implementation

The implementation file (`implementation.py`) contains the actual command logic:

```python
"""Custom command implementation."""

def run(**kwargs):
    """Run the command with the given arguments."""
    # Your command logic here
    return {"status": "success", "data": {...}}
```

## Project Structure

```
evai-cli/
 evai/                      # Main package
    __init__.py            # Package initialization
    cli/                   # CLI module
       __init__.py        # CLI package initialization
       cli.py             # Main CLI implementation
       commands/          # CLI command modules
           __init__.py    # Commands package initialization
           llmadd.py      # LLM-assisted command creation
    command_storage.py     # Command storage utilities
    llm_client.py          # LLM client for AI assistance
    mcp_server.py          # MCP server integration
 tests/                     # Test suite
 .venv/                     # Virtual environment (created during setup)
 pyproject.toml             # Project metadata and dependencies
 requirements.txt           # Pinned dependencies
 README.md                  # This file
```

## LLM Integration

EVAI CLI integrates with LLMs to help you:

1. Generate command metadata based on your description
2. Generate command implementation based on metadata
3. Suggest additional information needed for better command generation

## MCP Integration

EVAI CLI integrates with the Machine Control Protocol (MCP) to:

1. Expose your commands as tools in an MCP server
2. Provide built-in tools for managing commands
3. Support prompt templates for common tasks

## Development

### Setup Development Environment

```bash
# Clone the repository
git clone https://github.com/yourusername/evai-cli.git
cd evai-cli

# Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install development dependencies
pip install -e ".[dev]"
```

### Running Tests

```bash
pytest
```

## Troubleshooting

### Missing `__init__.py` in Commands Directory

If you encounter an error like:

```
TypeError: expected str, bytes or os.PathLike object, not NoneType
```

When running the `evai` command, ensure that there is an `__init__.py` file in the `evai/cli/commands/` directory. This file is required to make the commands directory a proper Python package.

## License

[MIT License](LICENSE)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

================
File: requirements.txt
================
click==8.1.8
-e file:///Users/lherron/projects/evai-cli
iniconfig==2.0.0
packaging==24.2
pip==25.0.1
pluggy==1.5.0
pytest==8.3.5
pyyaml==6.0.2
rich==13.9.4

================
File: test_deploy_artifact.py
================
#!/usr/bin/env python
"""Test script for the deploy_artifact tool."""

from evai.tool_storage import run_tool
import json

# Sample React component
component = '''
import React from 'react';
import { Button } from '@/components/ui/button';

interface ButtonProps {
  label: string;
  onClick: () => void;
}

export const CustomButton: React.FC<ButtonProps> = ({ label, onClick }) => {
  return (
    <Button 
      className='bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded'
      onClick={onClick}
    >
      {label}
    </Button>
  );
};
'''

# Run the deploy_artifact tool
result = run_tool('deploy_artifact', artifact_name='CustomButton', source_code=component)
print(json.dumps(result, indent=2))

================
File: uv.lock
================
version = 1
revision = 1
requires-python = ">=3.12"

[[package]]
name = "annotated-types"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/ee/67/531ea369ba64dcff5ec9c3402f9f51bf748cec26dde048a2f973a4eea7f5/annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89", size = 16081 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53", size = 13643 },
]

[[package]]
name = "anyio"
version = "4.8.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "idna" },
    { name = "sniffio" },
    { name = "typing-extensions", marker = "python_full_version < '3.13'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/a3/73/199a98fc2dae33535d6b8e8e6ec01f8c1d76c9adb096c6b7d64823038cde/anyio-4.8.0.tar.gz", hash = "sha256:1d9fe889df5212298c0c0723fa20479d1b94883a2df44bd3897aa91083316f7a", size = 181126 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/46/eb/e7f063ad1fec6b3178a3cd82d1a3c4de82cccf283fc42746168188e1cdd5/anyio-4.8.0-py3-none-any.whl", hash = "sha256:b5011f270ab5eb0abf13385f851315585cc37ef330dd88e27ec3d34d651fd47a", size = 96041 },
]

[[package]]
name = "certifi"
version = "2025.1.31"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/1c/ab/c9f1e32b7b1bf505bf26f0ef697775960db7932abeb7b516de930ba2705f/certifi-2025.1.31.tar.gz", hash = "sha256:3d5da6925056f6f18f119200434a4780a94263f10d1c21d032a6f6b2baa20651", size = 167577 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/38/fc/bce832fd4fd99766c04d1ee0eead6b0ec6486fb100ae5e74c1d91292b982/certifi-2025.1.31-py3-none-any.whl", hash = "sha256:ca78db4565a652026a4db2bcdf68f2fb589ea80d0be70e03929ed730746b84fe", size = 166393 },
]

[[package]]
name = "click"
version = "8.1.8"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b9/2e/0090cbf739cee7d23781ad4b89a9894a41538e4fcf4c31dcdd705b78eb8b/click-8.1.8.tar.gz", hash = "sha256:ed53c9d8990d83c2a27deae68e4ee337473f6330c040a31d4225c9574d16096a", size = 226593 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7e/d4/7ebdbd03970677812aac39c869717059dbb71a4cfc033ca6e5221787892c/click-8.1.8-py3-none-any.whl", hash = "sha256:63c132bbbed01578a06712a2d1f497bb62d9c1c0d329b7903a866228027263b2", size = 98188 },
]

[[package]]
name = "colorama"
version = "0.4.6"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d8/53/6f443c9a4a8358a93a6792e2acffb9d9d5cb0a5cfd8802644b7b1c9a02e4/colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44", size = 27697 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6", size = 25335 },
]

[[package]]
name = "evai-cli"
version = "0.1.0"
source = { editable = "." }
dependencies = [
    { name = "click" },
    { name = "mcp", extra = ["cli"] },
    { name = "pyyaml" },
]

[package.optional-dependencies]
dev = [
    { name = "flake8" },
    { name = "pytest" },
]

[package.metadata]
requires-dist = [
    { name = "click", specifier = ">=8.1.7" },
    { name = "flake8", marker = "extra == 'dev'", specifier = ">=6.1.0" },
    { name = "mcp", extras = ["cli"], specifier = ">=1.3.0" },
    { name = "pytest", marker = "extra == 'dev'", specifier = ">=7.4.0" },
    { name = "pyyaml", specifier = ">=6.0.1" },
]
provides-extras = ["dev"]

[[package]]
name = "flake8"
version = "7.1.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mccabe" },
    { name = "pycodestyle" },
    { name = "pyflakes" },
]
sdist = { url = "https://files.pythonhosted.org/packages/58/16/3f2a0bb700ad65ac9663262905a025917c020a3f92f014d2ba8964b4602c/flake8-7.1.2.tar.gz", hash = "sha256:c586ffd0b41540951ae41af572e6790dbd49fc12b3aa2541685d253d9bd504bd", size = 48119 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/35/f8/08d37b2cd89da306e3520bd27f8a85692122b42b56c0c2c3784ff09c022f/flake8-7.1.2-py2.py3-none-any.whl", hash = "sha256:1cbc62e65536f65e6d754dfe6f1bada7f5cf392d6f5db3c2b85892466c3e7c1a", size = 57745 },
]

[[package]]
name = "h11"
version = "0.14.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f5/38/3af3d3633a34a3316095b39c8e8fb4853a28a536e55d347bd8d8e9a14b03/h11-0.14.0.tar.gz", hash = "sha256:8f19fbbe99e72420ff35c00b27a34cb9937e902a8b810e2c88300c6f0a3b699d", size = 100418 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl", hash = "sha256:e3fe4ac4b851c468cc8363d500db52c2ead036020723024a109d37346efaa761", size = 58259 },
]

[[package]]
name = "httpcore"
version = "1.0.7"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "certifi" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6a/41/d7d0a89eb493922c37d343b607bc1b5da7f5be7e383740b4753ad8943e90/httpcore-1.0.7.tar.gz", hash = "sha256:8551cb62a169ec7162ac7be8d4817d561f60e08eaa485234898414bb5a8a0b4c", size = 85196 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/87/f5/72347bc88306acb359581ac4d52f23c0ef445b57157adedb9aee0cd689d2/httpcore-1.0.7-py3-none-any.whl", hash = "sha256:a3fff8f43dc260d5bd363d9f9cf1830fa3a458b332856f34282de498ed420edd", size = 78551 },
]

[[package]]
name = "httpx"
version = "0.28.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "certifi" },
    { name = "httpcore" },
    { name = "idna" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b1/df/48c586a5fe32a0f01324ee087459e112ebb7224f646c0b5023f5e79e9956/httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc", size = 141406 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/2a/39/e50c7c3a983047577ee07d2a9e53faf5a69493943ec3f6a384bdc792deb2/httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad", size = 73517 },
]

[[package]]
name = "httpx-sse"
version = "0.4.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/4c/60/8f4281fa9bbf3c8034fd54c0e7412e66edbab6bc74c4996bd616f8d0406e/httpx-sse-0.4.0.tar.gz", hash = "sha256:1e81a3a3070ce322add1d3529ed42eb5f70817f45ed6ec915ab753f961139721", size = 12624 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl", hash = "sha256:f329af6eae57eaa2bdfd962b42524764af68075ea87370a2de920af5341e318f", size = 7819 },
]

[[package]]
name = "idna"
version = "3.10"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/f1/70/7703c29685631f5a7590aa73f1f1d3fa9a380e654b86af429e0934a32f7d/idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9", size = 190490 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3", size = 70442 },
]

[[package]]
name = "iniconfig"
version = "2.0.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d7/4b/cbd8e699e64a6f16ca3a8220661b5f83792b3017d0f79807cb8708d33913/iniconfig-2.0.0.tar.gz", hash = "sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3", size = 4646 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/ef/a6/62565a6e1cf69e10f5727360368e451d4b7f58beeac6173dc9db836a5b46/iniconfig-2.0.0-py3-none-any.whl", hash = "sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374", size = 5892 },
]

[[package]]
name = "markdown-it-py"
version = "3.0.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "mdurl" },
]
sdist = { url = "https://files.pythonhosted.org/packages/38/71/3b932df36c1a044d397a1f92d1cf91ee0a503d91e470cbd670aa66b07ed0/markdown-it-py-3.0.0.tar.gz", hash = "sha256:e3f60a94fa066dc52ec76661e37c851cb232d92f9886b15cb560aaada2df8feb", size = 74596 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl", hash = "sha256:355216845c60bd96232cd8d8c40e8f9765cc86f46880e43a8fd22dc1a1a8cab1", size = 87528 },
]

[[package]]
name = "mccabe"
version = "0.7.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/e7/ff/0ffefdcac38932a54d2b5eed4e0ba8a408f215002cd178ad1df0f2806ff8/mccabe-0.7.0.tar.gz", hash = "sha256:348e0240c33b60bbdf4e523192ef919f28cb2c3d7d5c7794f74009290f236325", size = 9658 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/27/1a/1f68f9ba0c207934b35b86a8ca3aad8395a3d6dd7921c0686e23853ff5a9/mccabe-0.7.0-py2.py3-none-any.whl", hash = "sha256:6c2d30ab6be0e4a46919781807b4f0d834ebdd6c6e3dca0bda5a15f863427b6e", size = 7350 },
]

[[package]]
name = "mcp"
version = "1.3.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "httpx" },
    { name = "httpx-sse" },
    { name = "pydantic" },
    { name = "pydantic-settings" },
    { name = "sse-starlette" },
    { name = "starlette" },
    { name = "uvicorn" },
]
sdist = { url = "https://files.pythonhosted.org/packages/6b/b6/81e5f2490290351fc97bf46c24ff935128cb7d34d68e3987b522f26f7ada/mcp-1.3.0.tar.gz", hash = "sha256:f409ae4482ce9d53e7ac03f3f7808bcab735bdfc0fba937453782efb43882d45", size = 150235 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d0/d2/a9e87b506b2094f5aa9becc1af5178842701b27217fa43877353da2577e3/mcp-1.3.0-py3-none-any.whl", hash = "sha256:2829d67ce339a249f803f22eba5e90385eafcac45c94b00cab6cef7e8f217211", size = 70672 },
]

[package.optional-dependencies]
cli = [
    { name = "python-dotenv" },
    { name = "typer" },
]

[[package]]
name = "mdurl"
version = "0.1.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d6/54/cfe61301667036ec958cb99bd3efefba235e65cdeb9c84d24a8293ba1d90/mdurl-0.1.2.tar.gz", hash = "sha256:bb413d29f5eea38f31dd4754dd7377d4465116fb207585f97bf925588687c1ba", size = 8729 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl", hash = "sha256:84008a41e51615a49fc9966191ff91509e3c40b939176e643fd50a5c2196b8f8", size = 9979 },
]

[[package]]
name = "packaging"
version = "24.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/d0/63/68dbb6eb2de9cb10ee4c9c14a0148804425e13c4fb20d61cce69f53106da/packaging-24.2.tar.gz", hash = "sha256:c228a6dc5e932d346bc5739379109d49e8853dd8223571c7c5b55260edc0b97f", size = 163950 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/ef/eb23f262cca3c0c4eb7ab1933c3b1f03d021f2c48f54763065b6f0e321be/packaging-24.2-py3-none-any.whl", hash = "sha256:09abb1bccd265c01f4a3aa3f7a7db064b36514d2cba19a2f694fe6150451a759", size = 65451 },
]

[[package]]
name = "pluggy"
version = "1.5.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/96/2d/02d4312c973c6050a18b314a5ad0b3210edb65a906f868e31c111dede4a6/pluggy-1.5.0.tar.gz", hash = "sha256:2cffa88e94fdc978c4c574f15f9e59b7f4201d439195c3715ca9e2486f1d0cf1", size = 67955 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/88/5f/e351af9a41f866ac3f1fac4ca0613908d9a41741cfcf2228f4ad853b697d/pluggy-1.5.0-py3-none-any.whl", hash = "sha256:44e1ad92c8ca002de6377e165f3e0f1be63266ab4d554740532335b9d75ea669", size = 20556 },
]

[[package]]
name = "pycodestyle"
version = "2.12.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/43/aa/210b2c9aedd8c1cbeea31a50e42050ad56187754b34eb214c46709445801/pycodestyle-2.12.1.tar.gz", hash = "sha256:6838eae08bbce4f6accd5d5572075c63626a15ee3e6f842df996bf62f6d73521", size = 39232 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/3a/d8/a211b3f85e99a0daa2ddec96c949cac6824bd305b040571b82a03dd62636/pycodestyle-2.12.1-py2.py3-none-any.whl", hash = "sha256:46f0fb92069a7c28ab7bb558f05bfc0110dac69a0cd23c61ea0040283a9d78b3", size = 31284 },
]

[[package]]
name = "pydantic"
version = "2.10.6"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "annotated-types" },
    { name = "pydantic-core" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/b7/ae/d5220c5c52b158b1de7ca89fc5edb72f304a70a4c540c84c8844bf4008de/pydantic-2.10.6.tar.gz", hash = "sha256:ca5daa827cce33de7a42be142548b0096bf05a7e7b365aebfa5f8eeec7128236", size = 761681 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/f4/3c/8cc1cc84deffa6e25d2d0c688ebb80635dfdbf1dbea3e30c541c8cf4d860/pydantic-2.10.6-py3-none-any.whl", hash = "sha256:427d664bf0b8a2b34ff5dd0f5a18df00591adcee7198fbd71981054cef37b584", size = 431696 },
]

[[package]]
name = "pydantic-core"
version = "2.27.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/fc/01/f3e5ac5e7c25833db5eb555f7b7ab24cd6f8c322d3a3ad2d67a952dc0abc/pydantic_core-2.27.2.tar.gz", hash = "sha256:eb026e5a4c1fee05726072337ff51d1efb6f59090b7da90d30ea58625b1ffb39", size = 413443 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d6/74/51c8a5482ca447871c93e142d9d4a92ead74de6c8dc5e66733e22c9bba89/pydantic_core-2.27.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:9e0c8cfefa0ef83b4da9588448b6d8d2a2bf1a53c3f1ae5fca39eb3061e2f0b0", size = 1893127 },
    { url = "https://files.pythonhosted.org/packages/d3/f3/c97e80721735868313c58b89d2de85fa80fe8dfeeed84dc51598b92a135e/pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:83097677b8e3bd7eaa6775720ec8e0405f1575015a463285a92bfdfe254529ef", size = 1811340 },
    { url = "https://files.pythonhosted.org/packages/9e/91/840ec1375e686dbae1bd80a9e46c26a1e0083e1186abc610efa3d9a36180/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:172fce187655fece0c90d90a678424b013f8fbb0ca8b036ac266749c09438cb7", size = 1822900 },
    { url = "https://files.pythonhosted.org/packages/f6/31/4240bc96025035500c18adc149aa6ffdf1a0062a4b525c932065ceb4d868/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:519f29f5213271eeeeb3093f662ba2fd512b91c5f188f3bb7b27bc5973816934", size = 1869177 },
    { url = "https://files.pythonhosted.org/packages/fa/20/02fbaadb7808be578317015c462655c317a77a7c8f0ef274bc016a784c54/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:05e3a55d124407fffba0dd6b0c0cd056d10e983ceb4e5dbd10dda135c31071d6", size = 2038046 },
    { url = "https://files.pythonhosted.org/packages/06/86/7f306b904e6c9eccf0668248b3f272090e49c275bc488a7b88b0823444a4/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9c3ed807c7b91de05e63930188f19e921d1fe90de6b4f5cd43ee7fcc3525cb8c", size = 2685386 },
    { url = "https://files.pythonhosted.org/packages/8d/f0/49129b27c43396581a635d8710dae54a791b17dfc50c70164866bbf865e3/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6fb4aadc0b9a0c063206846d603b92030eb6f03069151a625667f982887153e2", size = 1997060 },
    { url = "https://files.pythonhosted.org/packages/0d/0f/943b4af7cd416c477fd40b187036c4f89b416a33d3cc0ab7b82708a667aa/pydantic_core-2.27.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:28ccb213807e037460326424ceb8b5245acb88f32f3d2777427476e1b32c48c4", size = 2004870 },
    { url = "https://files.pythonhosted.org/packages/35/40/aea70b5b1a63911c53a4c8117c0a828d6790483f858041f47bab0b779f44/pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:de3cd1899e2c279b140adde9357c4495ed9d47131b4a4eaff9052f23398076b3", size = 1999822 },
    { url = "https://files.pythonhosted.org/packages/f2/b3/807b94fd337d58effc5498fd1a7a4d9d59af4133e83e32ae39a96fddec9d/pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:220f892729375e2d736b97d0e51466252ad84c51857d4d15f5e9692f9ef12be4", size = 2130364 },
    { url = "https://files.pythonhosted.org/packages/fc/df/791c827cd4ee6efd59248dca9369fb35e80a9484462c33c6649a8d02b565/pydantic_core-2.27.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:a0fcd29cd6b4e74fe8ddd2c90330fd8edf2e30cb52acda47f06dd615ae72da57", size = 2158303 },
    { url = "https://files.pythonhosted.org/packages/9b/67/4e197c300976af185b7cef4c02203e175fb127e414125916bf1128b639a9/pydantic_core-2.27.2-cp312-cp312-win32.whl", hash = "sha256:1e2cb691ed9834cd6a8be61228471d0a503731abfb42f82458ff27be7b2186fc", size = 1834064 },
    { url = "https://files.pythonhosted.org/packages/1f/ea/cd7209a889163b8dcca139fe32b9687dd05249161a3edda62860430457a5/pydantic_core-2.27.2-cp312-cp312-win_amd64.whl", hash = "sha256:cc3f1a99a4f4f9dd1de4fe0312c114e740b5ddead65bb4102884b384c15d8bc9", size = 1989046 },
    { url = "https://files.pythonhosted.org/packages/bc/49/c54baab2f4658c26ac633d798dab66b4c3a9bbf47cff5284e9c182f4137a/pydantic_core-2.27.2-cp312-cp312-win_arm64.whl", hash = "sha256:3911ac9284cd8a1792d3cb26a2da18f3ca26c6908cc434a18f730dc0db7bfa3b", size = 1885092 },
    { url = "https://files.pythonhosted.org/packages/41/b1/9bc383f48f8002f99104e3acff6cba1231b29ef76cfa45d1506a5cad1f84/pydantic_core-2.27.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:7d14bd329640e63852364c306f4d23eb744e0f8193148d4044dd3dacdaacbd8b", size = 1892709 },
    { url = "https://files.pythonhosted.org/packages/10/6c/e62b8657b834f3eb2961b49ec8e301eb99946245e70bf42c8817350cbefc/pydantic_core-2.27.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:82f91663004eb8ed30ff478d77c4d1179b3563df6cdb15c0817cd1cdaf34d154", size = 1811273 },
    { url = "https://files.pythonhosted.org/packages/ba/15/52cfe49c8c986e081b863b102d6b859d9defc63446b642ccbbb3742bf371/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:71b24c7d61131bb83df10cc7e687433609963a944ccf45190cfc21e0887b08c9", size = 1823027 },
    { url = "https://files.pythonhosted.org/packages/b1/1c/b6f402cfc18ec0024120602bdbcebc7bdd5b856528c013bd4d13865ca473/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:fa8e459d4954f608fa26116118bb67f56b93b209c39b008277ace29937453dc9", size = 1868888 },
    { url = "https://files.pythonhosted.org/packages/bd/7b/8cb75b66ac37bc2975a3b7de99f3c6f355fcc4d89820b61dffa8f1e81677/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ce8918cbebc8da707ba805b7fd0b382816858728ae7fe19a942080c24e5b7cd1", size = 2037738 },
    { url = "https://files.pythonhosted.org/packages/c8/f1/786d8fe78970a06f61df22cba58e365ce304bf9b9f46cc71c8c424e0c334/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:eda3f5c2a021bbc5d976107bb302e0131351c2ba54343f8a496dc8783d3d3a6a", size = 2685138 },
    { url = "https://files.pythonhosted.org/packages/a6/74/d12b2cd841d8724dc8ffb13fc5cef86566a53ed358103150209ecd5d1999/pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:bd8086fa684c4775c27f03f062cbb9eaa6e17f064307e86b21b9e0abc9c0f02e", size = 1997025 },
    { url = "https://files.pythonhosted.org/packages/a0/6e/940bcd631bc4d9a06c9539b51f070b66e8f370ed0933f392db6ff350d873/pydantic_core-2.27.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:8d9b3388db186ba0c099a6d20f0604a44eabdeef1777ddd94786cdae158729e4", size = 2004633 },
    { url = "https://files.pythonhosted.org/packages/50/cc/a46b34f1708d82498c227d5d80ce615b2dd502ddcfd8376fc14a36655af1/pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:7a66efda2387de898c8f38c0cf7f14fca0b51a8ef0b24bfea5849f1b3c95af27", size = 1999404 },
    { url = "https://files.pythonhosted.org/packages/ca/2d/c365cfa930ed23bc58c41463bae347d1005537dc8db79e998af8ba28d35e/pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:18a101c168e4e092ab40dbc2503bdc0f62010e95d292b27827871dc85450d7ee", size = 2130130 },
    { url = "https://files.pythonhosted.org/packages/f4/d7/eb64d015c350b7cdb371145b54d96c919d4db516817f31cd1c650cae3b21/pydantic_core-2.27.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:ba5dd002f88b78a4215ed2f8ddbdf85e8513382820ba15ad5ad8955ce0ca19a1", size = 2157946 },
    { url = "https://files.pythonhosted.org/packages/a4/99/bddde3ddde76c03b65dfd5a66ab436c4e58ffc42927d4ff1198ffbf96f5f/pydantic_core-2.27.2-cp313-cp313-win32.whl", hash = "sha256:1ebaf1d0481914d004a573394f4be3a7616334be70261007e47c2a6fe7e50130", size = 1834387 },
    { url = "https://files.pythonhosted.org/packages/71/47/82b5e846e01b26ac6f1893d3c5f9f3a2eb6ba79be26eef0b759b4fe72946/pydantic_core-2.27.2-cp313-cp313-win_amd64.whl", hash = "sha256:953101387ecf2f5652883208769a79e48db18c6df442568a0b5ccd8c2723abee", size = 1990453 },
    { url = "https://files.pythonhosted.org/packages/51/b2/b2b50d5ecf21acf870190ae5d093602d95f66c9c31f9d5de6062eb329ad1/pydantic_core-2.27.2-cp313-cp313-win_arm64.whl", hash = "sha256:ac4dbfd1691affb8f48c2c13241a2e3b60ff23247cbcf981759c768b6633cf8b", size = 1885186 },
]

[[package]]
name = "pydantic-settings"
version = "2.8.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "pydantic" },
    { name = "python-dotenv" },
]
sdist = { url = "https://files.pythonhosted.org/packages/88/82/c79424d7d8c29b994fb01d277da57b0a9b09cc03c3ff875f9bd8a86b2145/pydantic_settings-2.8.1.tar.gz", hash = "sha256:d5c663dfbe9db9d5e1c646b2e161da12f0d734d422ee56f567d0ea2cee4e8585", size = 83550 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/0b/53/a64f03044927dc47aafe029c42a5b7aabc38dfb813475e0e1bf71c4a59d0/pydantic_settings-2.8.1-py3-none-any.whl", hash = "sha256:81942d5ac3d905f7f3ee1a70df5dfb62d5569c12f51a5a647defc1c3d9ee2e9c", size = 30839 },
]

[[package]]
name = "pyflakes"
version = "3.2.0"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/57/f9/669d8c9c86613c9d568757c7f5824bd3197d7b1c6c27553bc5618a27cce2/pyflakes-3.2.0.tar.gz", hash = "sha256:1c61603ff154621fb2a9172037d84dca3500def8c8b630657d1701f026f8af3f", size = 63788 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d4/d7/f1b7db88d8e4417c5d47adad627a93547f44bdc9028372dbd2313f34a855/pyflakes-3.2.0-py2.py3-none-any.whl", hash = "sha256:84b5be138a2dfbb40689ca07e2152deb896a65c3a3e24c251c5c62489568074a", size = 62725 },
]

[[package]]
name = "pygments"
version = "2.19.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/7c/2d/c3338d48ea6cc0feb8446d8e6937e1408088a72a39937982cc6111d17f84/pygments-2.19.1.tar.gz", hash = "sha256:61c16d2a8576dc0649d9f39e089b5f02bcd27fba10d8fb4dcc28173f7a45151f", size = 4968581 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl", hash = "sha256:9ea1544ad55cecf4b8242fab6dd35a93bbce657034b0611ee383099054ab6d8c", size = 1225293 },
]

[[package]]
name = "pytest"
version = "8.3.5"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "colorama", marker = "sys_platform == 'win32'" },
    { name = "iniconfig" },
    { name = "packaging" },
    { name = "pluggy" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ae/3c/c9d525a414d506893f0cd8a8d0de7706446213181570cdbd766691164e40/pytest-8.3.5.tar.gz", hash = "sha256:f4efe70cc14e511565ac476b57c279e12a855b11f48f212af1080ef2263d3845", size = 1450891 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/30/3d/64ad57c803f1fa1e963a7946b6e0fea4a70df53c1a7fed304586539c2bac/pytest-8.3.5-py3-none-any.whl", hash = "sha256:c69214aa47deac29fad6c2a4f590b9c4a9fdb16a403176fe154b79c0b4d4d820", size = 343634 },
]

[[package]]
name = "python-dotenv"
version = "1.0.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/bc/57/e84d88dfe0aec03b7a2d4327012c1627ab5f03652216c63d49846d7a6c58/python-dotenv-1.0.1.tar.gz", hash = "sha256:e324ee90a023d808f1959c46bcbc04446a10ced277783dc6ee09987c37ec10ca", size = 39115 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl", hash = "sha256:f7b63ef50f1b690dddf550d03497b66d609393b40b564ed0d674909a68ebf16a", size = 19863 },
]

[[package]]
name = "pyyaml"
version = "6.0.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/54/ed/79a089b6be93607fa5cdaedf301d7dfb23af5f25c398d5ead2525b063e17/pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e", size = 130631 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/86/0c/c581167fc46d6d6d7ddcfb8c843a4de25bdd27e4466938109ca68492292c/PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:c70c95198c015b85feafc136515252a261a84561b7b1d51e3384e0655ddf25ab", size = 183873 },
    { url = "https://files.pythonhosted.org/packages/a8/0c/38374f5bb272c051e2a69281d71cba6fdb983413e6758b84482905e29a5d/PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:ce826d6ef20b1bc864f0a68340c8b3287705cae2f8b4b1d932177dcc76721725", size = 173302 },
    { url = "https://files.pythonhosted.org/packages/c3/93/9916574aa8c00aa06bbac729972eb1071d002b8e158bd0e83a3b9a20a1f7/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1f71ea527786de97d1a0cc0eacd1defc0985dcf6b3f17bb77dcfc8c34bec4dc5", size = 739154 },
    { url = "https://files.pythonhosted.org/packages/95/0f/b8938f1cbd09739c6da569d172531567dbcc9789e0029aa070856f123984/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9b22676e8097e9e22e36d6b7bda33190d0d400f345f23d4065d48f4ca7ae0425", size = 766223 },
    { url = "https://files.pythonhosted.org/packages/b9/2b/614b4752f2e127db5cc206abc23a8c19678e92b23c3db30fc86ab731d3bd/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:80bab7bfc629882493af4aa31a4cfa43a4c57c83813253626916b8c7ada83476", size = 767542 },
    { url = "https://files.pythonhosted.org/packages/d4/00/dd137d5bcc7efea1836d6264f049359861cf548469d18da90cd8216cf05f/PyYAML-6.0.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:0833f8694549e586547b576dcfaba4a6b55b9e96098b36cdc7ebefe667dfed48", size = 731164 },
    { url = "https://files.pythonhosted.org/packages/c9/1f/4f998c900485e5c0ef43838363ba4a9723ac0ad73a9dc42068b12aaba4e4/PyYAML-6.0.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8b9c7197f7cb2738065c481a0461e50ad02f18c78cd75775628afb4d7137fb3b", size = 756611 },
    { url = "https://files.pythonhosted.org/packages/df/d1/f5a275fdb252768b7a11ec63585bc38d0e87c9e05668a139fea92b80634c/PyYAML-6.0.2-cp312-cp312-win32.whl", hash = "sha256:ef6107725bd54b262d6dedcc2af448a266975032bc85ef0172c5f059da6325b4", size = 140591 },
    { url = "https://files.pythonhosted.org/packages/0c/e8/4f648c598b17c3d06e8753d7d13d57542b30d56e6c2dedf9c331ae56312e/PyYAML-6.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:7e7401d0de89a9a855c839bc697c079a4af81cf878373abd7dc625847d25cbd8", size = 156338 },
    { url = "https://files.pythonhosted.org/packages/ef/e3/3af305b830494fa85d95f6d95ef7fa73f2ee1cc8ef5b495c7c3269fb835f/PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba", size = 181309 },
    { url = "https://files.pythonhosted.org/packages/45/9f/3b1c20a0b7a3200524eb0076cc027a970d320bd3a6592873c85c92a08731/PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1", size = 171679 },
    { url = "https://files.pythonhosted.org/packages/7c/9a/337322f27005c33bcb656c655fa78325b730324c78620e8328ae28b64d0c/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133", size = 733428 },
    { url = "https://files.pythonhosted.org/packages/a3/69/864fbe19e6c18ea3cc196cbe5d392175b4cf3d5d0ac1403ec3f2d237ebb5/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484", size = 763361 },
    { url = "https://files.pythonhosted.org/packages/04/24/b7721e4845c2f162d26f50521b825fb061bc0a5afcf9a386840f23ea19fa/PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5", size = 759523 },
    { url = "https://files.pythonhosted.org/packages/2b/b2/e3234f59ba06559c6ff63c4e10baea10e5e7df868092bf9ab40e5b9c56b6/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc", size = 726660 },
    { url = "https://files.pythonhosted.org/packages/fe/0f/25911a9f080464c59fab9027482f822b86bf0608957a5fcc6eaac85aa515/PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652", size = 751597 },
    { url = "https://files.pythonhosted.org/packages/14/0d/e2c3b43bbce3cf6bd97c840b46088a3031085179e596d4929729d8d68270/PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183", size = 140527 },
    { url = "https://files.pythonhosted.org/packages/fa/de/02b54f42487e3d3c6efb3f89428677074ca7bf43aae402517bc7cca949f3/PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563", size = 156446 },
]

[[package]]
name = "rich"
version = "13.9.4"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "markdown-it-py" },
    { name = "pygments" },
]
sdist = { url = "https://files.pythonhosted.org/packages/ab/3a/0316b28d0761c6734d6bc14e770d85506c986c85ffb239e688eeaab2c2bc/rich-13.9.4.tar.gz", hash = "sha256:439594978a49a09530cff7ebc4b5c7103ef57baf48d5ea3184f21d9a2befa098", size = 223149 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/19/71/39c7c0d87f8d4e6c020a393182060eaefeeae6c01dab6a84ec346f2567df/rich-13.9.4-py3-none-any.whl", hash = "sha256:6049d5e6ec054bf2779ab3358186963bac2ea89175919d699e378b99738c2a90", size = 242424 },
]

[[package]]
name = "shellingham"
version = "1.5.4"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/58/15/8b3609fd3830ef7b27b655beb4b4e9c62313a4e8da8c676e142cc210d58e/shellingham-1.5.4.tar.gz", hash = "sha256:8dbca0739d487e5bd35ab3ca4b36e11c4078f3a234bfce294b0a0291363404de", size = 10310 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl", hash = "sha256:7ecfff8f2fd72616f7481040475a65b2bf8af90a56c89140852d1120324e8686", size = 9755 },
]

[[package]]
name = "sniffio"
version = "1.3.1"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/a2/87/a6771e1546d97e7e041b6ae58d80074f81b7d5121207425c964ddf5cfdbd/sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc", size = 20372 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2", size = 10235 },
]

[[package]]
name = "sse-starlette"
version = "2.2.1"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
    { name = "starlette" },
]
sdist = { url = "https://files.pythonhosted.org/packages/71/a4/80d2a11af59fe75b48230846989e93979c892d3a20016b42bb44edb9e398/sse_starlette-2.2.1.tar.gz", hash = "sha256:54470d5f19274aeed6b2d473430b08b4b379ea851d953b11d7f1c4a2c118b419", size = 17376 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/d9/e0/5b8bd393f27f4a62461c5cf2479c75a2cc2ffa330976f9f00f5f6e4f50eb/sse_starlette-2.2.1-py3-none-any.whl", hash = "sha256:6410a3d3ba0c89e7675d4c273a301d64649c03a5ef1ca101f10b47f895fd0e99", size = 10120 },
]

[[package]]
name = "starlette"
version = "0.46.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "anyio" },
]
sdist = { url = "https://files.pythonhosted.org/packages/44/b6/fb9a32e3c5d59b1e383c357534c63c2d3caa6f25bf3c59dd89d296ecbaec/starlette-0.46.0.tar.gz", hash = "sha256:b359e4567456b28d473d0193f34c0de0ed49710d75ef183a74a5ce0499324f50", size = 2575568 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/41/94/8af675a62e3c91c2dee47cf92e602cfac86e8767b1a1ac3caf1b327c2ab0/starlette-0.46.0-py3-none-any.whl", hash = "sha256:913f0798bd90ba90a9156383bcf1350a17d6259451d0d8ee27fc0cf2db609038", size = 71991 },
]

[[package]]
name = "typer"
version = "0.15.2"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "rich" },
    { name = "shellingham" },
    { name = "typing-extensions" },
]
sdist = { url = "https://files.pythonhosted.org/packages/8b/6f/3991f0f1c7fcb2df31aef28e0594d8d54b05393a0e4e34c65e475c2a5d41/typer-0.15.2.tar.gz", hash = "sha256:ab2fab47533a813c49fe1f16b1a370fd5819099c00b119e0633df65f22144ba5", size = 100711 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/7f/fc/5b29fea8cee020515ca82cc68e3b8e1e34bb19a3535ad854cac9257b414c/typer-0.15.2-py3-none-any.whl", hash = "sha256:46a499c6107d645a9c13f7ee46c5d5096cae6f5fc57dd11eccbbb9ae3e44ddfc", size = 45061 },
]

[[package]]
name = "typing-extensions"
version = "4.12.2"
source = { registry = "https://pypi.org/simple" }
sdist = { url = "https://files.pythonhosted.org/packages/df/db/f35a00659bc03fec321ba8bce9420de607a1d37f8342eee1863174c69557/typing_extensions-4.12.2.tar.gz", hash = "sha256:1a7ead55c7e559dd4dee8856e3a88b41225abfe1ce8df57b7c13915fe121ffb8", size = 85321 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl", hash = "sha256:04e5ca0351e0f3f85c6853954072df659d0d13fac324d0072316b67d7794700d", size = 37438 },
]

[[package]]
name = "uvicorn"
version = "0.34.0"
source = { registry = "https://pypi.org/simple" }
dependencies = [
    { name = "click" },
    { name = "h11" },
]
sdist = { url = "https://files.pythonhosted.org/packages/4b/4d/938bd85e5bf2edeec766267a5015ad969730bb91e31b44021dfe8b22df6c/uvicorn-0.34.0.tar.gz", hash = "sha256:404051050cd7e905de2c9a7e61790943440b3416f49cb409f965d9dcd0fa73e9", size = 76568 }
wheels = [
    { url = "https://files.pythonhosted.org/packages/61/14/33a3a1352cfa71812a3a21e8c9bfb83f60b0011f5e36f2b1399d51928209/uvicorn-0.34.0-py3-none-any.whl", hash = "sha256:023dc038422502fa28a09c7a30bf2b6991512da7dcdb8fd35fe57cfc154126f4", size = 62315 },
]



================================================================
End of Codebase
================================================================
